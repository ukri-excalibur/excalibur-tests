{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#excalibur-tests","title":"ExCALIBUR tests","text":"<p>Performance benchmarks and regression tests for the ExCALIBUR project.</p> <p>These benchmarks are based on a similar project by StackHPC.</p> <p>Feel free to add new benchmark applications or support new systems that are part of the ExCALIBUR benchmarking collaboration.</p> <p>Note: at the moment the ExCALIBUR benchmarks are a work-in-progress.</p>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Installation</li> <li>Configuration</li> <li>Usage</li> <li>Post-processing</li> <li>Contributing</li> <li>Supported benchmarks</li> <li>Supported systems</li> <li>ARCHER2 tutorial</li> </ul>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This work was supported by the Engineering and Physical Sciences Research Council [EP/X031829/1].</p> <p>This work used the DiRAC@Durham facility managed by the Institute for Computational  Cosmology on behalf of the STFC DiRAC HPC Facility (www.dirac.ac.uk). The equipment  was funded by BEIS capital funding via STFC capital grants ST/P002293/1, ST/R002371/1 and ST/S002502/1, Durham University and STFC operations grant ST/R000832/1.  DiRAC is part of the National e-Infrastructure.</p> <p>The main outcomes of this work were published in a paper in the HPCTESTS workshop in SC23.</p> <p>This work was presented in RSECon23. A recording of the talk is available.</p>"},{"location":"contributing/","title":"How to contribute to ExCALIBUR tests","text":"<p>You are welcome to contribute new application benchmarks and new systems part of the ExCALIBUR benchmarking effort. The easiest way to contribute is to open an issue or a pull request to the repository.</p>"},{"location":"contributing/#adding-new-benchmarks","title":"Adding new benchmarks","text":"<p>In particular, adding new benchmarks that are useful to the scientific community is welcome!</p>"},{"location":"contributing/#spack-package","title":"Spack package","text":"<p>Before adding a new benchmark, make sure the application is available in Spack.  If it is not, you can read the Spack Package Creation Tutorial to contribute a new recipe to build the application.</p> <p>While we encourage users to contribute all Spack recipes upstream, we have a custom repo for packages not yet ready to be contributed to the main Spack repository. This is in the <code>spack/repo</code> directory, create a subdirectory inside <code>spack/repo/packages</code> with the name of the package you want to add, and place into it the <code>package.py</code> Spack recipe. On supported HPC systems, this repo is automatically added to the provided Spack environments.</p> <p>In Spack recipes, please avoid cloning the head of a branch. The state of the branch at the time it was cloned will not get recorded by Spack or ReFrame which leads to  issues with reproducibility. It is strongly recommended to only build tagged versions of  packages with Spack.</p>"},{"location":"contributing/#reframe-benchmark","title":"ReFrame benchmark","text":"<p>New benchmarks should be added in the <code>apps/</code> directory, under the specific application subdirectory.  Please, add also a <code>README.md</code> file explaining what the application does and how to run the benchmarks in the same directory. Then, link the <code>README.md</code> file under <code>nav: Supported Benchmarks:</code> in the mkdocs documentation config.</p> <p>For writing ReFrame benchmarks you can read the documentation, in particular</p> <ul> <li>ReFrame Tutorials</li> <li>Regression Test API</li> </ul> <p>but you can also have a look at the sombrero example.</p> <p>For GPU benchmarks you need to</p> <ul> <li>set <code>valid_systems = ['+gpu']</code></li> <li>set the <code>num_gpus_per_node</code> attribute,</li> <li>and add the key <code>gpu</code> to the <code>extra_resources</code> dictionary to request the appropriate number of GPUs.</li> </ul> <p>For an example of a GPU benchmark take a look at OpenMM.</p>"},{"location":"contributing/#adding-new-systems","title":"Adding new systems","text":"<p>If you configure the framework on a HPC system that is not included in supported systems, please upen a pull request to upload the configuration so that other users can benefit from it. To add a new system, consider the following items</p> <ul> <li>ReFrame configuration</li> <li>Spack configuration</li> <li>Documentation</li> </ul>"},{"location":"install/","title":"Installation","text":""},{"location":"install/#excalibur-tests","title":"Excalibur-tests","text":""},{"location":"install/#requirements","title":"Requirements","text":"<p>Python version 3.7 or later is required. </p>"},{"location":"install/#virtual-environments","title":"Virtual environments","text":"<p>On most systems, it is recommended to install  the package in a virtual environment. For example, using the python3  built-in virtual environment tool <code>venv</code>, create an environment called <code>my_environment</code> with</p> <pre><code>python3 -m venv ./my_environment\n</code></pre> <p>and activate it with</p> <pre><code>source ./my_environment/bin/activate\n</code></pre>"},{"location":"install/#installation_1","title":"Installation","text":"<p>First, clone the git repository</p> <pre><code>git clone https://github.com/ukri-excalibur/excalibur-tests.git\n</code></pre> <p>Install the excalibur-tests package and the necessary dependencies with <code>pip</code> by</p> <pre><code>pip install -e ./excalibur-tests\n</code></pre>"},{"location":"install/#notes","title":"Notes","text":"<p>The <code>-e/--editable</code> flag is recommended for two reasons.</p> <ul> <li>Spack installs packages in a <code>opt</code> directory under the spack environment. With <code>-e</code> the spack environment remains in your local directory and <code>pip</code> creates symlinks to it. Without <code>-e</code> spack will install packages inside your python environment.</li> <li>For development, the <code>-e</code> flag to <code>pip</code> links the installed package to the files in the local directory, instead of copying, to allow making changes to the installed package.</li> </ul> <p>Note that to use <code>-e</code> with a project configured with a <code>pyproject.toml</code> you need <code>pip</code> version 22 or later.</p>"},{"location":"install/#spack","title":"Spack","text":"<p>The <code>pip install</code> command will install a compatible version of ReFrame from PyPi. However, you will have to manually provide an installation of Spack.</p> <p>Spack is a package manager specifically designed for HPC facilities. In some HPC facilities there may be already a central Spack installation available. However, the version installed is most likely too old to support all the features used by this package. Therefore we recommend you install the latest version locally, following the instructions below.</p> <p>Follow the official instructions to install the latest version of Spack (summarised here for convenience, but not guaranteed to be up-to-date):</p>"},{"location":"install/#installation_2","title":"Installation","text":"<p>Git clone the spack repository <pre><code>git clone -c feature.manyFiles=true https://github.com/spack/spack.git\n</code></pre> Run spack setup script  <pre><code>source ./spack/share/spack/setup-env.sh\n</code></pre> Check spack is in <code>$PATH</code>, for example  <pre><code>spack --version\n</code></pre></p>"},{"location":"install/#version","title":"Version","text":"<p>We recommend using the latest spack version (develop). Stable spack releases starting from v0.20.0  are supported. Older versions of spack may work but are not regularly tested.</p>"},{"location":"install/#notes_1","title":"Notes","text":"<p>Note: if you have already installed spack locally and you want to upgrade to a newer version, you might first have to clear the cache to avoid conflicts: <code>spack clean -m</code></p>"},{"location":"setup/","title":"Configuration","text":""},{"location":"setup/#pre-configured-systems","title":"Pre-configured systems","text":"<p>A number of UK-based HPC systems that are part of the DiRAC and ExCALIBUR programs  have been pre-configured. See systems, or https://github.com/ukri-excalibur/excalibur-tests/tree/main/benchmarks/spack/ for a list.  On these systems the ReFrame configuration and Spack environment are included in the  <code>excalibur-tests</code> repository and all you need to do is point the framework to them.</p>"},{"location":"setup/#reframe","title":"ReFrame","text":"<p>You can point ReFrame to the provided config file by setting the <code>RFM_CONFIG_FILES</code> environment variable:</p> <pre><code>export RFM_CONFIG_FILES=\"&lt;path-to-excalibur-tests&gt;/benchmarks/reframe_config.py\"\n</code></pre> <p>If you want to use a different ReFrame configuration file, for example because you use a different system, you can set this environment variable to the path of that file.</p> <p>Note: in order to use the Spack build system in ReFrame, the <code>spack</code> executable must be in the <code>PATH</code> also on the compute nodes of a cluster, if you want to run your benchmarks on them. This is taken care of by adding it to your init file (see spack section below).</p> <p>However, you will also need to set the <code>RFM_USE_LOGIN_SHELL</code> environment variable  <pre><code>export RFM_USE_LOGIN_SHELL=\"true\"\n</code></pre> in order to make ReFrame use <code>!#/bin/bash -l</code> as  shebang line, which would load the user's init script.</p>"},{"location":"setup/#spack","title":"Spack","text":"<p>In order to use Spack in ReFrame,  the directory where the <code>spack</code> program is installed needs to be in the <code>PATH</code> environment variable. This is taken care of by the <code>setup-env.sh</code> script run in install. To have spack available in every session, you can have your shell init script (e.g. <code>.bashrc</code>) re-run it automatically, by adding the following lines to it: <pre><code>export SPACK_ROOT=\"/path/to/spack\"\nif [ -f \"${SPACK_ROOT}/share/spack/setup-env.sh\" ]; then\n    source \"${SPACK_ROOT}/share/spack/setup-env.sh\"\nfi\n</code></pre> replacing <code>/path/to/spack</code> with the actual path to your Spack installation.</p>"},{"location":"setup/#new-systems","title":"New systems","text":"<p>We need the ReFrame configuration and a Spack environment for a new system.</p>"},{"location":"setup/#reframe_1","title":"ReFrame","text":"<p>Add a new system to the ReFrame configuration in <code>benchmarks/reframe_config.py</code>.  Read ReFrame documentation about configuration for more details, or see the examples of the existing systems.  </p> <p>Note: you likely do not need to customise the programming environment in ReFrame, as we will use Spack as build system, which will deal with that.</p> <p>If available, the command <code>lscpu</code>, run on a compute node, is typically useful to get information about the CPUs, to be used in the <code>processor</code> item of the system configuration.  The numbers you need to watch out for are:</p> <ul> <li>\"CPU(s)\", (<code>num_cpus</code> in ReFrame configuration),</li> <li>\"Thread(s) per core\", (<code>num_cpus_per_core</code>),</li> <li>\"Socket(s)\", (<code>num_sockets</code>),</li> <li>\"Core(s) per socket\", (<code>num_cpus_per_socket</code>).</li> </ul>"},{"location":"setup/#spack_1","title":"Spack","text":"<p>When using Spack as build system, ReFrame needs a Spack environment to run its tests. Follow these steps to create a Spack environment for a new system:</p>"},{"location":"setup/#create-the-environment","title":"Create the environment","text":"<p><pre><code>spack env create --without-view -d /path/to/environment\n</code></pre> Remember to disable views with <code>--without-view</code> to avoid conflicts when installing incompatible packages in the same environment</p>"},{"location":"setup/#activate-the-environment","title":"Activate the environment","text":"<pre><code>spack env activate -d /path/to/environment\n</code></pre>"},{"location":"setup/#set-install_tree","title":"Set <code>install_tree</code>","text":"<pre><code>spack config add 'config:install_tree:root:$env/opt/spack'\n</code></pre>"},{"location":"setup/#add-compilers","title":"Add compilers","text":"<p>Make sure the compilers you want to add are in the <code>PATH</code> (e.g., load the relevant modules), then add them to the Spack environment with: <pre><code>spack compiler find\n</code></pre></p>"},{"location":"setup/#add-external-packages","title":"Add external packages","text":"<p>Add other packages (e.g., MPI): make sure the package you want to add are \"visible\" (e.g., load the relevant modules) and add them to the environment with <pre><code>spack external find PACKAGE-NAME ...\n</code></pre> where <code>PACKAGE-NAME ...</code> are the names of the corresponding Spack packages</p>"},{"location":"setup/#set-excalibur_spack_env-variable","title":"Set <code>EXCALIBUR_SPACK_ENV</code> variable","text":"<p>To use the new Spack environment in ReFrame, set the environment variable <code>EXCALIBUR_SPACK_ENV</code> to the path of the directory where the environment is, i.e. <pre><code>export EXCALIBUR_SPACK_ENV=/path/to/environment\n</code></pre> If this is not set, ReFrame will try to use the environment for the current system if known, otherwise it will automatically create a very basic environment (see Usage on unsupported systems.</p>"},{"location":"setup/#optional-make-manual-tweaks","title":"(optional) Make manual tweaks","text":"<p>If necessary, manually tweak the environment. For example, if there is already a global Spack available in the system, you can include its configuration files, or add its install trees as upstreams.</p>"},{"location":"setup/#optional-add-spack-repositories","title":"(optional) Add spack repositories","text":"<p>If you are using a custom repo for spack package recipes (see Spack package below), add it to the spack environment with <pre><code>spack -e /path/to/environment repo add /path/to/repo\n</code></pre></p>"},{"location":"systems/","title":"System-specific information","text":"<p>This framework strives to be as system-independent as possible, but there are some platform-specific details that you may need be aware of when running these benchmarks. Below we collect some information you may want to keep in mind on the different systems.</p>"},{"location":"systems/#archer2","title":"ARCHER2","text":""},{"location":"systems/#home-partition","title":"Home partition","text":"<p>ARCHER2 uses the standard Cray setup for which the home partition is not mounted on compute nodes, read Data management and transfer for more details. You likely want to install this benchmarking framework outside of your home directory, for example inside <code>/work/&lt;project code&gt;/&lt;project code&gt;/${USER}</code>, where <code>&lt;project code&gt;</code> is your project code.</p>"},{"location":"systems/#queue-options","title":"Queue options","text":"<p>When submitting jobs to compute nodes, you need to specify the job queue and maybe the project account. The former is specified by setting the Quality of Service, the latter is necessary if your user account is associated to multiple projects on ARCHER2 and you need to specify which one to use for the submitted jobs. We cannot automatically set these options for you because they are user-specific, but when you run a benchmark you can use the <code>-J</code>/<code>--job-option</code> flag to <code>reframe</code> to add new job options. Some examples:</p> <pre><code>reframe -c benchmarks/examples/sombrero -r --performance-report --system archer2 -J'--qos=serial'\nreframe -c benchmarks/examples/sombrero -r --performance-report --system archer2 -J'--qos=short' -J'--account=t01'\n</code></pre>"},{"location":"systems/#controlling-cpu-frequency","title":"Controlling CPU frequency","text":"<p>ARCHER2 allows choosing the CPU frequency during jobs by setting the environment variable <code>SLURM_CPU_FREQ_REQ</code> to specific values. In ReFrame v3 the list of environment variables set by the framework is held by the dictionary attribute called <code>env_vars</code>, and you can initialise it on the command line when running a benchmark with <code>-S</code>/<code>--setvar</code>. For more details, see Setting environment variables in usage. For example, to submit a benchmark using the lowest CPU frequency (1.5 GHz) you can use</p> <pre><code>reframe -c benchmarks/examples/sombrero -r --performance-report --system archer2 -J'--qos=serial' -S env_vars=SLURM_CPU_FREQ_REQ:1500000\n</code></pre>"},{"location":"systems/#using-python","title":"Using python","text":"<p>ARCHER2 is a Cray system, and they recommend using a cray optimised python version. The HPE Cray Python distribution can be loaded using <code>module load cray-python</code>. This is necessary to pip install excalibur-tests following the instructions in install.</p>"},{"location":"systems/#spack-install-path","title":"Spack install path","text":"<p>Spack has a limitation of 127 characters on the length of the path of the install tree. Because the path to the work directory on Archer2 is fairly long, and pip by default installs into <code>&lt;path/to/virtual/environment&gt;/pythonx.x/site-packages/</code>we may exceed the limit when installing to the default directory. If you see an error in ReFrame beginning with</p> <pre><code>==&gt; Error: SbangPathError: Install tree root is too long.\n</code></pre> <p>A possible work-around is to provide a shorter installation path to <code>pip</code>. Pass the installation path to <code>pip install</code> using <code>--target</code>, for example, <code>pip install --target = /work/&lt;project_code&gt;/&lt;project_code&gt;/&lt;username&gt;/pkg .</code>. Then add  the <code>bin</code> subdirectory to <code>$PATH</code>, for example, <code>export PATH = /work/&lt;project_code&gt;/&lt;project_code&gt;/&lt;username&gt;/pkg/bin:$PATH</code>.</p>"},{"location":"systems/#csd3","title":"CSD3","text":""},{"location":"systems/#queue-options_1","title":"Queue options","text":"<p>When submitting jobs to compute nodes, you need to specify the job queue, with the <code>--account</code> option to the scheduler. To do this, when you run a benchmark you can use the <code>-J</code>/<code>--job-option</code> flag to <code>reframe</code> to specify the account, for example:</p> <pre><code>reframe -c benchmarks/examples/sombrero -r --performance-report --system csd3-skylake:compute-node -J'--account=&lt;ACCOUNT&gt;'\n</code></pre> <p>where <code>&lt;ACCOUNT&gt;</code> is the project you want to charge. You can see the account balance of your projects with the <code>mybalance</code> command.</p>"},{"location":"systems/#cosma8","title":"Cosma8","text":""},{"location":"systems/#queue-options_2","title":"Queue options","text":"<p>When submitting jobs to compute nodes, you need to specify the job queue, with the <code>--account</code> option to the scheduler. To do this, when you run a benchmark you can use the <code>-J</code>/<code>--job-option</code> flag to <code>reframe</code> to specify the account, for example:</p> <pre><code>reframe -c benchmarks/examples/sombrero -r --performance-report --system cosma8:compute-node -J'--account=&lt;ACCOUNT&gt;'\n</code></pre> <p>where <code>&lt;ACCOUNT&gt;</code> is the project you want to charge. DiRAC users can find the account codes they have access to on SAFE.</p>"},{"location":"systems/#dial2","title":"DIaL2","text":""},{"location":"systems/#queue-options_3","title":"Queue options","text":"<p>When submitting jobs to compute nodes, you need to specify the job queue, with the <code>--account</code> option to the scheduler. To do this, when you run a benchmark you can use the <code>-J</code>/<code>--job-option</code> flag to <code>reframe</code> to specify the account, for example:</p> <pre><code>reframe -c benchmarks/examples/sombrero -r --performance-report --system dial2:compute-node -J'--account=&lt;ACCOUNT&gt;'\n</code></pre> <p>where <code>&lt;ACCOUNT&gt;</code> is the project you want to charge.</p> <p>Note: for exclusive access require to pass -J='-l naccesspolicy=SINGLEJOB -n'</p>"},{"location":"systems/#dial3","title":"DIaL3","text":""},{"location":"systems/#queue-options_4","title":"Queue options","text":"<p>When submitting jobs to compute nodes, you need to specify the job queue, with the <code>--account</code> option to the scheduler. To do this, when you run a benchmark you can use the <code>-J</code>/<code>--job-option</code> flag to <code>reframe</code> to specify the account, for example:</p> <pre><code>reframe -c benchmarks/examples/sombrero -r --performance-report --system dial3:compute-node -J'--account=&lt;ACCOUNT&gt;'\n</code></pre> <p>where <code>&lt;ACCOUNT&gt;</code> is the project you want to charge.</p>"},{"location":"systems/#isambard-2","title":"Isambard 2","text":""},{"location":"systems/#multi-architecture-comparison-system-macs-partition","title":"Multi-Architecture Comparison System (MACS) partition","text":""},{"location":"systems/#compilation-on-compute-nodes","title":"Compilation on compute nodes","text":"<p>Login nodes on the Isambard 2 MACS partition have Intel \"Broadwell\" CPUs, but most of the compute nodes use CPUs of different microarchitecture, which means that you cannot directly compile optimised code for the compute nodes with Spack while on the login nodes. To run compilation on the compute node, you have to set the attribute <code>build_locally</code> to <code>false</code> with <code>-S build_locally=false</code>, for example:</p> <pre><code>reframe -c benchmarks/examples/sombrero -r --performance-report --system isambard-macs:cascadelake -S build_locally=false\nreframe -c benchmarks/examples/sombrero -r --performance-report --system isambard-macs:rome -S build_locally=false\n</code></pre> <p>You may also need to compile GPU applications on the compute nodes, as the login node does not have any GPUs (this really depends on the build system of the application at hand, whether it needs access to a GPU during the build or it is sufficient to have the GPU toolkit available).</p>"},{"location":"systems/#myriad-and-kathleen","title":"Myriad and Kathleen","text":""},{"location":"systems/#python3-module","title":"Python3 module","text":"<p>The only default Python in the system is currently Python 2.7, but this may change in the future. We require Python v3.7 or later so you need to have <code>python3</code> available. This is provided by the <code>python3</code> module in the system. The <code>python3/recommended</code> module on myriad is built with an incompatible version of <code>openssl</code> for ReFrame. The easiest thing to do is to add the lines</p> <pre><code>module load python3/3.11\nexport RFM_USE_LOGIN_SHELL=\"True\"\n</code></pre> <p>to your shell init script (e.g. <code>~/.bashrc</code>). The second line tells ReFrame to always load the shell init script when running the jobs, so that the Python3 module is available also during the jobs, to run Spack.</p>"},{"location":"systems/#temporary-directory-for-building-packages-with-spack","title":"Temporary directory for building packages with Spack","text":"<p>Before moving it to the final installation place, Spack builds software in a temporary directory. By default on Myriad this is <code>/tmp</code>, but this directory is shared with other users and its partition is relatively small, so that building large software may always end up filling the entire disk, resulting in frequent <code>No space left on device</code> errors. To work around this issue you can use as temporary directory the one pointed to by <code>XDG_RUNTIME_DIR</code>, which use a larger partition, reserved only to your user. Note that this directory is automatically cleaned up after you log all of your sessions out of the system. You can add the following line to your shell init script (e.g., <code>~/.bashrc</code>) to make <code>TMPDIR</code> use <code>XDG_RUNTIME_DIR</code>, unless otherwise set:</p> <pre><code>export TMPDIR=\"${TMPDIR:-${XDG_RUNTIME_DIR:-/tmp}}\"\n</code></pre>"},{"location":"systems/#tursa","title":"Tursa","text":""},{"location":"systems/#queue-options_5","title":"Queue options","text":"<p>When submitting jobs to compute nodes, you need to specify the job queue, with the <code>--account</code> option to the scheduler. To do this, when you run a benchmark you can use the <code>-J</code>/<code>--job-option</code> flag to <code>reframe</code> to specify the account, for example:</p> <pre><code>reframe -c benchmarks/examples/sombrero -r --performance-report --system tursa:compute-node -J'--account=&lt;ACCOUNT&gt;'\n</code></pre> <p>where <code>&lt;ACCOUNT&gt;</code> is the project you want to charge.</p>"},{"location":"use/","title":"Usage","text":""},{"location":"use/#running-benchmarks","title":"Running benchmarks","text":"<p>Once you have set up Spack and ReFrame, you can execute a benchmark with</p> <pre><code>reframe -c benchmarks/apps/BENCH_NAME -r\n</code></pre> <p>where <code>benchmarks/apps/BENCH_NAME</code> is the directory where the benchmark is.  The command above assumes you have the program <code>reframe</code> in your PATH.  If you have followed the instructions to install using <code>pip</code> into the default directory, it should have been automatically added. If it is not the case, call <code>reframe</code> with its relative or absolute path.</p> <p>For example, to run the Sombrero benchmark in the <code>benchmarks/apps/sombrero</code> directory you can use</p> <pre><code>reframe -c benchmarks/apps/sombrero -r\n</code></pre>"},{"location":"use/#setting-reframe-command-line-options","title":"Setting ReFrame command line options","text":"<p>ReFrame supports a variety of command-line options that can be useful, or sometimes necessary.</p>"},{"location":"use/#system-specific-options","title":"System-specific options","text":"<p>While the aim is to automate as much system-specific configuration as possible, there are some options that have to be provided by the user, such as accounting details, and unfortunately the syntax can vary. See systems for information about the use of this framework on specific systems.</p>"},{"location":"use/#performance-report","title":"Performance report","text":"<p>You can use the <code>--performance-report</code> command-line option to ReFrame to get a nicely formatted performance report after the benchmark has completed.</p>"},{"location":"use/#selecting-spack-build-spec","title":"Selecting Spack build spec","text":"<p>For benchmarks that use the Spack build system, the tests define a default Spack specification to be installed in the environment, but users can change it when invoking ReFrame on the command line with the <code>-S</code> option to set the <code>spack_spec</code> variable:</p> <pre><code>reframe -c benchmarks/apps/sombrero -r --performance-report -S spack_spec='sombrero@2021-08-16%intel'\n</code></pre>"},{"location":"use/#selecting-system-and-queue-access-options","title":"Selecting system and queue access options","text":"<p>The provided ReFrame configuration file contains the settings for multiple systems.  If you use it, the automatic detection of the system may fail, as some systems may use clashing hostnames.  To avoid this, you can use the flag <code>--system NAME:PARTITION</code> to specify the system (and optionally the partition) to use.</p> <p>Additionally, if submitting jobs to the compute nodes requires additional options, like for example the resource group you belong to (for example <code>--account=...</code> for Slurm), you have to pass the command line flag <code>--job-option=...</code> to <code>reframe</code> (e.g., <code>--job-option='--account=...'</code>).</p>"},{"location":"use/#setting-environment-variables","title":"Setting environment variables","text":"<p>All the built-in fields of ReFrame regression classes can be set on a per-job basis using the <code>-S</code> command-line option. One useful such field is <code>env_vars</code>, which controls the environment variables used in a job. The syntax to set dictionary items, like for <code>env_vars</code>, is a comma-separated list of <code>key:value</code> pairs: <code>-S dict=key_1:value_1,key_2:value_2</code>. For example</p> <pre><code>reframe -c benchmarks/apps/sombrero -r --performance-report -S env_vars=OMP_PLACES:threads\n</code></pre> <p>runs the <code>benchmarks/apps/sombrero</code> benchmark setting the environment variable <code>OMP_PLACES</code> to <code>threads</code>.</p>"},{"location":"use/#usage-on-unsupported-systems","title":"Usage on unsupported systems","text":"<p>The configuration provided in <code>reframe_config.py</code> lets you run the benchmarks on pre-configured HPC systems.  However you can use this framework on any system by choosing the \"default\" system with <code>--system default</code>, or by using your own ReFrame configuration.  You can use the \"default\" system to run benchmarks in ReFrame without using a queue manager or an MPI launcher (e.g. on a personal workstation).</p> <p>If you choose the \"default\" system and a benchmark using the Spack build system, a new empty Spack environment will be automatically created in <code>benchmarks/spack/default</code> when ReFrame is launched for the first time. You should populate the environment with the packages already installed on your system before running Spack to avoid excessively rebuilding system packages. See setup for instructions on how to set up a Spack environment. In particular, make sure that at least a compiler and an MPI library are added into the environment. After the Spack environment is set up, tell ReFrame to use it by setting the environment variable <code>EXCALIBUR_SPACK_ENV</code>, as described in setup.</p>"},{"location":"use/#selecting-multiple-benchmarks","title":"Selecting multiple benchmarks","text":"<p>ReFrame tests may contain tags that allow the user to select which tests to run. These can be leveraged to defined sets of benchmarks. To run all tests in a directory, pass the <code>-R</code> flag to ReFrame. Then filter down to a specific tag by passing the <code>-t</code> flag.</p> <p>For example, the tag \"example\" is defined in the sombrero example. To select the sombrero example out of all benchmarks, run</p> <pre><code>reframe -c benchmarks/ -R -r -t example\n</code></pre> <p>Tests can contain multiple tags. To create a custom set of benchmarks, add a new tag to the tests you want to include in the set.</p>"},{"location":"apps/","title":"Supported benchmarks","text":"<p>This directory contains the benchmarks currently supported by the project. More can be added by opening a Pull Request following the guidance in contributing.</p>"},{"location":"apps/babelstream/","title":"BabelStream benchmarks","text":"<p>BabelStream </p> <p>Measure memory transfer rates to/from global device memory on GPUs. This benchmark is similar in spirit, and based on, the STREAM benchmark [1] for CPUs. Unlike other GPU memory bandwidth benchmarks this does not include the PCIe transfer time. There are multiple implementations of this benchmark in a variety of programming models. This code was previously called GPU-STREAM.</p>"},{"location":"apps/babelstream/#usage","title":"Usage","text":"<p>From the top-level directory of the repository, you can run the benchmarks with</p> <pre><code>reframe -c benchmarks/apps/babelstream -r  --tag &lt;TAG&gt; --system=&lt;ENV:PARTITION&gt; -Sbuild_locally=false -Sspack_spec='babelstream +tag &lt;extra flags&gt;'\n</code></pre>"},{"location":"apps/babelstream/#filtering-the-benchmarks","title":"Filtering the benchmarks","text":"<p>The Spack directives for the babelstream could be found here You can run individual benchmarks with the <code>--tag</code> option:</p> <ul> <li><code>omp</code>to run the <code>OpenMP</code> benchmark.</li> <li><code>ocl</code> to run the <code>OpenCL</code> benchmark.</li> <li><code>std</code> to run the <code>STD</code> benchmark.</li> <li><code>std20</code>to run the <code>STD20</code> benchmark.</li> <li><code>hip</code> to run the <code>HIP</code> benchmark.</li> <li><code>cuda</code>to run the <code>CUDA</code> benchmark.</li> <li><code>kokkos</code> to run the <code>Kokkos</code> benchmark.</li> <li><code>sycl</code> to run the <code>SYCL</code> benchmark.</li> <li><code>sycl2020</code> to run the <code>SYCL2020</code> benchmark.</li> <li><code>acc</code> to run the <code>ACC</code> benchmark.</li> <li><code>raja</code> to run the <code>RAJA</code> benchmark.</li> <li><code>tbb</code> to run the <code>TBB</code> benchmark.</li> <li><code>thrust</code> to run the <code>THRUST</code> benchmark,</li> </ul> <p>Examples:</p> <pre><code>reframe -c benchmarks/apps/babelstream -r --tag omp --system=isambard-macs:volta -S build_locally=false -S spack_spec='babelstream%gcc@9.2.0 +omp cuda_arch=70'\nreframe -c benchmarks/apps/babelstream -r --tag tbb --system=isambard-macs:cascadelake -S build_locally=false -S spack_spec='babelstream@develop +tbb'\nreframe -c benchmarks/apps/babelstream -r --tag cuda --system=isambard-macs:volta -S build_locally=false -S spack_spec='babelstream@develop%gcc@9.2.0 +cuda cuda_arch=70'\n</code></pre>"},{"location":"apps/babelstream/#setting-the-number-of-threads-and-mpi-processes","title":"Setting the number of threads and MPI processes","text":"<p>By default, these benchmarks will use</p> <ul> <li>[<code>num_gpus_per_node</code>](https://reframe-hpc.readthedocs.io/en/stable/regression_test_api.html#reframe.core.pipeline.RegressionTest.num_gpus_per_node: This value is by default 1 for the benchmarks requiring GPU. (e.g. CUDA,HIP) </li> </ul> <p>You can override the value of this variable from the command line with the <code>--setvar</code> option, for example</p> <pre><code>reframe -c benchmarks/apps/babelstream -r --tag cuda --system=isambard-macs:volta -S build_locally=false -S spack_spec='babelstream@develop%gcc@9.2.0 +cuda cuda_arch=70' --setvar=num_gpus_per_node=2\n</code></pre> <p>Note: you're responsible for overriding this variable in a consistent way, so that, for example, <code>num_gpus_per_node</code> doesn't exceed the number of total GPUs runnable on each node.</p>"},{"location":"apps/babelstream/#figure-of-merit","title":"Figure of merit","text":"<p>The figure of merit captured by these benchmarks is the bandwidth. For example, if the output of the program is</p> <pre><code>BabelStream\nVersion: 4.0\nImplementation: OpenMP\nRunning kernels 100 times\nPrecision: double\nArray size: 268.4 MB (=0.3 GB)\nTotal size: 805.3 MB (=0.8 GB)\nFunction    MBytes/sec  Min (sec)   Max         Average     \nCopy        91018.241   0.00590     0.01087     0.00721     \nMul         80014.622   0.00671     0.01173     0.00837     \nAdd         92644.967   0.00869     0.01636     0.01121     \nTriad       93484.396   0.00861     0.01416     0.01142     \nDot         114688.364  0.00468     0.01382     0.00707\n</code></pre> <p>the output numbers </p> <pre><code>Copy : 91018.241\nMul : 80014.622\nAdd : 92644.967 \nTriad : 93484.396\nDot : 114688.364\n</code></pre> <p>will be captured.</p>"},{"location":"apps/cp2k/","title":"CP2K benchmarks","text":"<p>CP2K is a quantum chemistry and solid state physics software package. This directory includes the <code>H2O-64</code>, <code>H20-256</code>, and <code>LiH_HFX</code> CP2K benchmarks based on ARCHER 2 HPC benchmarks.</p>"},{"location":"apps/cp2k/#usage","title":"Usage","text":"<p>From the top-level directory of the repository, you can run the benchmarks with</p> <pre><code>reframe -c benchmarks/apps/cp2k -r --performance-report\n</code></pre>"},{"location":"apps/cp2k/#filtering-the-benchmarks","title":"Filtering the benchmarks","text":"<p>By default all benchmarks will be run. You can run individual benchmarks with the <code>--tag</code> option:</p> <ul> <li><code>h2o-64</code> to run the <code>H2O-64</code> benchmark,</li> <li><code>h2o-256</code> to run the <code>H2O-256</code> benchmark,</li> <li><code>lih-hfx</code> to run the <code>LiH_HFX</code> benchmark.</li> </ul> <p>Examples:</p> <pre><code>reframe -c benchmarks/apps/cp2k -r --performance-report --tag h2o-64\nreframe -c benchmarks/apps/cp2k -r --performance-report --tag h2o-256\nreframe -c benchmarks/apps/cp2k -r --performance-report --tag lih-hfx\n</code></pre>"},{"location":"apps/cp2k/#setting-the-number-of-threads-and-mpi-processes","title":"Setting the number of threads and MPI processes","text":"<p>By default, these benchmarks will use</p> <ul> <li><code>num_cpus_per_task</code>:   2</li> <li><code>num_tasks</code>:   <code>current_partition.processor.num_cpus // min(1, current_partition.processor.num_cpus_per_core) // num_cpus_per_task</code></li> <li><code>num_tasks_per_node</code>: <code>current_partition.processor.num_cpus // num_cpus_per_task</code></li> </ul> <p>You can override the values of these variables from the command line with the <code>--setvar</code> option, for example</p> <pre><code>reframe -c benchmarks/apps/cp2k -r --performance-report --setvar=num_cpus_per_task=4 --setvar=num_tasks=16\n</code></pre> <p>Note: you're responsible for overriding these variables in a consistent way, so that, for example, <code>num_tasks_per_node</code> doesn't exceed the number of total tasks runnable on each node.</p>"},{"location":"apps/cp2k/#figure-of-merit","title":"Figure of merit","text":"<p>The figure of merit captured by these benchmarks is the maximum total CP2K time. For example, if the output of the program is</p> <pre><code> -------------------------------------------------------------------------------\n -                                                                             -\n -                                T I M I N G                                  -\n -                                                                             -\n -------------------------------------------------------------------------------\n SUBROUTINE                       CALLS  ASD         SELF TIME        TOTAL TIME\n                                MAXIMUM       AVERAGE  MAXIMUM  AVERAGE  MAXIMUM\n CP2K                                 1  1.0    0.178    0.295  200.814  200.816\n qs_energies                          1  2.0    0.000    0.000  200.091  200.093\n scf_env_do_scf                       1  3.0    0.000    0.000  198.017  198.018\n qs_ks_update_qs_env                  8  5.0    0.000    0.000  161.422  161.440\n rebuild_ks_matrix                    7  6.0    0.000    0.000  161.419  161.437\n qs_ks_build_kohn_sham_matrix         7  7.0    0.001    0.001  161.419  161.437\n hfx_ks_matrix                        7  8.0    0.000    0.000  154.464  154.495\n</code></pre> <p>the number <code>200.816</code> will be captured.</p>"},{"location":"apps/grid/","title":"GRID","text":"<p>ReFrame benchmarks for the GRID code, a data parallel C++ mathematical object library.</p>"},{"location":"apps/grid/#usage","title":"Usage","text":"<p>From the top-level directory of the repository, you can run the benchmarks with</p> <pre><code>reframe -c benchmarks/apps/grid -r --performance-report\n</code></pre>"},{"location":"apps/grid/#filtering-the-benchmarks","title":"Filtering the benchmarks","text":"<p>You can run individual benchmarks with the <code>--tag</code> option.  At the moment we have the following tags:</p> <ul> <li><code>ITT</code> to run the <code>Benchmark_ITT</code> application.</li> </ul> <p>Examples:</p> <pre><code>reframe -c benchmarks/apps/grid -r --performance-report --tag ITT\n</code></pre>"},{"location":"apps/grid/#options-memory-number-of-threads-and-mpi-processes","title":"Options (memory, number of threads and MPI processes)","text":"<p>There are some options you can set to control the settings of the benchmark. These are the currently available options, with their default values:</p> <ul> <li><code>mpi</code>: <code>'1.1.1.1'</code>.  This is the string to pass to the benchmarking applications with the   <code>--mpi</code> flag.  This will also automatically set the ReFrame variable   <code>num_tasks</code></li> <li><code>num_cpus_per_task</code>:   <code>current_partition.processor.num_cpus // min(1, current_partition.processor.num_cpus_per_core)</code></li> <li><code>num_tasks_per_node</code>:   <code>current_partition.processor.num_cpus // num_cpus_per_task</code></li> <li><code>shm</code>: <code>1024</code>.  This is the size of the shared memory used by the benchmark, in MiB, as an   integer.</li> </ul> <p>You can override the values of these variables from the command line with the <code>--setvar</code> option, for example</p> <pre><code>reframe -c benchmarks/apps/grid -r --performance-report --setvar=mpi='2.2.1.1' --setvar=num_cpus_per_task=12\nreframe -c benchmarks/apps/grid -r --performance-report --setvar=mpi='4.4.4.4' --setvar=shm=4096\n</code></pre> <p>Note: you're responsible for overriding these variables in a consistent way, so that, for example, <code>num_tasks_per_node</code> doesn't exceed the number of total tasks runnable on each node.</p>"},{"location":"apps/grid/#figure-of-merit","title":"Figure of merit","text":"<p>If the output of the program contains</p> <pre><code>Grid : Message : 380809 ms :  Comparison point  result: 143382.7 Mflop/s per node\n</code></pre> <p>the number <code>143382.7</code> will be captured as figure of merit.</p>"},{"location":"apps/hpcg/","title":"HPCG benchmarks","text":"<p>These are based upon the HPCG Conjugate Gradient solver benchmark. At the time of writing, there are three benchmarks in the suite: the original implementation, one which solves the same problem with a hard-coded stencil, and one  which solves a different problem with an LFRic stencil and data.</p>"},{"location":"apps/hpcg/#usage","title":"Usage","text":"<p>From the top-level directory of the repository, you can run the benchmarks with</p> <pre><code>reframe -c benchmarks/apps/hpcg -r --performance-report\n</code></pre> <p>You can use the <code>-n/--name</code> argument to pick <code>HPCG_Original / HPCG_Stencil / HPCG_LFRic</code> to select a particular benchmark. Alternatively, if you want to compare the two implementations of the 27 point stencil problem (Original and Stencil), you can filter by tag <code>-t 27pt_stencil</code>.</p> <p>This app is currently intended to be parallelized with MPI, and it is recommended to use the <code>--system</code> argument to pick up the appropriate hardware details, as well as Spack libraries.</p>"},{"location":"apps/hpgmg/","title":"HPGMG","text":"<p>See hpgmg</p>"},{"location":"apps/hpl/","title":"High Performance Linpack","text":"<p>Run Intel optimised HPL tests on one and all nodes.</p>"},{"location":"apps/hpl/#requirements","title":"Requirements","text":""},{"location":"apps/hpl/#intel-hpl","title":"Intel HPL","text":"<p>This uses the pre-built binaries supplied with Intel's MKL package. Note: Intel MPI is also required.</p> <p>By default the <code>intel-mkl</code> and <code>intel-mpi</code> Spack recipes will be used. If these packages are already available on the system you are using and the Spack environment knows about them, the system packages will be automatically used, otherwise Spack will download and install them for you.</p> <p>If you want to use the oneAPI distribution of MKL and MPI, pass <code>--setvar spack_spec=\"intel-oneapi-mkl ^intel-oneapi-mpi\"</code> as additional argument to the ReFrame invocation (see below). As usual, if these packages are available in the system and the Spack environment knows about them, those packages will be used.</p>"},{"location":"apps/hpl/#hpldat-configuration-files","title":"<code>HPL.dat</code> configuration files","text":"<p>Appropriate <code>HPL.dat</code> configuration files must be generated and placed in <code>&lt;repo_root&gt;/benchmarks/apps/hpl/&lt;sysname&gt;/&lt;number of tasks&gt;</code>, if not already available. ReFrame will copy these files into the staging directories before running a test, so changes made to these files will persist and apply to the next run.</p> <p>Hints:</p> <ul> <li>Generally, set PxQ to equal number of nodes, with P equal or smaller than Q (as using 1x MPI rank per node)</li> <li>Select problem size N to use e.g. 80% of total memory</li> <li>Check Intel documentation to select appropriate block size NB</li> <li>When running, check on a single node that <code>pstree</code> and <code>top</code> appear as expected.</li> </ul> <p>Note: not all systems have appropriate input data, or not for the number of tasks you want to run, so you may have to create the <code>HPL.DAT</code> file yourself.</p> <p>If you want to use an <code>HPL.dat</code> file in a different directory, you can pass <code>--setvar config_dir=&lt;DIRECTORY&gt;</code> as additional argument to the ReFrame invocation (see below), where <code>&lt;DIRECTORY&gt;</code> is the absolute path of the directory where <code>HPL.dat</code> is.</p>"},{"location":"apps/hpl/#running-tests","title":"Running tests","text":"<p>Run using e.g.:</p> <pre><code>reframe -c benchmarks/apps/hpl --run --performance-report\n</code></pre> <p>You can set the number of nodes and tasks per node to use by setting the following variables:</p> <ul> <li><code>num_tasks_per_node</code> (default = 1)</li> <li><code>num_tasks</code> (default = 1)</li> </ul> <p>For example</p> <pre><code>reframe -c benchmarks/apps/hpl --run --performance-report --setvar num_tasks=4 # 4 MPI ranks\nreframe -c benchmarks/apps/hpl --run --performance-report --setvar num_tasks=8 --setvar num_tasks_per_node=2 # 8 MPI ranks, 2 for each node (for a total of 4 nodes)\n</code></pre>"},{"location":"apps/hpl/#outputs","title":"Outputs","text":"<p>The ReFrame performance variable is:</p> <ul> <li><code>Gflops</code>: The performance.</li> </ul>"},{"location":"apps/imb/","title":"Intel MPI Benchmarks","text":"<p>https://software.intel.com/en-us/imb-user-guide</p> <p>Builds automatically using spack.</p> <p>Runs the following MPI1 tests using Intel MPI and OpenMPI:</p> <ul> <li>PingPong (latency/bandwidth) on 2 nodes using 1 process per node</li> <li>Uniband and Biband (bandwidth) using a range of processes from 2 up to 256 using default task pinning (Fill up nodes one by one)</li> </ul> <p>The following tags are defined:</p> <ul> <li>Test mode, one of \"pingpong\", \"biband\", \"uniband\".</li> <li>MPI implementation, one of \"openmpi\", \"intel-mpi\"</li> </ul>"},{"location":"apps/omb/","title":"OSU Micro-Benchmarks","text":"<p>http://mvapich.cse.ohio-state.edu/static/media/mvapich/README-OMB.txt</p> <p>The following tests are run (extracted performance variables described in brackets):</p> <p>On 2x nodes using 1x process per node:</p> <ul> <li><code>osu_bw</code> - bandwidth (max value over all message sizes)</li> <li><code>osu_latency</code> - latency (min value over all message sizes)</li> <li><code>osu_bibw</code> - bi-directional bandwidth (max value over all message sizes)</li> </ul> <p>On 2x nodes using as many processes per node as there are physical cores:</p> <ul> <li><code>osu_allgather</code> - latency (mean value calculated at each message size over pairs, then min taken over all message sizes)</li> <li><code>osu_allreduce</code> - as above</li> <li><code>osu_alltoall</code> - as above</li> </ul> <p>The following tags are defined:</p> <ul> <li>Test name, as given above without the leading \"osu_\"</li> </ul>"},{"location":"apps/omb/#running","title":"Running","text":"<p>Run all tests using e.g.:</p> <pre><code>reframe -C reframe_config.py -c benchmarks/apps/omb/ --run --performance-report\n</code></pre> <p>Run only specified benchmark, by choosing the corresponding tag:</p> <pre><code>reframe -C reframe_config.py -c benchmarks/apps/omb/ --run --performance-report --tag alltoall\nreframe -C reframe_config.py -c benchmarks/apps/omb/ --run --performance-report --tag bw\n</code></pre>"},{"location":"apps/openmm/","title":"OpenMM benchmark","text":"<p>OpenMM is high-performance toolkit for molecular simulation. This directory includes a test based on the 1400k atom benchmark from the HECBioSim suite. Note: this benchmark can run only on systems with a CUDA GPU.</p>"},{"location":"apps/openmm/#usage","title":"Usage","text":"<p>From the top-level directory of the repository, you can run the benchmarks with</p> <pre><code>reframe -c benchmarks/apps/openmm -r --performance-report\n</code></pre>"},{"location":"apps/openmm/#figure-of-merit","title":"Figure of merit","text":"<p>The output of the program looks like</p> <pre><code>#\"Progress (%)\" \"Step\"  \"Potential Energy (kJ/mole)\"    \"Kinetic Energy (kJ/mole)\"      \"Total Energy (kJ/mole)\"        \"Temperature (K)\"       \"Speed (ns/day)\"        \"Time Remaining\"\n10.0%   1000    -15688785.887127012     3656752.4413931114      -12032033.445733901     301.1644297760901       0       --\n20.0%   2000    -15722326.52227436      3651648.2543405197      -12070678.26793384      300.7440568884525       8.58    2:41\n30.0%   3000    -15748457.618506134     3653282.2518931925      -12095175.366612941     300.8786303793008       8.6     2:20\n40.0%   4000    -15766187.389856085     3650127.3583686342      -12116060.03148745      300.6187982674595       8.6     2:00\n50.0%   5000    -15771978.47168088      3640930.7606806774      -12131047.711000202     299.86138082043146      8.61    1:40\n60.0%   6000    -15779433.041706115     3640669.6428865143      -12138763.398819601     299.8398755660168       8.65    1:19\n70.0%   7000    -15774388.543227583     3646512.6161559885      -12127875.927071594     300.3210937346243       8.67    0:59\n80.0%   8000    -15777731.520400822     3641287.017230322       -12136444.5031705       299.89072155441534      8.68    0:39\n90.0%   9000    -15784781.923775911     3647212.6162459007      -12137569.30753001      300.3787446506489       8.7     0:19\n100.0%  10000   -15794411.8787518       3646944.5551444986      -12147467.323607301     300.3566675562755       8.71    0:00\n</code></pre> <p>The figure of merit is the speed of the last step, in units of <code>ns/day</code>. In this example, the capture figure of merit is <code>8.71</code>.</p>"},{"location":"apps/ramses/","title":"Ramses","text":""},{"location":"apps/ramses/#prerequisites","title":"Prerequisites","text":"<p>This code is currently hosted on a private GitHub repo for the benchmarking purposes. If you want to run this benchmark you will first need to request access. Please speak to the RSE team at Leicester for access.</p> <p>This code requires the following input data.</p> <ul> <li><code>cosmo3d-IC-256.tar.gz</code></li> <li><code>cosmo3d-IC-322.tar.gz</code></li> <li><code>cosmo3d-IC-406.tar.gz</code></li> <li><code>cosmo3d-IC-512.tar.gz</code></li> </ul> <p>They are publicly available on zenodo.</p> <p>NB They will be automatically downloaded by reframe, but it takes roughly 15 mins at 5MB/s. They will only be downloaded once per run, but if you manually re-run tests you may prefer to use the following options <code>--restore-session</code> and <code>--keep-stage-files</code>.</p>"},{"location":"apps/ramses/#usage","title":"Usage","text":"<p>From the top-level directory of the repository, you can run the benchmarks with</p> <pre><code>reframe -c benchmarks/apps/ramses -r --performance-report\n</code></pre>"},{"location":"apps/ramses/#filtering-the-benchmarks","title":"Filtering the benchmarks","text":"<p>By default all benchmarks will be run. You can run individual benchmarks with the <code>--tag</code> option:</p> <ul> <li><code>weak</code> to run the weak scaling benchmarks</li> <li><code>strong</code> to run the strong scaling benchmarks</li> </ul> <p>Examples:</p> <pre><code>reframe -c benchmarks/apps/ramses -r --performance-report --tag weak\nreframe -c benchmarks/apps/ramses -r --performance-report --tag strong\n</code></pre>"},{"location":"apps/ramses/#compiler-support","title":"Compiler support","text":"<p>Currently, only the intel compiler is supported for this program.</p>"},{"location":"apps/sombrero/","title":"SOMBERO","text":"<p>SOMBRERO is a benchmarking utility  for high performance computing based on lattice field theory applications.</p> <p>SOMBRERO is composed of 6 similar benchmarks  that are based on different lattice field theories,  each one with a different arithmetic intensity  and a different compute/communication balance. Each benchmark consists of a fixed number (50) of iterations of the Conjugate Gradient algorithm, using the underlying Lattice Dirac operator  built in the relative theory.</p> <p>See the documentation  for more information.</p> <p>SOMBRERO uses a pure-mpi parallelisation. </p> <p>There are four benchmark cases that can be chosen  using the <code>--tag=&lt;TAG&gt;</code> command line option of <code>reframe</code>:</p> <ul> <li><code>mini</code>: A debug run, on a very small lattice, on 2 processes.</li> <li><code>ITT-sn</code>: A run on a single node, using all the cores in each node     (as described here).</li> <li><code>ITT-64n</code>: A run on 64 nodes, using all the cores in each node    (as described here).    The number of nodes used can be changed by setting the variable <code>num_nodes</code>,    for example <code>reframe ... -S num_nodes=48</code>.</li> <li><code>scaling</code>: A large benchmarking campaign, where of the benchmarks is launched               on a range of number of processes              (depending on the setup of the machine)              and 4 different lattice sizes               (details depend on how the cases are filtered). In all these cases, the benchmark for each theory is launched.</li> </ul> <p>The following performance variables are captured:</p> <ul> <li>'flops' : the computing performance (Gflop/second)</li> <li>'time' : time spent in the CG algorithm (seconds)</li> <li>'communicated': number of bytes communicated via MPI (bytes)</li> <li>'avg_arithmetic_intensity': average arithmetic intensity (from DRAM or L3) (Flops/byte)</li> <li>'computation/communication': the ratio of floating point operations                                 over the bytes communicated.</li> </ul>"},{"location":"apps/sphng/","title":"sphNG","text":""},{"location":"apps/sphng/#prerequisites","title":"Prerequisites","text":"<p>This code is currently hosted on a private GitHub repo for the benchmarking purposes. If you want to run this benchmark you will first need to request access. Please speak to the RSE team at Leicester for access.</p> <p>The main code is available at https://bitbucket.org/mrbate/sphng/src/master/g but the spack recipe is not currently set up to work with it. We are working on this and soon we will switch over to the actual version. This is because we need to be able to verify that the version we run is the same as the one already used for benchmarking.</p>"},{"location":"apps/sphng/#usage","title":"Usage","text":"<p>From the top-level directory of the repository, you can run the benchmarks with</p> <pre><code>reframe -c benchmarks/apps/sphng -r --performance-report\n</code></pre>"},{"location":"apps/sphng/#filtering-the-benchmarks","title":"Filtering the benchmarks","text":"<p>By default all benchmarks will be run. You can run individual benchmarks with the <code>--tag</code> option:</p> <ul> <li><code>single-node</code> to run benchmarks on a single node</li> </ul> <p>Examples:</p> <pre><code>reframe -c benchmarks/apps/sphng -r --performance-report --tag single-node\n</code></pre>"},{"location":"apps/sphng/#compiler-support","title":"Compiler support","text":"<p>Currently, only the intel compiler is supported for this program.</p>"},{"location":"apps/swift/","title":"Swift","text":"<p>See swift</p>"},{"location":"apps/trove/","title":"Trove","text":""},{"location":"apps/trove/#prerequisites","title":"Prerequisites","text":"<p>This code is currently hosted on a private GitHub repo for the benchmarking purposes. If you want to run this benchmark you will first need to request access. Please speak to the RSE team at Leicester for access.</p> <p>The main code is publicly available at https://github.com/Trovemaster/TROVE but the spack recipe is not currently set up to work with it. We are working on this and soon we will switch over to the public version. This is because we need to be able to verify that the version we run is the same as the one already used for benchmarking.</p>"},{"location":"apps/trove/#usage","title":"Usage","text":"<p>From the top-level directory of the repository, you can run the benchmarks with</p> <pre><code>reframe -c benchmarks/apps/trove -r --performance-report\n</code></pre>"},{"location":"apps/trove/#filtering-the-benchmarks","title":"Filtering the benchmarks","text":"<p>By default all benchmarks will be run. You can run individual benchmarks with the <code>--tag</code> option:</p> <ul> <li><code>12N</code> to run benchmarks w.r.t. N12.inp</li> </ul> <p>Examples:</p> <pre><code>reframe -c benchmarks/apps/trove -r --performance-report --tag 12N\n</code></pre>"},{"location":"apps/trove/#compiler-support","title":"Compiler support","text":"<p>Currently, only the intel compiler is supported for this program.</p>"},{"location":"apps/trove-pdsyev/","title":"Trove Pdsyev","text":""},{"location":"apps/trove-pdsyev/#prerequisites","title":"Prerequisites","text":"<p>This code is currently hosted on a private GitHub repo for the benchmarking purposes. If you want to run this benchmark you will first need to request access. Please speak to the RSE team at Leicester for access.</p> <p>The main code is publicly available at https://github.com/Trovemaster/PDSYEV but the spack recipe is not currently set up to work with it. We are working on this and soon we will switch over to the public version. This is because we need to be able to verify that the version we run is the same as the one already used for benchmarking.</p>"},{"location":"apps/trove-pdsyev/#usage","title":"Usage","text":"<p>From the top-level directory of the repository, you can run the benchmarks with</p> <pre><code>reframe -c benchmarks/apps/trove-pdsyev -r --performance-report\n</code></pre>"},{"location":"apps/trove-pdsyev/#filtering-the-benchmarks","title":"Filtering the benchmarks","text":"<p>By default all benchmarks will be run. You can run individual benchmarks with the <code>--tag</code> option:</p> <ul> <li><code>single-node</code> to run benchmarks on a single node</li> </ul> <p>Examples:</p> <pre><code>reframe -c benchmarks/apps/trove-pdsyev -r --performance-report --tag single-node\n</code></pre>"},{"location":"apps/trove-pdsyev/#compiler-support","title":"Compiler support","text":"<p>Currently, only the intel compiler is supported for this program.</p>"},{"location":"apps/wrf/","title":"Weather Research and Forecasting (WRF) Model","text":"<p>Results from WRF, the Weather Research &amp; Forecasting Model using the WRFV3 benchmarks:</p> <ul> <li>12km CONUS (medium-size case), tag <code>12km</code>: <p>48-hour, 12km resolution case over the Continental U.S. (CONUS) domain October 24, 2001 with a time step of 72 seconds. The benchmark period is hours 25-27 (3 hours), starting from a restart file from the end of hour 24.</p> </li> <li>2.5km CONUS (large case), tag <code>2.5km</code>: <p>Latter 3 hours of a 9-hour, 2.5km resolution case covering the Continental U.S. (CONUS) domain June 4, 2005 with a 15 second time step.  The benchmark period is hours 6-9 (3 hours), starting from a restart file from the end of the initial 6 hour period Descriptions from the above benchmark page.</p> </li> </ul> <p>The following performance variables are captured:</p> <ul> <li>'gflops': Gigaflops per second, calculated as described in the benchmark page, using the average time required per model timestep and the number of floating point operations required for the benchmark. The time required for each model timestep is reported by WRF itself.</li> </ul>"},{"location":"apps/wrf/#usage","title":"Usage","text":"<p>Run using e.g.:</p> <pre><code>reframe/bin/reframe -C reframe_config.py -c benchmarks/apps/wrf/ --run --performance-report\n</code></pre> <p>A precursor task automatically downloads the required benchmark files. This may take some time due to the files size.</p>"},{"location":"apps/wrf/#filtering-the-benchmark","title":"Filtering the benchmark","text":"<p>You can filter the benchmark to run by filtering by tag:</p> <pre><code># For the 12km data\nreframe/bin/reframe -c benchmarks/apps/wrf/ --run --performance-report --tag '12km'\n# For the 2.5km data\nreframe/bin/reframe -c benchmarks/apps/wrf/ --run --performance-report --tag '2.5km'\n</code></pre>"},{"location":"apps/wrf/#setting-the-number-of-threads-and-mpi-processes","title":"Setting the number of threads and MPI processes","text":"<p>By default, these benchmarks will use</p> <ul> <li><code>num_cpus_per_task</code>:   2</li> <li><code>num_tasks</code>:   <code>current_partition.processor.num_cpus // min(1, current_partition.processor.num_cpus_per_core) // num_cpus_per_task</code></li> <li><code>num_tasks_per_node</code>: <code>current_partition.processor.num_cpus // num_cpus_per_task</code></li> </ul> <p>You can override the values of these variables from the command line with the <code>--setvar</code> option, for example</p> <pre><code>reframe -c benchmarks/apps/wrf -r --performance-report --setvar=num_cpus_per_task=4 --setvar=num_tasks=16\n</code></pre> <p>Note: you're responsible for overriding these variables in a consistent way, so that, for example, <code>num_tasks_per_node</code> doesn't exceed the number of total tasks runnable on each node.</p>"},{"location":"post-processing/","title":"Post-processing","text":""},{"location":"post-processing/#benchmark-results-post-processing","title":"Benchmark Results Post-Processing","text":""},{"location":"post-processing/#overview","title":"Overview","text":"<p>The post-processing scripts provided with the ExCALIBUR tests package are intended to grant users a quick starting point for visualising benchmark results with basic graphs and tables. Their components can also be used inside custom users' scripts.</p> <p>There are four main post-processing components:</p>"},{"location":"post-processing/#perflog-parsing","title":"<code>Perflog parsing</code>","text":"<ul> <li>Data from benchmark performance logs are stored in a pandas DataFrame.</li> </ul>"},{"location":"post-processing/#data-filtering","title":"<code>Data filtering</code>","text":"<ul> <li>If more than one perflog is used for plotting, DataFrames from individual perflogs are concatenated together into one DataFrame.</li> <li>The DataFrame is then filtered, keeping only relevant rows and columns.</li> </ul>"},{"location":"post-processing/#data-transformation","title":"<code>Data transformation</code>","text":"<ul> <li>Axis value columns in the DataFrame are scaled according to user specifications.</li> </ul>"},{"location":"post-processing/#plotting","title":"<code>Plotting</code>","text":"<ul> <li>A filtered and transformed DataFrame is passed to a plotting script, which produces a graph and embeds it in a simple HTML file.</li> <li>Users may run the plotting script to generate a generic bar chart. Graph settings should be specified in a configuration YAML file.</li> </ul>"},{"location":"post-processing/#installation","title":"Installation","text":"<p>Post-processing is an optional dependency of the ExCALIBUR tests package, as it requires Python version 3.9 or later (while the base package requires Python version 3.7 or later).</p> <p>You can include post-processing in your <code>pip</code> installation of the package with the following command:</p> <pre><code>pip install -e .[post-processing]\n</code></pre>"},{"location":"post-processing/#usage","title":"Usage","text":""},{"location":"post-processing/#command-line","title":"Command line","text":"<pre><code>python post_processing.py log_path config_path [-p plot_type]\n</code></pre> <ul> <li><code>log_path</code> - Path to a perflog file, or a directory containing perflog files.</li> <li><code>config_path</code> - Path to a configuration file containing plot details.</li> <li><code>plot_type</code> - (Optional.) Type of plot to be generated. (<code>Note: only a generic bar chart is currently implemented.</code>)</li> </ul> <p>Run <code>post_processing.py -h</code> for more information (including debugging and file output flags).</p>"},{"location":"post-processing/#streamlit","title":"Streamlit","text":"<p>You may also run post-processing with Streamlit to interact with your plots:</p> <p><code>streamlit run streamlit_post_processing.py log_path -- [-c config_path]</code></p> <p>The config path is optional when running with Streamlit, as the UI allows you to create a new config on the fly. If you would still like to supply a config path, make sure to include <code>--</code> before any post-processing flags to indicate that the arguments belong to the post-processing script rather than Streamlit itself.</p>"},{"location":"post-processing/#configuration-structure","title":"Configuration Structure","text":"<p>Before running post-processing, create a config file including all necessary information for graph generation (you must specify at least plot title, x-axis, y-axis, and column types). See below for a template, an example, and some clarifying notes.</p> <ul> <li><code>title</code> - Plot title.</li> <li><code>x_axis</code>, <code>y_axis</code> - Axis information.<ul> <li><code>value</code> - Axis data points. Specified with a column name.</li> <li><code>units</code> - Axis units. Specified either with a column name or a custom label (may be null).</li> <li><code>scaling</code> - (Optional.) Scale axis values by either a column or a custom value.</li> <li><code>sort</code> - (Optional.) Sort categorical x-axis in descending order (otherwise values are sorted in ascending order by default).</li> </ul> </li> <li><code>filters</code> - (Optional.) Filter data rows based on specified conditions. (Specify an empty list if no filters are required.)<ul> <li><code>and</code> - Filter mask is determined from a logical AND of conditions in list.</li> <li><code>or</code> - Filter mask is determined from a logical OR of conditions in list.</li> <li><code>Format: [column_name, operator, value]</code></li> <li><code>Accepted operators: \"==\", \"!=\", \"&lt;\", \"&gt;\", \"&lt;=\", \"&gt;=\"</code></li> </ul> </li> <li><code>series</code> - (Optional.) Display several plots in the same graph and group x-axis data by specified column values. (Specify an empty list if there is only one series.)<ul> <li><code>Format: [column_name, value]</code></li> </ul> </li> <li><code>column_types</code> - Pandas dtype for each relevant column (axes, units, filters, series). Specified with a dictionary.<ul> <li><code>Accepted types: \"str\"/\"string\"/\"object\", \"int\"/\"int64\", \"float\"/\"float64\", \"datetime\"/\"datetime64\"</code></li> </ul> </li> <li><code>extra_columns_to_csv</code> - (Optional.) List of additional columns to include when exporting benchmark data to a CSV, in addition to the ones above. These columns are not used in plotting. (Specify an empty list if no additional columns are required.)</li> </ul>"},{"location":"post-processing/#a-note-on-replaced-reframe-columns","title":"A Note on Replaced ReFrame Columns","text":"<p>A perflog contains certain columns with complex information that has to be unpacked in order to be useful. Currently, such columns are <code>display_name</code>, <code>extra_resources</code>, <code>env_vars</code>, and <code>spack_spec_dict</code>. Those columns are parsed by the postprocessing, removed from the DataFrame, and substituted by new columns with the unpacked information. Therefore they will not be present in the DataFrame available to the graphing script and should not be referenced in a plot config file.</p> <p>When the row contents of <code>display_name</code> are parsed, they are separated into their constituent benchmark names and parameters. This column is replaced with a new <code>test_name</code> column and new parameter columns (if present). Similarly, the <code>extra_resources</code>, <code>env_vars</code>, and <code>spack_spec_dict</code> columns are replaced with their respective dictionary row contents (keys become columns, values become row contents).</p>"},{"location":"post-processing/#complete-config-template","title":"Complete Config Template","text":"<p>This template includes all possible config fields, some of which are optional or mutually exclusive (e.g. <code>column</code> and <code>custom</code>).</p> <pre><code>title: &lt;custom_label&gt;\n\nx_axis:\n  value: &lt;column_name&gt;\n  # use one of 'column' or 'custom'\n  units:\n    column: &lt;column_name&gt;\n    custom: &lt;custom_label&gt;\n  # optional (default: ascending)\n  sort: \"descending\"\n\ny_axis:\n  value: &lt;column_name&gt;\n  # use one of 'column' or 'custom'\n  units:\n    column: &lt;column_name&gt;\n    custom: &lt;custom_label&gt;\n  # optional (default: no data transformation)\n  # use one of 'column' or 'custom'\n  scaling:\n    column:\n      name: &lt;column_name&gt;\n      series: &lt;index&gt;\n      x_value: &lt;column_value&gt;\n    custom: &lt;custom_value&gt;\n\n# optional (default: include all data)\n# entry format: [&lt;column_name&gt;, &lt;operator&gt;, &lt;column_value&gt;]\n# accepted operators: ==, !=, &lt;, &gt;, &lt;=, &gt;=\nfilters:\n  and: &lt;condition_list&gt;\n  or: &lt;condition_list&gt;\n\n# optional (default: no x-axis grouping, one plot per graph)\n# entry format: [&lt;column_name&gt;, &lt;column_value&gt;]\nseries: &lt;series_list&gt;\n\n# include types for each column that is used in the config\n# accepted types: string/object, int, float, datetime\ncolumn_types:\n  &lt;column_name&gt;: &lt;column_type&gt;\n\n# optional (default: no extra columns exported to CSV file in addition to the ones above)\nextra_columns_to_csv: &lt;columns_list&gt;\n</code></pre>"},{"location":"post-processing/#example-config","title":"Example Config","text":"<p>This example more accurately illustrates what an actual config file may look like.</p> <pre><code>title: \"Plot Title\"\n\nx_axis:\n  value: \"x_axis_col\"\n  units:\n    custom: \"unit_label\"\n  sort: \"descending\"\n\ny_axis:\n  value: \"y_axis_col\"\n  units:\n    column: \"unit_col\"\n  scaling:\n    column:\n      name: \"scaling_col\"\n      series: 0\n      x_value: \"x_val_s\"\n\nfilters:\n  and: [[\"filter_col_1\", \"&lt;=\", filter_val_1],\n        [\"filter_col_2\", \"!=\", filter_val_2]]\n  or: []\n\nseries: [[\"series_col\", \"series_val_1\"],\n         [\"series_col\", \"series_val_2\"]]\n\ncolumn_types:\n  x_axis_col: \"str\"\n  y_axis_col: \"float\"\n  unit_col: \"str\"\n  scaling_col: \"float\"\n  filter_col_1: \"datetime\"\n  filter_col_2: \"int\"\n  series_col: \"str\"\n\nadditional_columns_to_csv:\n  [\"additional_col_1\", \"additional_col_2\"]\n</code></pre>"},{"location":"post-processing/#x-axis-grouping","title":"X-axis Grouping","text":"<p>The settings above will produce a graph that will have its x-axis data grouped based on the values in <code>x_axis_col</code> and <code>series_col</code>. (<code>Note: only groupings with one series column are currently supported.</code>) If we imagine that <code>x_axis_col</code> has two unique values, <code>\"x_val_1\"</code> and <code>\"x_val_2\"</code>, there will be four groups (and four bars) along the x-axis:</p> <ul> <li>(<code>x_val_1</code>, <code>series_val_1</code>)</li> <li>(<code>x_val_1</code>, <code>series_val_2</code>)</li> <li>(<code>x_val_2</code>, <code>series_val_1</code>)</li> <li>(<code>x_val_2</code>, <code>series_val_2</code>)</li> </ul>"},{"location":"post-processing/#scaling","title":"Scaling","text":"<p>When axis values are scaled, they are all divided by a number or a list of numbers. If using more than one number for scaling, the length of the list must match the length of the axis column being scaled. (<code>Note: scaling is currently only supported for y-axis data, as graphs with a non-categorical x-axis are still a work in progress.</code>)</p> <p>Custom Scaling</p> <p>Manually specify one value to scale axis values by.</p> <pre><code>y_axis:\n  value: \"y_axis_col\"\n  units:\n    column: \"unit_col\"\n  scaling:\n    custom: 2\n</code></pre> <p>In the snippet above, all y-axis values are to be divided by 2.</p> y_axis_col scaled_y_axis_col 3.2 3.2 / 2.0 = 1.6 5.4 5.4 / 2.0 = 2.7 2.4 2.4 / 2.0 = 1.2 5.0 5.0 / 2.0 = 2.5 <p>Column Scaling</p> <p>Specify one column to scale axis values by.</p> <pre><code>y_axis:\n  value: \"y_axis_col\"\n  units:\n    column: \"unit_col\"\n  scaling:\n    column:\n      name: \"scaling_col\"\n</code></pre> <p>In the snippet above, all y-axis values are to be divided by the corresponding values in the scaling column.</p> y_axis_col scaling_col scaled_y_axis_col 3.2 <code>1.6</code> 3.2 / 1.6 = 2.0 5.4 <code>2.0</code> 5.4 / 2.0 = 2.7 2.4 <code>0.6</code> 2.4 / 0.6 = 4.0 5.0 <code>2.5</code> 5.0 / 2.5 = 2.0 <p>Series Scaling</p> <p>Specify one series to scale axis values by. This is done with an index, which is used to find the correct series from a list.</p> <p>In the case of the list of series from the example config above, index 0 would select a scaling series of <code>[\"series_col\", \"series_val_1\"]</code>, while index 1 would scale by <code>[\"series_col\", \"series_val_2\"]</code>.</p> <pre><code>y_axis:\n  value: \"y_axis_col\"\n  units:\n    column: \"unit_col\"\n  scaling:\n    column:\n      name: \"scaling_col\"\n      series: 0\n</code></pre> <p>In the snippet above, all y-axis values are to be split by series and divided by the corresponding values in the scaling series.</p> y_axis_col scaling_col series_col scaled_y_axis_col 3.2 <code>1.6</code> <code>series_val_1</code> 3.2 / 1.6 = 2.0 5.4 <code>2.0</code> <code>series_val_1</code> 5.4 / 2.0 = 2.7 2.4 0.6 series_val_2 2.4 / 1.6 = 1.5 5.0 2.5 series_val_2 5.0 / 2.0 = 2.5 <p>Selected Value Scaling</p> <p>Specify one value from a column to scale axis values by.</p> <pre><code>y_axis:\n  value: \"y_axis_col\"\n  units:\n    column: \"unit_col\"\n  scaling:\n    column:\n      name: \"scaling_col\"\n      series: 0\n      x_value: \"x_val_s\"\n</code></pre> <p>In the snippet above, all y-axis values are to be divided by the scaling value found by filtering the scaling column by both series and x-axis value.</p> x_axis_col y_axis_col scaling_col series_col scaled_y_axis_col x_val_1 3.2 1.6 series_val_1 3.2 / 2.0 = 1.6 <code>x_val_s</code> 5.4 <code>2.0</code> <code>series_val_1</code> 5.4 / 2.0 = 2.7 x_val_2 2.4 0.7 series_val_2 2.4 / 2.0 = 1.2 x_val_s 5.0 2.5 series_val_2 5.0 / 2.0 = 2.5 <p>(<code>Note: if series are not present and x-axis values are all unique, it is enough to specify just the column name and x-value.</code>)</p>"},{"location":"post-processing/#filters","title":"Filters","text":"<p>A condition list for filtering has entries in the format <code>[&lt;column_name&gt;, &lt;operator&gt;, &lt;column_value&gt;]</code>. AND filters and OR filters are combined with a logical AND to produce the final filter mask applied to the DataFrame prior to graphing. For example:</p> <ul> <li><code>and_filters</code> = <code>cond1</code>, <code>cond2</code></li> <li><code>or_filters</code>= <code>cond3</code>, <code>cond4</code></li> </ul> <p>The filters above would produce the final filter <code>mask</code> = (<code>cond1</code> AND <code>cond2</code>) AND (<code>cond3</code> OR <code>cond4</code>).</p>"},{"location":"post-processing/#column-types","title":"Column Types","text":"<p>Types must be specified for all columns included in the config in the format <code>&lt;column_name&gt;:&lt;column_type&gt;</code>. Accepted types include <code>string/object</code>, <code>int</code>, <code>float</code>, and <code>datetime</code>.</p> <p>All user-specified types are internally converted to their nullable incarnations. As such:</p> <ul> <li>Strings are treated as <code>object</code> (str or mixed type).</li> <li>Floats are treated as <code>float64</code>.</li> <li>Integers are treated as <code>Int64</code>.</li> <li>Datetimes are treated as <code>datetime64[ns]</code>.</li> </ul>"},{"location":"post-processing/#future-development","title":"Future Development","text":"<p>The post-processing capabilities are still a work in progress. Some upcoming developments:</p> <ul> <li>Embed graphs in GitHub Pages, instead of a bare HTML file.</li> <li>Add scaling and regression plots.</li> </ul>"},{"location":"tutorial/tutorial/","title":"ARCHER2 Tutorial","text":""},{"location":"tutorial/tutorial/#using-reframe-for-reproducible-and-portable-performance-benchmarking","title":"Using ReFrame for reproducible and portable performance benchmarking","text":"<p>In this tutorial you will set up the benchmarking framework on the ARCHER2 supercomputer, build and run example benchmarks, create a new benchmark and explore benchmark data.</p>"},{"location":"tutorial/tutorial/#getting-started","title":"Getting Started","text":"<p>To complete this tutorial, you need to connect to ARCHER2 via ssh. You will need</p> <ol> <li>An ARCHER2 account. You can request a new account if you haven't got one you can use. Use the project code <code>ta131</code> to request your account. You can use an existing ARCHER2 account to complete this workshop.</li> <li>A command line terminal with an ssh client. Most Linux and Mac systems come with these preinstalled. Please see Connecting to ARCHER2 for more information and Windows instructions.</li> </ol>"},{"location":"tutorial/tutorial/#ssh","title":"ssh","text":"<p>Once you have the above prerequisites, you have to generate an ssh key pair and upload the public key to SAFE. </p> <p>When you are done, check that you are able to connect to ARCHER2 with</p> <pre><code>ssh username@login.archer2.ac.uk\n</code></pre>"},{"location":"tutorial/tutorial/#archer2-mfa","title":"ARCHER2 MFA","text":"<p>ARCHER2 is deploying mandatory multi-factor authentication (MFA) Today!</p> <p>This was deployed between 0900 and 1000 this morning (06/12/2023).</p> <p>This means that once the switch has happened, SSH keys will work as before, but instead of your ARCHER2 password, a Time-based One-Time Password (TOTP) code will be requested. </p> <p>TOTP is a six digit number, refreshed every 30 seconds, which is generated typically by an app running on your mobile phone or laptop.</p> <p>Thus authentication will require two factors:</p> <p>1) SSH key and passphrase 2) TOTP</p>"},{"location":"tutorial/tutorial/#archer2-mfa-docs-and-support","title":"ARCHER2 MFA Docs and Support","text":"<p>The SAFE documentation which details how to set up MFA on machine accounts (ARCHER2) is available at: https://epcced.github.io/safe-docs/safe-for-users/#how-to-turn-on-mfa-on-your-machine-account</p> <p>The documentation includes how to set this up without the need of a personal smartphone device.</p> <p>We have also updated the ARCHER2 documentation with details of the new connection process: https://docs.archer2.ac.uk/user-guide/connecting-totp/ https://docs.archer2.ac.uk/quick-start/quickstart-users-totp/</p> <p>If there are any issues or concerns please contact us at: </p> <p>support@archer2.ac.uk</p>"},{"location":"tutorial/tutorial/#installing-the-framework","title":"Installing the Framework","text":""},{"location":"tutorial/tutorial/#set-up-python","title":"Set up python","text":"<p>We are going to use <code>python</code> and the <code>pip</code> package installer to install and run the framework. Load the <code>cray-python</code> module to get a python version that fills the requirements. <pre><code>module load cray-python\n</code></pre> You can check with <code>python3 --version</code> that your python version is <code>3.8</code> or greater. You will have to load this module every time you login.</p> <p>(at the time of writing, the default version was <code>3.9.13</code>).</p>"},{"location":"tutorial/tutorial/#change-to-work-directory","title":"Change to work directory","text":"<p>On ARCHER2, the compute nodes do not have access to your home directory, therefore it is important to install everything in a work file system. Change to the work directory with</p> <pre><code>cd /work/ta131/ta131/${USER}\n</code></pre> <p>If you are tempted to use a symlink here, ensure you use <code>cd -P</code> when changing directory. Archer2 compute nodes cannot read from <code>/home</code>, only <code>/work</code>, so not completely following symlinks can result in a broken installation.</p>"},{"location":"tutorial/tutorial/#clone-the-framework-repository","title":"Clone the framework repository","text":"<p>In the work directory, clone the excalibur-tests repository with</p> <pre><code>git clone https://github.com/ukri-excalibur/excalibur-tests.git\n</code></pre>"},{"location":"tutorial/tutorial/#create-a-virtual-environment","title":"Create a virtual environment","text":"<p>Before proceeding to install the software, we recommend creating a python virtual environment to avoid clashes with other installed python packages. You can do this with <pre><code>python3 -m venv excalibur-env\nsource excalibur-env/bin/activate\n</code></pre></p> <p>You should now see the name of the environment in parenthesis your terminal prompt, for example: <pre><code>(excalibur-env) tk-d193@ln03:/work/d193/d193/tk-d193&gt;\n</code></pre></p> <p>You will have to activate the environment each time you login. To deactivate the environment run <code>deactivate</code>.</p>"},{"location":"tutorial/tutorial/#install-the-excalibur-tests-framework","title":"Install the excalibur-tests framework","text":"<p>Now we can use <code>pip</code> to install the package in the virtual environment. Update pip to the latest version with  <pre><code>pip install --upgrade pip\n</code></pre> then install the framework with <pre><code>pip install -e ./excalibur-tests[post-processing]\n</code></pre> We used the <code>editable</code> flag <code>-e</code> because later in the tutorial you will edit the repository to develop a new benchmark.</p> <p>We included optional dependencies with <code>[post-processing]</code>. We will need those in the postprocessing section.</p>"},{"location":"tutorial/tutorial/#set-configuration-variables","title":"Set configuration variables","text":"<p>Configure the framework by setting these environment variables</p> <pre><code>export RFM_CONFIG_FILES=\"$(pwd)/excalibur-tests/benchmarks/reframe_config.py\"\nexport RFM_USE_LOGIN_SHELL=\"true\"\n</code></pre>"},{"location":"tutorial/tutorial/#install-and-configure-spack","title":"Install and configure spack","text":"<p>Finally, we need to install the <code>spack</code> package manager. The framework will use it to build the benchmarks. Clone spack with</p> <pre><code>git clone -c feature.manyFiles=true https://github.com/spack/spack.git\n</code></pre> <p>Then configure <code>spack</code> with</p> <pre><code>source ./spack/share/spack/setup-env.sh\n</code></pre> <p>Spack should now be in the default search path.</p>"},{"location":"tutorial/tutorial/#check-installation-was-successful","title":"Check installation was successful","text":"<p>You can check everything has been installed successfully by checking that <code>spack</code> and <code>reframe</code> are in path and the path to the ReFrame config file is set correctly</p> <pre><code>$ spack --version\n0.22.0.dev0 (88e738c34346031ce875fdd510dd2251aa63dad7)\n$ reframe --version\n4.4.1\n$ ls $RFM_CONFIG_FILES\n/work/d193/d193/tk-d193/excalibur-tests/benchmarks/reframe_config.py\n</code></pre>"},{"location":"tutorial/tutorial/#environment-summary","title":"Environment summary","text":"<p>If you log out and back in, you will have to run some of the above commands again to recreate your environment. These are (from your <code>work</code> directory):</p> <pre><code>module load cray-python\nsource excalibur-env/bin/activate\nexport RFM_CONFIG_FILES=\"$(pwd)/excalibur-tests/benchmarks/reframe_config.py\"\nexport RFM_USE_LOGIN_SHELL=\"true\"\nsource ./spack/share/spack/setup-env.sh\n</code></pre>"},{"location":"tutorial/tutorial/#run-sombrero-example","title":"Run Sombrero Example","text":"<p>You can now use ReFrame to run benchmarks from the <code>benchmarks/examples</code> and <code>benchmarks/apps</code> directories. The basic syntax is  <pre><code>reframe -c &lt;path/to/benchmark&gt; -r\n</code></pre></p>"},{"location":"tutorial/tutorial/#archer2-specific-commands","title":"ARCHER2 specific commands","text":"<p>In addition, on ARCHER2, you have to provide the quality of service (QoS) type for your job to ReFrame on the command line with <code>-J</code>. Use the \"short\" QoS to run the sombrero example with <pre><code>reframe -c excalibur-tests/benchmarks/examples/sombrero -r -J'--qos=short'\n</code></pre> You may notice you actually ran four benchmarks with that single command! That is because the benchmark is parametrized. We will talk about this in the next section.</p>"},{"location":"tutorial/tutorial/#output-sample","title":"Output sample","text":"<pre><code>$ reframe -c benchmarks/examples/sombrero/ -r -J'--qos=short' --performance-report\n[ReFrame Setup]\n  version:           4.3.0\n  command:           '/mnt/lustre/a2fs-work3/work/d193/d193/tk-d193/excalibur-env/bin/reframe -c benchmarks/examples/sombrero/ -r -J--qos=short'\n  launched by:       tk-d193@ln03\n  working directory: '/mnt/lustre/a2fs-work3/work/d193/d193/tk-d193/excalibur-tests'\n  settings files:    '&lt;builtin&gt;', '/work/d193/d193/tk-d193/excalibur-tests/benchmarks/reframe_config.py'\n  check search path: '/mnt/lustre/a2fs-work3/work/d193/d193/tk-d193/excalibur-tests/benchmarks/examples/sombrero'\n  stage directory:   '/mnt/lustre/a2fs-work3/work/d193/d193/tk-d193/excalibur-tests/stage'\n  output directory:  '/mnt/lustre/a2fs-work3/work/d193/d193/tk-d193/excalibur-tests/output'\n  log files:         '/tmp/rfm-u1l6yt7f.log'\n\n[==========] Running 4 check(s)\n[==========] Started on Fri Jul  7 15:47:45 2023 \n\n[----------] start processing checks\n[ RUN      ] SombreroBenchmark %tasks=2 %cpus_per_task=2 /de04c10b @archer2:compute-node+default\n[ RUN      ] SombreroBenchmark %tasks=2 %cpus_per_task=1 /c52a123d @archer2:compute-node+default\n[ RUN      ] SombreroBenchmark %tasks=1 %cpus_per_task=2 /c1c3a3f1 @archer2:compute-node+default\n[ RUN      ] SombreroBenchmark %tasks=1 %cpus_per_task=1 /52e1ce98 @archer2:compute-node+default\n[       OK ] (1/4) SombreroBenchmark %tasks=1 %cpus_per_task=2 /c1c3a3f1 @archer2:compute-node+default\nP: flops: 0.67 Gflops/seconds (r:1.2, l:None, u:None)\n[       OK ] (2/4) SombreroBenchmark %tasks=1 %cpus_per_task=1 /52e1ce98 @archer2:compute-node+default\nP: flops: 0.67 Gflops/seconds (r:1.2, l:None, u:None)\n[       OK ] (3/4) SombreroBenchmark %tasks=2 %cpus_per_task=2 /de04c10b @archer2:compute-node+default\nP: flops: 1.27 Gflops/seconds (r:1.2, l:None, u:None)\n[       OK ] (4/4) SombreroBenchmark %tasks=2 %cpus_per_task=1 /c52a123d @archer2:compute-node+default\nP: flops: 1.24 Gflops/seconds (r:1.2, l:None, u:None)\n[----------] all spawned checks have finished\n\n[  PASSED  ] Ran 4/4 test case(s) from 4 check(s) (0 failure(s), 0 skipped, 0 aborted)\n[==========] Finished on Fri Jul  7 15:48:23 2023 \nLog file(s) saved in '/tmp/rfm-u1l6yt7f.log'\n</code></pre>"},{"location":"tutorial/tutorial/#benchmark-output","title":"Benchmark output","text":"<p>You can find build and run logs in the <code>output/</code> directory of a successful benchmark. They record how the benchmark was built by spack and ran by ReFrame.</p> <p>While the benchmark is running, the log files are kept in the <code>stage/</code> directory. They remain there if the benchmark fails to build or run.</p> <p>You can find the performance log file from the benchmark in <code>perflogs/</code>. The perflog records the captured figures of merit, environment variables and metadata about the job.</p>"},{"location":"tutorial/tutorial/#postprocess-benchmark-results","title":"Postprocess Benchmark Results","text":"<p>Now let's look at the Benchmark performance results, and create a plot to visualise them.</p> <p>NOTE: The post-processing package is currently under heavy development. Please refer to the latest <code>post-processing/README.md</code> and <code>post-processing/post_processing_config.yaml</code> to use it.</p>"},{"location":"tutorial/tutorial/#the-perflog","title":"The perflog","text":"<p>After running the Sombrero benchmark once you should have a perflog in <code>perflogs/archer2/compute-node/SombreroBenchmark.log</code> that looks like this <pre><code>job_completion_time|version|info|jobid|num_tasks|num_cpus_per_task|num_tasks_per_node|num_gpus_per_node|flops_value|flops_unit|flops_ref|flops_lower_thres|flops_upper_thres|spack_spec|display_name|system|partition|environ|extra_resources|env_vars|tags\n2023-08-25T11:23:46|reframe 4.3.2|SombreroBenchmark %tasks=2 %cpus_per_task=2 /de04c10b @archer2:compute-node+default|4323431|2|2|1|null|1.31|Gflops/seconds|1.2|-0.2|None|sombrero@2021-08-16|SombreroBenchmark %tasks=2 %cpus_per_task=2|archer2|compute-node|default|{}|{\"OMP_NUM_THREADS\": \"2\"}|example\n2023-08-25T11:23:48|reframe 4.3.2|SombreroBenchmark %tasks=1 %cpus_per_task=2 /c1c3a3f1 @archer2:compute-node+default|4323433|1|2|1|null|0.67|Gflops/seconds|1.2|-0.2|None|sombrero@2021-08-16|SombreroBenchmark %tasks=1 %cpus_per_task=2|archer2|compute-node|default|{}|{\"OMP_NUM_THREADS\": \"2\"}|example\n2023-08-25T11:23:48|reframe 4.3.2|SombreroBenchmark %tasks=1 %cpus_per_task=1 /52e1ce98 @archer2:compute-node+default|4323434|1|1|1|null|0.67|Gflops/seconds|1.2|-0.2|None|sombrero@2021-08-16|SombreroBenchmark %tasks=1 %cpus_per_task=1|archer2|compute-node|default|{}|{\"OMP_NUM_THREADS\": \"1\"}|example\n2023-08-25T11:23:48|reframe 4.3.2|SombreroBenchmark %tasks=2 %cpus_per_task=1 /c52a123d @archer2:compute-node+default|4323432|2|1|1|null|1.29|Gflops/seconds|1.2|-0.2|None|sombrero@2021-08-16|SombreroBenchmark %tasks=2 %cpus_per_task=1|archer2|compute-node|default|{}|{\"OMP_NUM_THREADS\": \"1\"}|example\n</code></pre> Every time the same benchmark is run, a line is appended in this perflog.</p> <p>The perflog contains - Some general info about the benchmark run, including system, spack, and environment info. - The Figure(s) Of Merit (FOM) value, units, reference value, and lower and upper limits (<code>flops</code> in this case) - The <code>display_name</code> field, which encodes the benchmark name and parameters (<code>SombreroBenchmark %tasks=... %cpus_per_task=...</code> in this case) - Other quantities the user might want to compare performance for, passed as environment variables and encoded in the <code>env_vars</code> field. - The benchmark <code>tag</code> - another way to encode benchmark inputs, defined by the benchmark developers.</p>"},{"location":"tutorial/tutorial/#the-plotting-configuration-file","title":"The plotting configuration file","text":"<p>The framework contains tools to plot the FOMs of benchmarks against any of the other parameters in the perflog. This generic plotting is driven by a configuration YAML file. Let's make one, and save it in <code>excalibur-tests/post-processing/post_processing_config.yaml</code>.</p> <p>The file needs to include - Plot title  - Axis information - Data series - Filters - Data types</p>"},{"location":"tutorial/tutorial/#title-and-axes","title":"Title and Axes","text":"<p>Axes must have a value specified with a perflog column name or a benchmark parameter name, and units specified with either a perflog column name or a custom label (including <code>null</code>). <pre><code>title: Performance vs number of tasks and CPUs_per_task\n\nx_axis:\n  value: \"tasks\"\n  units:\n    custom: null\n\ny_axis:\n  value: \"flops_value\"\n  units:\n    column: \"flops_unit\"\n</code></pre></p>"},{"location":"tutorial/tutorial/#data-series","title":"Data series","text":"<p>Display several data series in the same plot and group x-axis data by specified column values. Specify an empty list if you only want one series plotted. In our sombrero example, we have two parameters. Therefore we need to either filter down to one, or make them separate series. Let's use separate series:</p> <p>Format: <code>[column_name, value]</code> <pre><code>series: [[\"cpus_per_task\", \"1\"], [\"cpus_per_task\", \"2\"]]\n</code></pre> NOTE: Currently, only one distinct <code>column_name</code> is supported. In the future, a second one will be allowed to be added. But in any case, unlimited number of series can be plotted for the same <code>column_name</code> but different <code>value</code>.</p>"},{"location":"tutorial/tutorial/#filtering","title":"Filtering","text":"<p>You can filter data rows based on specified conditions. Specify an empty list for no filters.</p> <p>Format: <code>[column_name, operator, value]</code>,  Accepted operators: \"==\", \"!=\", \"&lt;\", \"&gt;\", \"&lt;=\", \"&gt;=\" <pre><code>filters: []\n</code></pre></p> <p>NOTE: After re-running the benchmarks a few times your perflog will get populated with multiple lines and you'll have to filter down to what you want to plot. Feel free to experiment with a dirtier perflog file (eg. the one in <code>excalibur-tests/tutorial</code>) or a folder with several perflog files.</p>"},{"location":"tutorial/tutorial/#data-types","title":"Data types","text":"<p>All columns used in axes, filters, and series must have a user-specified type for the data they contain. This would be the pandas dtype, e.g. <code>str/string/object</code>, <code>int/int64</code>, <code>float/float64</code>, <code>datetime/datetime64</code>. <pre><code>column_types:\n  tasks: \"int\"\n  flops_value: \"float\"\n  flops_unit: \"str\"\n  cpus_per_task: \"int\"\n</code></pre></p>"},{"location":"tutorial/tutorial/#run-the-postprocessing","title":"Run the postprocessing","text":"<p>The postprocessing package is an optional dependency of the framework. Install it with <pre><code>pip install -e ./excalibur-tests/[post-processing]\n</code></pre></p> <p>We can now run the postprocessing with <pre><code>python post_processing.py &lt;log_path&gt; &lt;config_path&gt;\n</code></pre> where - <code>&lt;log_path&gt;</code> is the path to a perflog file or a directory containing perflog files. - <code>&lt;config_path&gt;</code> is the path to the configuration YAML file.</p> <p>In our case, <pre><code>python excalibur-tests/post-processing/post_processing.py perflogs excalibur-tests/post-processing/post_processing_config.yaml\n</code></pre></p>"},{"location":"tutorial/tutorial/#view-the-output","title":"View the Output","text":"<p><code>scp</code> over the <code>Performance_vs_number_of_tasks_and_CPUs_per_task.html</code> file created in <code>excalibur-tests/post-processing</code>, and behold!</p> <p></p>"},{"location":"tutorial/tutorial/#create-a-benchmark","title":"Create a Benchmark","text":"<p>In this section you will create a ReFrame benchmark by writing a python class that tells ReFrame how to build and run an application and collect data from its output. </p> <p>For simplicity, we use the <code>STREAM</code> benchmark. It is a simple memory bandwidth benchmark with minimal build dependencies.</p>"},{"location":"tutorial/tutorial/#how-reframe-works","title":"How ReFrame works","text":"<p>When ReFrame executes a test it runs a pipeline of the following stages</p> <p></p> <p>You can customise the behaviour of each stage or add a hook before or after each of them.  For more details, read the ReFrame pipeline documentation.</p>"},{"location":"tutorial/tutorial/#getting-started_1","title":"Getting started","text":"<p>To get started, open an empty <code>.py</code> file where you will write the ReFrame class, e.g. <code>stream.py</code>. Save the file in a new directory e.g. <code>excalibur-tests/benchmarks/apps/stream</code>.</p>"},{"location":"tutorial/tutorial/#include-reframe-modules","title":"Include ReFrame modules","text":"<p>The first thing you need is include a few modules from ReFrame. These should be available if the installation step was successful.</p> <pre><code>import reframe as rfm\nimport reframe.utility.sanity as sn\n</code></pre>"},{"location":"tutorial/tutorial/#create-a-test-class","title":"Create a Test Class","text":"<p>ReFrame has built-in support for the Spack package manager. In the following we will use the custom class <code>SpackTest</code> we created for our <code>benchmarks</code> module, which provides a tighter integration with Spack and reduces the boilerplate code you'd otherwise have to include.</p> <pre><code>from benchmarks.modules.utils import SpackTest\n\n@rfm.simple_test\nclass StreamBenchmark(SpackTest):\n</code></pre> <p>The data members and methods detailed in the following sections should be placed inside this class.</p>"},{"location":"tutorial/tutorial/#add-build-recipe","title":"Add Build Recipe","text":"<p>We prefer installing packages via spack whenever possible. In this exercise, the spack package for <code>stream</code> already exists in the global spack repository.</p> <p>The <code>SpackTest</code> base class takes care of setting up spack as the build system ReFrame uses. We only need to instruct ReFrame to install version <code>5.10</code> of the <code>stream</code> spack package using the <code>openmp</code> variant.</p> <pre><code>spack_spec = 'stream@5.10 +openmp'\n</code></pre> <p>Note that we did not specify a compiler. Spack will use a compiler from the spack environment. The complete spec is recorded in the build log.</p>"},{"location":"tutorial/tutorial/#add-run-configuration","title":"Add Run Configuration","text":"<p>The ReFrame class tells ReFrame where and how to run the benchmark. We want to run on one task on a full archer2 node using 128 OpenMP threads to use the full node.</p> <pre><code>valid_systems = ['archer2']\nvalid_prog_environs = ['default']\nexecutable = 'stream_c.exe'\nnum_tasks = 1\nnum_cpus_per_task = 128\ntime_limit = '5m'\nuse_multithreading = False\n</code></pre>"},{"location":"tutorial/tutorial/#add-environment-variables","title":"Add environment variables","text":"<p>Environment variables can be added to the <code>env_vars</code> attribute.</p> <pre><code>env_vars['OMP_NUM_THREADS'] = f'{num_cpus_per_task}'\nenv_vars['OMP_PLACES'] = 'cores'\n</code></pre>"},{"location":"tutorial/tutorial/#add-sanity-check","title":"Add Sanity Check","text":"<p>The rest of the benchmark follows the Writing a Performance Test ReFrame Tutorial. First we need a sanity check that ensures the benchmark ran successfully. A function decorated with the <code>@sanity_function</code> decorator is used by ReFrame to check that the test ran successfully. The sanity function can perform a number of checks, in this case we want to match a line of the expected standard output.</p> <pre><code>@sanity_function\ndef validate_solution(self):\n    return sn.assert_found(r'Solution Validates', self.stdout)\n</code></pre>"},{"location":"tutorial/tutorial/#add-performance-pattern-check","title":"Add Performance Pattern Check","text":"<p>To record the performance of the benchmark, ReFrame should extract a figure of merit from the output of the test. A function decorated with the <code>@performance_function</code> decorator extracts or computes a performance metric from the test\u2019s output.</p> <p>In this example, we extract four performance variables, namely the memory bandwidth values for each of the \u201cCopy\u201d, \u201cScale\u201d, \u201cAdd\u201d and \u201cTriad\u201d sub-benchmarks of STREAM, where each of the performance functions use the <code>extractsingle()</code> utility function. For each of the sub-benchmarks we extract the \u201cBest Rate MB/s\u201d column of the output (see below) and we convert that to a float.</p>"},{"location":"tutorial/tutorial/#performance-pattern-check","title":"Performance Pattern Check","text":"<pre><code>@performance_function('MB/s', perf_key='Copy')\ndef extract_copy_perf(self):\n    return sn.extractsingle(r'Copy:\\s+(\\S+)\\s+.*', self.stdout, 1, float)\n\n@performance_function('MB/s', perf_key='Scale')\ndef extract_scale_perf(self):\n    return sn.extractsingle(r'Scale:\\s+(\\S+)\\s+.*', self.stdout, 1, float)\n\n@performance_function('MB/s', perf_key='Add')\ndef extract_add_perf(self):\n    return sn.extractsingle(r'Add:\\s+(\\S+)\\s+.*', self.stdout, 1, float)\n\n@performance_function('MB/s', perf_key='Triad')\ndef extract_triad_perf(self):\n    return sn.extractsingle(r'Triad:\\s+(\\S+)\\s+.*', self.stdout, 1, float)\n</code></pre>"},{"location":"tutorial/tutorial/#run-stream-benchmark","title":"Run Stream Benchmark","text":"<p>You can now run the benchmark in the same way as the previous sombrero example</p> <pre><code>reframe -c excalibur-tests/benchmarks/apps/stream/ -r --system archer2 -J'--qos=short'\n</code></pre>"},{"location":"tutorial/tutorial/#sample-output","title":"Sample Output","text":"<pre><code>$ reframe -c excalibur-tests/benchmarks/examples/stream/ -r -J'--qos=short'\n[ReFrame Setup]\n  version:           4.4.1\n  command:           '/mnt/lustre/a2fs-work3/work/d193/d193/tk-d193/ciuk-demo/demo-env/bin/reframe -c excalibur-tests/benchmarks/examples/stream/ -r -J--qos=short'\n  launched by:       tk-d193@ln03\n  working directory: '/mnt/lustre/a2fs-work3/work/d193/d193/tk-d193/ciuk-demo'\n  settings files:    '&lt;builtin&gt;', '/work/d193/d193/tk-d193/ciuk-demo/excalibur-tests/benchmarks/reframe_config.py'\n  check search path: '/mnt/lustre/a2fs-work3/work/d193/d193/tk-d193/ciuk-demo/excalibur-tests/benchmarks/examples/stream'\n  stage directory:   '/mnt/lustre/a2fs-work3/work/d193/d193/tk-d193/ciuk-demo/stage'\n  output directory:  '/mnt/lustre/a2fs-work3/work/d193/d193/tk-d193/ciuk-demo/output'\n  log files:         '/tmp/rfm-z87x4min.log'\n\n[==========] Running 1 check(s)\n[==========] Started on Thu Nov 30 14:50:21 2023 \n\n[----------] start processing checks\n[ RUN      ] StreamBenchmark /8aeff853 @archer2:compute-node+default\n[       OK ] (1/1) StreamBenchmark /8aeff853 @archer2:compute-node+default\nP: Copy: 1380840.8 MB/s (r:0, l:None, u:None)\nP: Scale: 1369568.7 MB/s (r:0, l:None, u:None)\nP: Add: 1548666.1 MB/s (r:0, l:None, u:None)\nP: Triad: 1548666.1 MB/s (r:0, l:None, u:None)\n[----------] all spawned checks have finished\n\n[  PASSED  ] Ran 1/1 test case(s) from 1 check(s) (0 failure(s), 0 skipped, 0 aborted)\n[==========] Finished on Thu Nov 30 14:51:13 2023 \nLog file(s) saved in '/tmp/rfm-z87x4min.log'\n</code></pre>"},{"location":"tutorial/tutorial/#interpreting-stream-results","title":"Interpreting STREAM results","text":"<p>With default compile options, STREAM uses arrays of 10 million elements. On a full ARCHER2 node, the default array size fits into cache, and the benchmark does not report the correct memory bandwidth. Therefore the numbers from this tutorial are not comparable with other, published, results.</p> <p>To avoid caching, increase the array size during build by adding e.g. <code>stream_array_size=64000000</code> to the spack spec. </p>"},{"location":"tutorial/tutorial/#parametrized-tests","title":"Parametrized tests","text":"<p>You can pass a list to the <code>parameter()</code> built-in function in the class body to create a parametrized test. You cannot access the individual parameter value within the class body, so any reference to them should be placed in the appropriate function, for example <code>__init__()</code></p> <p>Example: Parametrize the array size</p> <pre><code>array_size = parameter(int(i) for i in [4e6,8e6,16e6,32e6,64e6])\ndef __init__(self):\n    self.spack_spec = f\"stream@5.10 +openmp stream_array_size={self.array_size}\"\n</code></pre> <pre><code>[----------] start processing checks\n[ RUN      ] StreamBenchmark %array_size=64000000 /bbfd0e71 @archer2:compute-node+default\n[ RUN      ] StreamBenchmark %array_size=32000000 /e16f9017 @archer2:compute-node+default\n[ RUN      ] StreamBenchmark %array_size=16000000 /abc01230 @archer2:compute-node+default\n[ RUN      ] StreamBenchmark %array_size=8000000 /51d83d77 @archer2:compute-node+default\n[ RUN      ] StreamBenchmark %array_size=4000000 /8399bc0b @archer2:compute-node+default\n[       OK ] (1/5) StreamBenchmark %array_size=32000000 /e16f9017 @archer2:compute-node+default\nP: Copy: 343432.5 MB/s (r:0, l:None, u:None)\nP: Scale: 291065.8 MB/s (r:0, l:None, u:None)\nP: Add: 275577.5 MB/s (r:0, l:None, u:None)\nP: Triad: 247425.0 MB/s (r:0, l:None, u:None)\n[       OK ] (2/5) StreamBenchmark %array_size=16000000 /abc01230 @archer2:compute-node+default\nP: Copy: 2538396.7 MB/s (r:0, l:None, u:None)\nP: Scale: 2349544.5 MB/s (r:0, l:None, u:None)\nP: Add: 2912500.4 MB/s (r:0, l:None, u:None)\nP: Triad: 2886402.8 MB/s (r:0, l:None, u:None)\n[       OK ] (3/5) StreamBenchmark %array_size=8000000 /51d83d77 @archer2:compute-node+default\nP: Copy: 1641807.1 MB/s (r:0, l:None, u:None)\nP: Scale: 1362616.5 MB/s (r:0, l:None, u:None)\nP: Add: 1959382.9 MB/s (r:0, l:None, u:None)\nP: Triad: 1940497.3 MB/s (r:0, l:None, u:None)\n[       OK ] (4/5) StreamBenchmark %array_size=64000000 /bbfd0e71 @archer2:compute-node+default\nP: Copy: 255622.4 MB/s (r:0, l:None, u:None)\nP: Scale: 235186.0 MB/s (r:0, l:None, u:None)\nP: Add: 204853.9 MB/s (r:0, l:None, u:None)\nP: Triad: 213072.2 MB/s (r:0, l:None, u:None)\n[       OK ] (5/5) StreamBenchmark %array_size=4000000 /8399bc0b @archer2:compute-node+default\nP: Copy: 1231355.3 MB/s (r:0, l:None, u:None)\nP: Scale: 1086783.2 MB/s (r:0, l:None, u:None)\nP: Add: 1519446.0 MB/s (r:0, l:None, u:None)\nP: Triad: 1548666.1 MB/s (r:0, l:None, u:None)\n[----------] all spawned checks have finished\n\n[  PASSED  ] Ran 5/5 test case(s) from 5 check(s) (0 failure(s), 0 skipped, 0 aborted)\n[==========] Finished on Thu Nov 30 14:34:48 2023 \n</code></pre>"},{"location":"tutorial/tutorial/#reference-values","title":"Reference values","text":"<p>ReFrame can automate checking that the results fall within an expected range. We can use it in our previous example of increasing the array size to avoid caching. You can set a different reference value for each <code>perf_key</code> in the performance function. For example, set the test to fail if it falls outside of +-25% of the values obtained with the largest array size.</p> <pre><code>reference = {\n    'archer2': {\n        'Copy':  (260000, -0.25, 0.25, 'MB/s'),\n        'Scale': (230000, -0.25, 0.25, 'MB/s'),\n        'Add':   (210000, -0.25, 0.25, 'MB/s'),\n        'Triad': (210000, -0.25, 0.25, 'MB/s')\n    }\n}\n</code></pre> <p>The performance reference tuple consists of the reference value, the lower and upper thresholds expressed as fractional numbers relative to the reference value, and the unit of measurement. If any of the thresholds is not relevant, None may be used instead. Also, the units in this reference variable are entirely optional, since they were already provided through the @performance_function decorator.</p>"},{"location":"tutorial/tutorial/#plotting-stream-benchmark-output","title":"Plotting STREAM benchmark output","text":"<pre><code>title: Stream Triad Bandwidth\n\nx_axis:\n  value: \"array_size\"\n  units:\n    custom: null\n\ny_axis:\n  value: \"Triad_value\"\n  units: \n    column: \"Triad_unit\"\n\nseries: []\nfilters: [[\"test_name\",\"==\",\"StreamBenchmark\"]]\n</code></pre>"},{"location":"tutorial/tutorial/#portability-demo","title":"Portability Demo","text":"<p>Having gone through the process of setting up the framework on multiple systems enables you to run benchmarks configured in the repository on those systems. As a proof of this concept, this demo shows how to run a benchmark (e.g. <code>hpgmg</code>) on a list of systems (ARCHER2, csd3, cosma8, isambard-macs). Note that to run this demo, you will need an account and a CPU time allocation on each of these systems.</p> <p>The commands to set up and run the demo are recorded in scripts in the exaclibur-tests repository. It is not feasible to make the progress completely system-agnostic, in our case we need to manually</p> <ul> <li>Load a compatible python module</li> <li>Specify the user account for charging CPU time</li> <li>Change the working directory and select quality of service (on ARCHER2)</li> </ul> <p>That is done differently on each system. The framework attempts to automtically identify the system it is being run on, but due to ambiguity in login node names this can fail, and we also recommend specifying the system on the command line.</p> <pre><code>#!/bin/bash -l\n\nsystem=$1\n\n# System specific part of setup. Mostly load the correct python module\nif [ $system == archer2 ]\nthen\n    module load cray-python\n    cd /work/d193/d193/tk-d193\nelif [ $system == csd3 ]\nthen\n    module load python/3.8\nelif [ $system == cosma ]\nthen\n    module swap python/3.10.7\nelif [ $system == isambard ]\nthen\n    module load python37\n    export PATH=/home/ri-tkoskela/.local/bin:$PATH\nfi\n\n# Setup\nmkdir demo\ncd demo\npython3 --version\npython3 -m venv demo-env\nsource ./demo-env/bin/activate\ngit clone git@github.com:ukri-excalibur/excalibur-tests.git\ngit clone -c feature.manyFiles=true git@github.com:spack/spack.git\nsource ./spack/share/spack/setup-env.sh\nexport RFM_CONFIG_FILES=\"$(pwd)/excalibur-tests/benchmarks/reframe_config.py\"\nexport RFM_USE_LOGIN_SHELL=\"true\"\npip install --upgrade pip\npip install -e ./excalibur-tests\n</code></pre> <pre><code>#!/bin/bash\n\napp=$1\ncompiler=$2\nsystem=$3\nspec=$app\\%$compiler\n\napps_dir=excalibur-tests/benchmarks/apps\n\nif [ $system == archer2 ]\nthen\n    reframe -c $apps_dir/$app -r -J'--qos=standard' --system archer2 -S spack_spec=$spec --setvar=num_cpus_per_task=8  --setvar=num_tasks_per_node=2 --setvar=num_tasks=8\nelif [ $system == cosma ]\nthen\n    reframe -c $apps_dir/$app -r -J'--account=do006' --system cosma8 -S spack_spec=$spec --setvar=num_cpus_per_task=8  --setvar=num_tasks_per_node=2 --setvar=num_tasks=8\nelif [ $system == csd3 ]\nthen\n    reframe -c $apps_dir/$app -r -J'--account=DIRAC-DO006-CPU' --system csd3-cascadelake -S spack_spec=$spec --setvar=num_cpus_per_task=8  --setvar=num_tasks_per_node=2 --setvar=num_tasks=8\nelif [ $system == isambard ]\nthen\n    reframe -c $apps_dir/$app -r --system isambard-macs:cascadelake -S build_locally=false -S spack_spec=$spec --setvar=num_cpus_per_task=8  --setvar=num_tasks_per_node=2 --setvar=num_tasks=8\nfi\n</code></pre>"},{"location":"tutorial/tutorial/#useful-reading","title":"Useful Reading","text":""},{"location":"tutorial/tutorial/#reframe","title":"ReFrame","text":"<ul> <li>ReFrame Documentation</li> <li>ReFrame tutorials<ul> <li>Tutorial 1: Getting started with ReFrame</li> <li>Tutorial 2: Customizing Further a Regression Test</li> <li>Tutorial 3: Using Dependencies in ReFrame Tests</li> <li>Tutorial 4: Using Test Fixtures</li> <li>Tutorial 5: Using Build Automation Tools As a Build System</li> <li>Tutorial 6: Tips and Tricks</li> </ul> </li> <li>Libraries of ReFrame tests<ul> <li>Official ReFrame test library</li> <li>ReFrame GitHub organisation with various contributed test libraries</li> </ul> </li> </ul>"},{"location":"tutorial/tutorial/#spack","title":"Spack","text":"<ul> <li>Spack documentation</li> <li>Spack tutorial (including YouTube recordings)</li> <li>Spack package searchable list</li> </ul>"}]}