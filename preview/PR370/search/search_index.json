{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#excalibur-tests","title":"ExCALIBUR tests","text":"<p>Performance benchmarks and regression tests for the ExCALIBUR project.</p> <p>These benchmarks are based on a similar project by StackHPC.</p> <p>Feel free to add new benchmark applications or support new systems that are part of the ExCALIBUR benchmarking collaboration.</p> <p>Note: at the moment the ExCALIBUR benchmarks are a work-in-progress.</p>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Installation</li> <li>Configuration</li> <li>Usage</li> <li>Post-processing</li> <li>Contributing</li> <li>Supported benchmarks</li> <li>Supported systems</li> <li>ReFrame tutorial</li> <li>ARCHER2 tutorial</li> </ul>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This work was supported by the Engineering and Physical Sciences Research Council [EP/X031829/1].</p> <p>This work used the DiRAC@Durham facility managed by the Institute for Computational  Cosmology on behalf of the STFC DiRAC HPC Facility (www.dirac.ac.uk). The equipment  was funded by BEIS capital funding via STFC capital grants ST/P002293/1, ST/R002371/1 and ST/S002502/1, Durham University and STFC operations grant ST/R000832/1.  DiRAC is part of the National e-Infrastructure.</p> <p>The main outcomes of this work were published in a paper in the HPCTESTS workshop in SC23.</p> <p>This work was presented in RSECon23. A recording of the talk is available.</p>"},{"location":"contributing/","title":"How to contribute to ExCALIBUR tests","text":"<p>You are welcome to contribute new application benchmarks and new systems part of the ExCALIBUR benchmarking effort. The easiest way to contribute is to open an issue or a pull request to the repository.</p>"},{"location":"contributing/#adding-new-benchmarks","title":"Adding new benchmarks","text":"<p>In particular, adding new benchmarks that are useful to the scientific community is welcome!</p>"},{"location":"contributing/#spack-package","title":"Spack package","text":"<p>Before adding a new benchmark, make sure the application is available in Spack.  If it is not, you can read the Spack Package Creation Tutorial to contribute a new recipe to build the application.</p> <p>While we encourage users to contribute all Spack recipes upstream, we have a custom repo for packages not yet ready to be contributed to the main Spack repository. This is in the <code>spack/repo</code> directory, create a subdirectory inside <code>spack/repo/packages</code> with the name of the package you want to add, and place into it the <code>package.py</code> Spack recipe. On supported HPC systems, this repo is automatically added to the provided Spack environments.</p> <p>In Spack recipes, please avoid cloning the head of a branch. The state of the branch at the time it was cloned will not get recorded by Spack or ReFrame which leads to  issues with reproducibility. It is strongly recommended to only build tagged versions of  packages with Spack.</p>"},{"location":"contributing/#reframe-benchmark","title":"ReFrame benchmark","text":"<p>New benchmarks should be added in the <code>apps/</code> directory, under the specific application subdirectory.  Please, add also a <code>README.md</code> file explaining what the application does and how to run the benchmarks in the same directory. Then, link the <code>README.md</code> file under <code>nav: Supported Benchmarks:</code> in the mkdocs documentation config.</p> <p>For writing ReFrame benchmarks you can read the documentation, in particular</p> <ul> <li>ReFrame Tutorials</li> <li>Regression Test API</li> </ul> <p>but you can also have a look at the sombrero example.</p> <p>For GPU benchmarks you need to</p> <ul> <li>set <code>valid_systems = ['+gpu']</code></li> <li>set the <code>num_gpus_per_node</code> attribute,</li> <li>and add the key <code>gpu</code> to the <code>extra_resources</code> dictionary to request the appropriate number of GPUs.</li> </ul> <p>For an example of a GPU benchmark take a look at OpenMM.</p>"},{"location":"contributing/#adding-new-systems","title":"Adding new systems","text":"<p>If you configure the framework on a HPC system that is not included in supported systems, please upen a pull request to upload the configuration so that other users can benefit from it. To add a new system, consider the following items</p> <ul> <li>ReFrame configuration</li> <li>Spack configuration</li> <li>Documentation</li> </ul>"},{"location":"install/","title":"Installation","text":""},{"location":"install/#excalibur-tests","title":"Excalibur-tests","text":""},{"location":"install/#requirements","title":"Requirements","text":"<p>Python version 3.7 or later is required. </p>"},{"location":"install/#virtual-environments","title":"Virtual environments","text":"<p>On most systems, it is recommended to install  the package in a virtual environment. For example, using the python3  built-in virtual environment tool <code>venv</code>, create an environment called <code>my_environment</code> with</p> <pre><code>python3 -m venv ./my_environment\n</code></pre> <p>and activate it with</p> <pre><code>source ./my_environment/bin/activate\n</code></pre>"},{"location":"install/#installation_1","title":"Installation","text":"<p>First, clone the git repository</p> <pre><code>git clone https://github.com/ukri-excalibur/excalibur-tests.git\n</code></pre> <p>Install the excalibur-tests package and the necessary dependencies with <code>pip</code> by</p> <pre><code>pip install -e ./excalibur-tests\n</code></pre>"},{"location":"install/#notes","title":"Notes","text":"<p>The <code>-e/--editable</code> flag is recommended for two reasons.</p> <ul> <li>Spack installs packages in a <code>opt</code> directory under the spack environment. With <code>-e</code> the spack environment remains in your local directory and <code>pip</code> creates symlinks to it. Without <code>-e</code> spack will install packages inside your python environment.</li> <li>For development, the <code>-e</code> flag to <code>pip</code> links the installed package to the files in the local directory, instead of copying, to allow making changes to the installed package.</li> </ul> <p>Note that to use <code>-e</code> with a project configured with a <code>pyproject.toml</code> you need <code>pip</code> version 22 or later.</p>"},{"location":"install/#spack","title":"Spack","text":"<p>The <code>pip install</code> command will install a compatible version of ReFrame from PyPi. However, you will have to manually provide an installation of Spack.</p> <p>Spack is a package manager specifically designed for HPC facilities. In some HPC facilities there may be already a central Spack installation available. However, the version installed is most likely too old to support all the features used by this package. Therefore we recommend you install the latest version locally, following the instructions below.</p> <p>Follow the official instructions to install the latest version of Spack (summarised here for convenience, but not guaranteed to be up-to-date):</p>"},{"location":"install/#installation_2","title":"Installation","text":"<p>Git clone the spack repository <pre><code>git clone -c feature.manyFiles=true https://github.com/spack/spack.git\n</code></pre> Run spack setup script  <pre><code>source ./spack/share/spack/setup-env.sh\n</code></pre> Check spack is in <code>$PATH</code>, for example  <pre><code>spack --version\n</code></pre></p>"},{"location":"install/#version","title":"Version","text":"<p>We recommend using the latest spack version (develop). Stable spack releases starting from v0.20.0  are supported. Older versions of spack may work but are not regularly tested.</p>"},{"location":"install/#notes_1","title":"Notes","text":"<p>Note: if you have already installed spack locally and you want to upgrade to a newer version, you might first have to clear the cache to avoid conflicts: <code>spack clean -m</code></p> <p>It is recommended to always have the python virtual environment active when working with <code>excalibur-tests</code>. However, it should be noted that since <code>Spack</code> is not installed via <code>pip</code>, it will be installed outside of the python virtual environment. Also, keep in mind that the Spack environments that are discussed in the next section  are different and independent of the python virtual environment.</p>"},{"location":"setup/","title":"Configuration","text":""},{"location":"setup/#pre-configured-systems","title":"Pre-configured systems","text":"<p>A number of UK-based HPC systems that are part of the DiRAC and ExCALIBUR programs  have been pre-configured. See systems, or https://github.com/ukri-excalibur/excalibur-tests/tree/main/benchmarks/spack/ for a list.  On these systems the ReFrame configuration and Spack environment are included in the  <code>excalibur-tests</code> repository and all you need to do is point the framework to them.</p>"},{"location":"setup/#reframe","title":"ReFrame","text":"<p>You can point ReFrame to the provided config file by setting the <code>RFM_CONFIG_FILES</code> environment variable:</p> <pre><code>export RFM_CONFIG_FILES=\"&lt;path-to-excalibur-tests&gt;/benchmarks/reframe_config.py\"\n</code></pre> <p>If you want to use a different ReFrame configuration file, for example because you use a different system, you can set this environment variable to the path of that file.</p> <p>Note: in order to use the Spack build system in ReFrame, the <code>spack</code> executable must be in the <code>PATH</code> also on the compute nodes of a cluster, if you want to run your benchmarks on them. This is taken care of by adding it to your init file (see spack section below).</p> <p>However, you will also need to set the <code>RFM_USE_LOGIN_SHELL</code> environment variable  <pre><code>export RFM_USE_LOGIN_SHELL=\"true\"\n</code></pre> in order to make ReFrame use <code>!#/bin/bash -l</code> as  shebang line, which would load the user's init script.</p>"},{"location":"setup/#spack","title":"Spack","text":"<p>In order to use Spack in ReFrame,  the directory where the <code>spack</code> program is installed needs to be in the <code>PATH</code> environment variable. This is taken care of by the <code>setup-env.sh</code> script run in install. To have spack available in every session, you can have your shell init script (e.g. <code>.bashrc</code>) re-run it automatically, by adding the following lines to it: <pre><code>export SPACK_ROOT=\"/path/to/spack\"\nif [ -f \"${SPACK_ROOT}/share/spack/setup-env.sh\" ]; then\n    source \"${SPACK_ROOT}/share/spack/setup-env.sh\"\nfi\n</code></pre> replacing <code>/path/to/spack</code> with the actual path to your Spack installation.</p>"},{"location":"setup/#new-systems","title":"New systems","text":"<p>We need the ReFrame configuration and a Spack environment for a new system.</p>"},{"location":"setup/#reframe_1","title":"ReFrame","text":"<p>Add a new system to the ReFrame configuration in <code>benchmarks/reframe_config.py</code>.  Read ReFrame documentation about configuration for more details, or see the examples of the existing systems.  </p> <p>Note: you likely do not need to customise the programming environment in ReFrame, as we will use Spack as build system, which will deal with that.</p> <p>If available, the command <code>lscpu</code>, run on a compute node, is typically useful to get information about the CPUs, to be used in the <code>processor</code> item of the system configuration.  The numbers you need to watch out for are:</p> <ul> <li>\"CPU(s)\", (<code>num_cpus</code> in ReFrame configuration),</li> <li>\"Thread(s) per core\", (<code>num_cpus_per_core</code>),</li> <li>\"Socket(s)\", (<code>num_sockets</code>),</li> <li>\"Core(s) per socket\", (<code>num_cpus_per_socket</code>).</li> </ul>"},{"location":"setup/#spack_1","title":"Spack","text":"<p>When using Spack as build system, ReFrame needs a Spack environment to run its tests. The Spack environment is separate and independent of the python virtual environment. Follow these steps to create a Spack environment for a new system:</p>"},{"location":"setup/#create-the-environment","title":"Create the environment","text":"<p><pre><code>spack env create --without-view -d /path/to/environment\n</code></pre> Remember to disable views with <code>--without-view</code> to avoid conflicts when installing incompatible packages in the same environment</p>"},{"location":"setup/#activate-the-environment","title":"Activate the environment","text":"<pre><code>spack env activate -d /path/to/environment\n</code></pre>"},{"location":"setup/#set-install_tree","title":"Set <code>install_tree</code>","text":"<pre><code>spack config add 'config:install_tree:root:$env/opt/spack'\n</code></pre>"},{"location":"setup/#add-compilers","title":"Add compilers","text":"<p>Make sure the compilers you want to add are in the <code>PATH</code> (e.g., load the relevant modules), then add them to the Spack environment with: <pre><code>spack compiler find\n</code></pre></p>"},{"location":"setup/#add-external-packages","title":"Add external packages","text":"<p>Add other packages (e.g., MPI): make sure the package you want to add are \"visible\" (e.g., load the relevant modules) and add them to the environment with <pre><code>spack external find PACKAGE-NAME ...\n</code></pre> where <code>PACKAGE-NAME ...</code> are the names of the corresponding Spack packages</p>"},{"location":"setup/#set-excalibur_spack_env-variable","title":"Set <code>EXCALIBUR_SPACK_ENV</code> variable","text":"<p>To use the new Spack environment in ReFrame, set the environment variable <code>EXCALIBUR_SPACK_ENV</code> to the path of the directory where the environment is, i.e. <pre><code>export EXCALIBUR_SPACK_ENV=/path/to/environment\n</code></pre> If this is not set, ReFrame will try to use the environment for the current system if known, otherwise it will automatically create a very basic environment (see Usage on unsupported systems.</p>"},{"location":"setup/#optional-make-manual-tweaks","title":"(optional) Make manual tweaks","text":"<p>If necessary, manually tweak the environment. For example, if there is already a global Spack available in the system, you can include its configuration files, or add its install trees as upstreams.</p>"},{"location":"setup/#optional-add-spack-repositories","title":"(optional) Add spack repositories","text":"<p>If you are using a custom repo for spack package recipes (see Spack package below), add it to the spack environment with <pre><code>spack -e /path/to/environment repo add /path/to/repo\n</code></pre></p>"},{"location":"setup/#optional-override-default-spack-cache-path","title":"(optional) Override default spack cache path","text":"<p>Spack also, by default, keeps various caches and user data in <code>~/.spack</code>, but users may want to override these locations. Spack provides environment variables that allow you to override or opt out of configuration locations:</p> <p><code>SPACK_USER_CONFIG_PATH</code>: Override the path to use for the user scope (<code>~/.spack</code> by default).</p> <p><code>SPACK_SYSTEM_CONFIG_PATH</code>: Override the path to use for the system scope (<code>/etc/spack</code> by default).</p> <p><code>SPACK_USER_CACHE_PATH</code>: Override the default path to use for user data (misc_cache, tests, reports, etc.)</p> <p>For more details, see spack docs.</p>"},{"location":"systems/","title":"System-specific information","text":"<p>This framework strives to be as system-independent as possible, but there are some platform-specific details that you may need be aware of when running these benchmarks. Below we collect some information you may want to keep in mind on the different systems.</p>"},{"location":"systems/#archer2","title":"ARCHER2","text":""},{"location":"systems/#home-partition","title":"Home partition","text":"<p>ARCHER2 uses the standard Cray setup for which the home partition is not mounted on compute nodes, read Data management and transfer for more details. You likely want to install this benchmarking framework outside of your home directory, for example inside <code>/work/&lt;project code&gt;/&lt;project code&gt;/${USER}</code>, where <code>&lt;project code&gt;</code> is your project code.</p>"},{"location":"systems/#queue-options","title":"Queue options","text":"<p>When submitting jobs to compute nodes, you need to specify the job queue and maybe the project account. The former is specified by setting the Quality of Service, the latter is necessary if your user account is associated to multiple projects on ARCHER2 and you need to specify which one to use for the submitted jobs. We cannot automatically set these options for you because they are user-specific, but when you run a benchmark you can use the <code>-J</code>/<code>--job-option</code> flag to <code>reframe</code> to add new job options. Some examples:</p> <pre><code>reframe -c benchmarks/examples/sombrero -r --performance-report --system archer2 -J'--qos=serial'\nreframe -c benchmarks/examples/sombrero -r --performance-report --system archer2 -J'--qos=short' -J'--account=t01'\n</code></pre>"},{"location":"systems/#controlling-cpu-frequency","title":"Controlling CPU frequency","text":"<p>ARCHER2 allows choosing the CPU frequency during jobs by setting the environment variable <code>SLURM_CPU_FREQ_REQ</code> to specific values. In ReFrame v3 the list of environment variables set by the framework is held by the dictionary attribute called <code>env_vars</code>, and you can initialise it on the command line when running a benchmark with <code>-S</code>/<code>--setvar</code>. For more details, see Setting environment variables in usage. For example, to submit a benchmark using the lowest CPU frequency (1.5 GHz) you can use</p> <pre><code>reframe -c benchmarks/examples/sombrero -r --performance-report --system archer2 -J'--qos=serial' -S env_vars=SLURM_CPU_FREQ_REQ:1500000\n</code></pre>"},{"location":"systems/#using-python","title":"Using python","text":"<p>ARCHER2 is a Cray system, and they recommend using a cray optimised python version. The HPE Cray Python distribution can be loaded using <code>module load cray-python</code>. This is necessary to pip install excalibur-tests following the instructions in install.</p>"},{"location":"systems/#spack-install-path","title":"Spack install path","text":"<p>Spack has a limitation of 127 characters on the length of the path of the install tree. Because the path to the work directory on Archer2 is fairly long, and pip by default installs into <code>&lt;path/to/virtual/environment&gt;/pythonx.x/site-packages/</code>we may exceed the limit when installing to the default directory. If you see an error in ReFrame beginning with</p> <pre><code>==&gt; Error: SbangPathError: Install tree root is too long.\n</code></pre> <p>A possible work-around is to provide a shorter installation path to <code>pip</code>. Pass the installation path to <code>pip install</code> using <code>--target</code>, for example, <code>pip install --target = /work/&lt;project_code&gt;/&lt;project_code&gt;/&lt;username&gt;/pkg .</code>. Then add  the <code>bin</code> subdirectory to <code>$PATH</code>, for example, <code>export PATH = /work/&lt;project_code&gt;/&lt;project_code&gt;/&lt;username&gt;/pkg/bin:$PATH</code>.</p>"},{"location":"systems/#csd3","title":"CSD3","text":""},{"location":"systems/#queue-options_1","title":"Queue options","text":"<p>When submitting jobs to compute nodes, you need to specify the job queue, with the <code>--account</code> option to the scheduler. To do this, when you run a benchmark you can use the <code>-J</code>/<code>--job-option</code> flag to <code>reframe</code> to specify the account, for example:</p> <pre><code>reframe -c benchmarks/examples/sombrero -r --performance-report --system csd3-skylake:compute-node -J'--account=&lt;ACCOUNT&gt;'\n</code></pre> <p>where <code>&lt;ACCOUNT&gt;</code> is the project you want to charge. You can see the account balance of your projects with the <code>mybalance</code> command.</p>"},{"location":"systems/#cosma8","title":"Cosma8","text":""},{"location":"systems/#queue-options_2","title":"Queue options","text":"<p>When submitting jobs to compute nodes, you need to specify the job queue, with the <code>--account</code> option to the scheduler. To do this, when you run a benchmark you can use the <code>-J</code>/<code>--job-option</code> flag to <code>reframe</code> to specify the account, for example:</p> <pre><code>reframe -c benchmarks/examples/sombrero -r --performance-report --system cosma8:compute-node -J'--account=&lt;ACCOUNT&gt;'\n</code></pre> <p>where <code>&lt;ACCOUNT&gt;</code> is the project you want to charge. DiRAC users can find the account codes they have access to on SAFE.</p>"},{"location":"systems/#dial2","title":"DIaL2","text":""},{"location":"systems/#queue-options_3","title":"Queue options","text":"<p>When submitting jobs to compute nodes, you need to specify the job queue, with the <code>--account</code> option to the scheduler. To do this, when you run a benchmark you can use the <code>-J</code>/<code>--job-option</code> flag to <code>reframe</code> to specify the account, for example:</p> <pre><code>reframe -c benchmarks/examples/sombrero -r --performance-report --system dial2:compute-node -J'--account=&lt;ACCOUNT&gt;'\n</code></pre> <p>where <code>&lt;ACCOUNT&gt;</code> is the project you want to charge.</p> <p>Note: for exclusive access require to pass -J='-l naccesspolicy=SINGLEJOB -n'</p>"},{"location":"systems/#dial3","title":"DIaL3","text":""},{"location":"systems/#queue-options_4","title":"Queue options","text":"<p>When submitting jobs to compute nodes, you need to specify the job queue, with the <code>--account</code> option to the scheduler. To do this, when you run a benchmark you can use the <code>-J</code>/<code>--job-option</code> flag to <code>reframe</code> to specify the account, for example:</p> <pre><code>reframe -c benchmarks/examples/sombrero -r --performance-report --system dial3:compute-node -J'--account=&lt;ACCOUNT&gt;'\n</code></pre> <p>where <code>&lt;ACCOUNT&gt;</code> is the project you want to charge.</p>"},{"location":"systems/#isambard-2","title":"Isambard 2","text":""},{"location":"systems/#multi-architecture-comparison-system-macs-partition","title":"Multi-Architecture Comparison System (MACS) partition","text":""},{"location":"systems/#compilation-on-compute-nodes","title":"Compilation on compute nodes","text":"<p>Login nodes on the Isambard 2 MACS partition have Intel \"Broadwell\" CPUs, but most of the compute nodes use CPUs of different microarchitecture, which means that you cannot directly compile optimised code for the compute nodes with Spack while on the login nodes. To run compilation on the compute node, you have to set the attribute <code>build_locally</code> to <code>false</code> with <code>-S build_locally=false</code>, for example:</p> <pre><code>reframe -c benchmarks/examples/sombrero -r --performance-report --system isambard-macs:cascadelake -S build_locally=false\nreframe -c benchmarks/examples/sombrero -r --performance-report --system isambard-macs:rome -S build_locally=false\n</code></pre> <p>You may also need to compile GPU applications on the compute nodes, as the login node does not have any GPUs (this really depends on the build system of the application at hand, whether it needs access to a GPU during the build or it is sufficient to have the GPU toolkit available).</p>"},{"location":"systems/#myriad-and-kathleen","title":"Myriad and Kathleen","text":""},{"location":"systems/#python3-module","title":"Python3 module","text":"<p>The only default Python in the system is currently Python 2.7, but this may change in the future. We require Python v3.7 or later so you need to have <code>python3</code> available. This is provided by the <code>python3</code> module in the system. The <code>python3/recommended</code> module on myriad is built with an incompatible version of <code>openssl</code> for ReFrame. The easiest thing to do is to add the lines</p> <pre><code>module load python3/3.11\nexport RFM_USE_LOGIN_SHELL=\"True\"\n</code></pre> <p>to your shell init script (e.g. <code>~/.bashrc</code>). The second line tells ReFrame to always load the shell init script when running the jobs, so that the Python3 module is available also during the jobs, to run Spack.</p>"},{"location":"systems/#temporary-directory-for-building-packages-with-spack","title":"Temporary directory for building packages with Spack","text":"<p>Before moving it to the final installation place, Spack builds software in a temporary directory. By default on Myriad this is <code>/tmp</code>, but this directory is shared with other users and its partition is relatively small, so that building large software may always end up filling the entire disk, resulting in frequent <code>No space left on device</code> errors. To work around this issue you can use as temporary directory the one pointed to by <code>XDG_RUNTIME_DIR</code>, which use a larger partition, reserved only to your user. Note that this directory is automatically cleaned up after you log all of your sessions out of the system. You can add the following line to your shell init script (e.g., <code>~/.bashrc</code>) to make <code>TMPDIR</code> use <code>XDG_RUNTIME_DIR</code>, unless otherwise set:</p> <pre><code>export TMPDIR=\"${TMPDIR:-${XDG_RUNTIME_DIR:-/tmp}}\"\n</code></pre>"},{"location":"systems/#tursa","title":"Tursa","text":""},{"location":"systems/#queue-options_5","title":"Queue options","text":"<p>When submitting jobs to compute nodes, you need to specify the job queue, with the <code>--account</code> option to the scheduler. To do this, when you run a benchmark you can use the <code>-J</code>/<code>--job-option</code> flag to <code>reframe</code> to specify the account, for example:</p> <pre><code>reframe -c benchmarks/examples/sombrero -r --performance-report --system tursa:compute-node -J'--account=&lt;ACCOUNT&gt;'\n</code></pre> <p>where <code>&lt;ACCOUNT&gt;</code> is the project you want to charge.</p>"},{"location":"use/","title":"Usage","text":""},{"location":"use/#running-benchmarks","title":"Running benchmarks","text":"<p>Once you have set up Spack and ReFrame, you can execute a benchmark with</p> <pre><code>reframe -c benchmarks/apps/BENCH_NAME -r\n</code></pre> <p>where <code>benchmarks/apps/BENCH_NAME</code> is the directory where the benchmark is.  The command above assumes you have the program <code>reframe</code> in your PATH.  If you have followed the instructions to install using <code>pip</code> into the default directory, it should have been automatically added. If it is not the case, call <code>reframe</code> with its relative or absolute path.</p> <p>For example, to run the Sombrero benchmark in the <code>benchmarks/apps/sombrero</code> directory you can use</p> <pre><code>reframe -c benchmarks/apps/sombrero -r\n</code></pre>"},{"location":"use/#setting-reframe-command-line-options","title":"Setting ReFrame command line options","text":"<p>ReFrame supports a variety of command-line options that can be useful, or sometimes necessary.</p>"},{"location":"use/#system-specific-options","title":"System-specific options","text":"<p>While the aim is to automate as much system-specific configuration as possible, there are some options that have to be provided by the user, such as accounting details, and unfortunately the syntax can vary. See systems for information about the use of this framework on specific systems.</p>"},{"location":"use/#performance-report","title":"Performance report","text":"<p>You can use the <code>--performance-report</code> command-line option to ReFrame to get a nicely formatted performance report after the benchmark has completed.</p>"},{"location":"use/#selecting-spack-build-spec","title":"Selecting Spack build spec","text":"<p>For benchmarks that use the Spack build system, the tests define a default Spack specification to be installed in the environment, but users can change it when invoking ReFrame on the command line with the <code>-S</code> option to set the <code>spack_spec</code> variable:</p> <pre><code>reframe -c benchmarks/apps/sombrero -r --performance-report -S spack_spec='sombrero@2021-08-16%intel'\n</code></pre>"},{"location":"use/#selecting-system-and-queue-access-options","title":"Selecting system and queue access options","text":"<p>The provided ReFrame configuration file contains the settings for multiple systems.  If you use it, the automatic detection of the system may fail, as some systems may use clashing hostnames.  To avoid this, you can use the flag <code>--system NAME:PARTITION</code> to specify the system (and optionally the partition) to use.</p> <p>Additionally, if submitting jobs to the compute nodes requires additional options, like for example the resource group you belong to (for example <code>--account=...</code> for Slurm), you have to pass the command line flag <code>--job-option=...</code> to <code>reframe</code> (e.g., <code>--job-option='--account=...'</code>).</p>"},{"location":"use/#setting-environment-variables","title":"Setting environment variables","text":"<p>All the built-in fields of ReFrame regression classes can be set on a per-job basis using the <code>-S</code> command-line option. One useful such field is <code>env_vars</code>, which controls the environment variables used in a job. The syntax to set dictionary items, like for <code>env_vars</code>, is a comma-separated list of <code>key:value</code> pairs: <code>-S dict=key_1:value_1,key_2:value_2</code>. For example</p> <pre><code>reframe -c benchmarks/apps/sombrero -r --performance-report -S env_vars=OMP_PLACES:threads\n</code></pre> <p>runs the <code>benchmarks/apps/sombrero</code> benchmark setting the environment variable <code>OMP_PLACES</code> to <code>threads</code>.</p>"},{"location":"use/#output-directories","title":"Output directories","text":"<p>By default <code>reframe</code> creates three output directories (<code>stage</code>, <code>output</code> and <code>perflogs</code>) in the directory  where it is run. Output can be written to a different base directory using the <code>--prefix</code> command-line option.</p> <p>The individual output directories can also be changed using the <code>--stage</code>, <code>--outputdir</code> and <code>--perflogdir</code> options.</p>"},{"location":"use/#usage-on-unsupported-systems","title":"Usage on unsupported systems","text":"<p>The configuration provided in <code>reframe_config.py</code> lets you run the benchmarks on pre-configured HPC systems.  However you can use this framework on any system by choosing the \"default\" system with <code>--system default</code>, or by using your own ReFrame configuration.  You can use the \"default\" system to run benchmarks in ReFrame without using a queue manager or an MPI launcher (e.g. on a personal workstation).</p> <p>If you choose the \"default\" system and a benchmark using the Spack build system, a new empty Spack environment will be automatically created in <code>benchmarks/spack/default</code> when ReFrame is launched for the first time. You should populate the environment with the packages already installed on your system before running Spack to avoid excessively rebuilding system packages. See setup for instructions on how to set up a Spack environment. In particular, make sure that at least a compiler and an MPI library are added into the environment. After the Spack environment is set up, tell ReFrame to use it by setting the environment variable <code>EXCALIBUR_SPACK_ENV</code>, as described in setup.</p>"},{"location":"use/#selecting-multiple-benchmarks","title":"Selecting multiple benchmarks","text":"<p>ReFrame tests may contain tags that allow the user to select which tests to run. These can be leveraged to defined sets of benchmarks. To run all tests in a directory, pass the <code>-R</code> flag to ReFrame. Then filter down to a specific tag by passing the <code>-t</code> flag.</p> <p>For example, the tag \"example\" is defined in the sombrero example. To select the sombrero example out of all benchmarks, run</p> <pre><code>reframe -c benchmarks/ -R -r -t example\n</code></pre> <p>Tests can contain multiple tags. To create a custom set of benchmarks, add a new tag to the tests you want to include in the set.</p>"},{"location":"use/#running-a-benchmark-with-a-profiler-experimental","title":"Running a benchmark with a profiler (experimental)","text":"<p>Experimental feature</p> <p>This is an experimental feature and its interface may be changed in the future on short notice. Feedback to improve it is welcome!</p> <p>This framework allows you to run a profiler together with a benchmark application. To do this, you can set the <code>profiler</code> attribute on the command line using the <code>-S profiler=...</code> syntax, e.g.</p> <pre><code>reframe -c benchmarks/ -R -r -t example -S profiler=vtune\n</code></pre> <p>Currently supported values for the profiler attribute are:</p> <ul> <li><code>advisor-roofline</code>: it produces a roofline model of your program using Intel Advisor;</li> <li><code>nsight</code>: it runs the code with the NVIDIA Nsight Systems profiler;</li> <li><code>vtune</code>: it runs the code with the Intel VTune profiler.</li> </ul> <p>Availability and requirements</p> <p>Many profilers, especially those provided by vendors, are available only on some platforms (e.g. Intel profilers are only available on the x86 architecture), double check if the Spack package for your desired profiler is available for the system you want to use. Furthermore, to collect useful information with some profilers you may need specific values of the Linux <code>perf_event_paranoid</code> setting, typically less than or equal to 2, you can check the value on a specific node by reading the content of the file <code>/proc/sys/kernel/perf_event_paranoid</code>.</p> <p>The profiler trace collected during the run will be automatically copied to the output directory after a successful run. Toward the bottom of the standard output file (<code>rfm_job.out</code>) you should find some information about how to visualise the profiling trace with the tools corresponding to the profiler used.</p> <p>Running graphic visualisers</p> <p>Using graphic visualisers requires being able to run graphic applications on the machine where you ran the benchmakrs, in the case of remote ones you will need to ensure your connection supports forwarding graphic applications, and consider that the interaction may be slow due to the network latency. An alternative is to copy the profile trace to your local machine and using a locally-installed visualiser.</p>"},{"location":"apps/","title":"Supported benchmarks","text":"<p>This directory contains the benchmarks currently supported by the project. More can be added by opening a Pull Request following the guidance in contributing.</p>"},{"location":"apps/babelstream/","title":"BabelStream benchmarks","text":"<p>BabelStream </p> <p>Measure memory transfer rates to/from global device memory on GPUs. This benchmark is similar in spirit, and based on, the STREAM benchmark [1] for CPUs. Unlike other GPU memory bandwidth benchmarks this does not include the PCIe transfer time. There are multiple implementations of this benchmark in a variety of programming models. This code was previously called GPU-STREAM.</p>"},{"location":"apps/babelstream/#usage","title":"Usage","text":"<p>From the top-level directory of the repository, you can run the benchmarks with</p> <pre><code>reframe -c benchmarks/apps/babelstream -r  --tag &lt;TAG&gt; --system=&lt;ENV:PARTITION&gt; -Sbuild_locally=false -Sspack_spec='babelstream +tag &lt;extra flags&gt;'\n</code></pre>"},{"location":"apps/babelstream/#filtering-the-benchmarks","title":"Filtering the benchmarks","text":"<p>The Spack directives for the babelstream could be found here You can run individual benchmarks with the <code>--tag</code> option:</p> <ul> <li><code>omp</code>to run the <code>OpenMP</code> benchmark.</li> <li><code>ocl</code> to run the <code>OpenCL</code> benchmark.</li> <li><code>std</code> to run the <code>STD</code> benchmark.</li> <li><code>std20</code>to run the <code>STD20</code> benchmark.</li> <li><code>hip</code> to run the <code>HIP</code> benchmark.</li> <li><code>cuda</code>to run the <code>CUDA</code> benchmark.</li> <li><code>kokkos</code> to run the <code>Kokkos</code> benchmark.</li> <li><code>sycl</code> to run the <code>SYCL</code> benchmark.</li> <li><code>sycl2020</code> to run the <code>SYCL2020</code> benchmark.</li> <li><code>acc</code> to run the <code>ACC</code> benchmark.</li> <li><code>raja</code> to run the <code>RAJA</code> benchmark.</li> <li><code>tbb</code> to run the <code>TBB</code> benchmark.</li> <li><code>thrust</code> to run the <code>THRUST</code> benchmark,</li> </ul> <p>Examples:</p> <pre><code>reframe -c benchmarks/apps/babelstream -r --tag omp --system=isambard-macs:volta -S build_locally=false -S spack_spec='babelstream%gcc@9.2.0 +omp cuda_arch=70'\nreframe -c benchmarks/apps/babelstream -r --tag tbb --system=isambard-macs:cascadelake -S build_locally=false -S spack_spec='babelstream@develop +tbb'\nreframe -c benchmarks/apps/babelstream -r --tag cuda --system=isambard-macs:volta -S build_locally=false -S spack_spec='babelstream@develop%gcc@9.2.0 +cuda cuda_arch=70'\n</code></pre>"},{"location":"apps/babelstream/#setting-the-number-of-threads-and-mpi-processes","title":"Setting the number of threads and MPI processes","text":"<p>By default, these benchmarks will use</p> <ul> <li>[<code>num_gpus_per_node</code>](https://reframe-hpc.readthedocs.io/en/stable/regression_test_api.html#reframe.core.pipeline.RegressionTest.num_gpus_per_node: This value is by default 1 for the benchmarks requiring GPU. (e.g. CUDA,HIP) </li> </ul> <p>You can override the value of this variable from the command line with the <code>--setvar</code> option, for example</p> <pre><code>reframe -c benchmarks/apps/babelstream -r --tag cuda --system=isambard-macs:volta -S build_locally=false -S spack_spec='babelstream@develop%gcc@9.2.0 +cuda cuda_arch=70' --setvar=num_gpus_per_node=2\n</code></pre> <p>Note: you're responsible for overriding this variable in a consistent way, so that, for example, <code>num_gpus_per_node</code> doesn't exceed the number of total GPUs runnable on each node.</p>"},{"location":"apps/babelstream/#figure-of-merit","title":"Figure of merit","text":"<p>The figure of merit captured by these benchmarks is the bandwidth. For example, if the output of the program is</p> <pre><code>BabelStream\nVersion: 4.0\nImplementation: OpenMP\nRunning kernels 100 times\nPrecision: double\nArray size: 268.4 MB (=0.3 GB)\nTotal size: 805.3 MB (=0.8 GB)\nFunction    MBytes/sec  Min (sec)   Max         Average     \nCopy        91018.241   0.00590     0.01087     0.00721     \nMul         80014.622   0.00671     0.01173     0.00837     \nAdd         92644.967   0.00869     0.01636     0.01121     \nTriad       93484.396   0.00861     0.01416     0.01142     \nDot         114688.364  0.00468     0.01382     0.00707\n</code></pre> <p>the output numbers </p> <pre><code>Copy : 91018.241\nMul : 80014.622\nAdd : 92644.967 \nTriad : 93484.396\nDot : 114688.364\n</code></pre> <p>will be captured.</p>"},{"location":"apps/cp2k/","title":"CP2K benchmarks","text":"<p>CP2K is a quantum chemistry and solid state physics software package. This directory includes the <code>H2O-64</code>, <code>H20-256</code>, and <code>LiH_HFX</code> CP2K benchmarks based on ARCHER 2 HPC benchmarks.</p>"},{"location":"apps/cp2k/#usage","title":"Usage","text":"<p>From the top-level directory of the repository, you can run the benchmarks with</p> <pre><code>reframe -c benchmarks/apps/cp2k -r --performance-report\n</code></pre>"},{"location":"apps/cp2k/#filtering-the-benchmarks","title":"Filtering the benchmarks","text":"<p>By default all benchmarks will be run. You can run individual benchmarks with the <code>--tag</code> option:</p> <ul> <li><code>h2o-64</code> to run the <code>H2O-64</code> benchmark,</li> <li><code>h2o-256</code> to run the <code>H2O-256</code> benchmark,</li> <li><code>lih-hfx</code> to run the <code>LiH_HFX</code> benchmark.</li> </ul> <p>Examples:</p> <pre><code>reframe -c benchmarks/apps/cp2k -r --performance-report --tag h2o-64\nreframe -c benchmarks/apps/cp2k -r --performance-report --tag h2o-256\nreframe -c benchmarks/apps/cp2k -r --performance-report --tag lih-hfx\n</code></pre>"},{"location":"apps/cp2k/#setting-the-number-of-threads-and-mpi-processes","title":"Setting the number of threads and MPI processes","text":"<p>By default, these benchmarks will use</p> <ul> <li><code>num_cpus_per_task</code>:   2</li> <li><code>num_tasks</code>:   <code>current_partition.processor.num_cpus // min(1, current_partition.processor.num_cpus_per_core) // num_cpus_per_task</code></li> <li><code>num_tasks_per_node</code>: <code>current_partition.processor.num_cpus // num_cpus_per_task</code></li> </ul> <p>You can override the values of these variables from the command line with the <code>--setvar</code> option, for example</p> <pre><code>reframe -c benchmarks/apps/cp2k -r --performance-report --setvar=num_cpus_per_task=4 --setvar=num_tasks=16\n</code></pre> <p>Note: you're responsible for overriding these variables in a consistent way, so that, for example, <code>num_tasks_per_node</code> doesn't exceed the number of total tasks runnable on each node.</p>"},{"location":"apps/cp2k/#figure-of-merit","title":"Figure of merit","text":"<p>The figure of merit captured by these benchmarks is the maximum total CP2K time. For example, if the output of the program is</p> <pre><code> -------------------------------------------------------------------------------\n -                                                                             -\n -                                T I M I N G                                  -\n -                                                                             -\n -------------------------------------------------------------------------------\n SUBROUTINE                       CALLS  ASD         SELF TIME        TOTAL TIME\n                                MAXIMUM       AVERAGE  MAXIMUM  AVERAGE  MAXIMUM\n CP2K                                 1  1.0    0.178    0.295  200.814  200.816\n qs_energies                          1  2.0    0.000    0.000  200.091  200.093\n scf_env_do_scf                       1  3.0    0.000    0.000  198.017  198.018\n qs_ks_update_qs_env                  8  5.0    0.000    0.000  161.422  161.440\n rebuild_ks_matrix                    7  6.0    0.000    0.000  161.419  161.437\n qs_ks_build_kohn_sham_matrix         7  7.0    0.001    0.001  161.419  161.437\n hfx_ks_matrix                        7  8.0    0.000    0.000  154.464  154.495\n</code></pre> <p>the number <code>200.816</code> will be captured.</p>"},{"location":"apps/grid/","title":"GRID","text":"<p>ReFrame benchmarks for the GRID code, a data parallel C++ mathematical object library.</p>"},{"location":"apps/grid/#usage","title":"Usage","text":"<p>From the top-level directory of the repository, you can run the benchmarks with</p> <pre><code>reframe -c benchmarks/apps/grid -r --performance-report\n</code></pre>"},{"location":"apps/grid/#filtering-the-benchmarks","title":"Filtering the benchmarks","text":"<p>You can run individual benchmarks with the <code>--tag</code> option.  At the moment we have the following tags:</p> <ul> <li><code>ITT</code> to run the <code>Benchmark_ITT</code> application.</li> </ul> <p>Examples:</p> <pre><code>reframe -c benchmarks/apps/grid -r --performance-report --tag ITT\n</code></pre>"},{"location":"apps/grid/#options-memory-number-of-threads-and-mpi-processes","title":"Options (memory, number of threads and MPI processes)","text":"<p>There are some options you can set to control the settings of the benchmark. These are the currently available options, with their default values:</p> <ul> <li><code>mpi</code>: <code>'1.1.1.1'</code>.  This is the string to pass to the benchmarking applications with the   <code>--mpi</code> flag.  This will also automatically set the ReFrame variable   <code>num_tasks</code></li> <li><code>num_cpus_per_task</code>:   <code>current_partition.processor.num_cpus // min(1, current_partition.processor.num_cpus_per_core)</code></li> <li><code>num_tasks_per_node</code>:   <code>current_partition.processor.num_cpus // num_cpus_per_task</code></li> <li><code>shm</code>: <code>1024</code>.  This is the size of the shared memory used by the benchmark, in MiB, as an   integer.</li> </ul> <p>You can override the values of these variables from the command line with the <code>--setvar</code> option, for example</p> <pre><code>reframe -c benchmarks/apps/grid -r --performance-report --setvar=mpi='2.2.1.1' --setvar=num_cpus_per_task=12\nreframe -c benchmarks/apps/grid -r --performance-report --setvar=mpi='4.4.4.4' --setvar=shm=4096\n</code></pre> <p>Note: you're responsible for overriding these variables in a consistent way, so that, for example, <code>num_tasks_per_node</code> doesn't exceed the number of total tasks runnable on each node.</p>"},{"location":"apps/grid/#figure-of-merit","title":"Figure of merit","text":"<p>If the output of the program contains</p> <pre><code>Grid : Message : 380809 ms :  Comparison point  result: 143382.7 Mflop/s per node\n</code></pre> <p>the number <code>143382.7</code> will be captured as figure of merit.</p>"},{"location":"apps/gromacs/","title":"GROMACS benchmark","text":"<p>GROMACS is a free and open-source software suite for high-performance molecular dynamics and output analysis. This directory includes a data file, <code>gromacs_1400k_atoms.tpr</code>, which can be used to benchmark both GPU and CPU machines. This data file was taken from https://www.hecbiosim.ac.uk/access-hpc/benchmarks.</p> <p>The benchmarks are designed as strong scaling tests which run across multiple full nodes for a given system.</p>"},{"location":"apps/gromacs/#usage","title":"Usage","text":"<p>Without filtering, all benchmarks (CPU and GPU) will be run. This is unlikely to be what we want. Therefore, from the top-level directory of the repository, you can run the benchmarks using tags to filter for CPU benchmarks.</p> <pre><code>reframe --system &lt;your_system&gt; -c benchmarks/apps/gromacs -r --tag cpu --performance-report\n</code></pre> <p>Or GPU benchmarks.</p> <pre><code>reframe --system &lt;your_system&gt; -c benchmarks/apps/gromacs -r --tag gpu --performance-report\n</code></pre>"},{"location":"apps/gromacs/#filtering-the-benchmarks","title":"Filtering the benchmarks","text":"<p>Note that, since we are running strong scaling tests over multiple nodes, the benchmarks may need to be further filtered if usage rules restrict the size of jobs which can be run on any given system. To do this filtering, use the <code>-n</code> option like so</p> <pre><code>reframe --system &lt;your_system&gt; -c benchmarks/apps/gromacs -r --tag cpu --performance-report -n '.*num_nodes_param=&lt;2|4|8|16&gt;.*'\n</code></pre> <p>Again, depending on the system's rules/machine size, additional filtering may be needed for GPU tests i.e.</p> <pre><code>reframe --system &lt;your_system&gt; -c benchmarks/apps/gromacs -r --tag gpu --performance-report -n '.*num_nodes_param=&lt;2|4|8|16&gt;.*num_gpus_per_node_param=&lt;1|2|4&gt;'\n</code></pre>"},{"location":"apps/gromacs/#figure-of-merit","title":"Figure of Merit","text":"<p>The figure of merit captured by these benchmarks is the performance metric Rate, measured in ns/day. Here, ns/day refers to the number of nanoseconds of simulation that you can do in a day of computation.</p> <p>This output can be found in the <code>md.log</code> file within the output directory produced by Reframe. If the end of this file resembles what is shown below, the Rate <code>4.892</code> would be captured. <pre><code>               Core t (s)   Wall t (s)        (%)\n       Time:    28258.913      353.239     7999.9\n                 (ns/day)    (hour/ns)\nPerformance:        4.892        4.906\nFinished mdrun on rank 0 Tue Apr  1 13:43:36 2025\n</code></pre></p>"},{"location":"apps/hpcg/","title":"HPCG benchmarks","text":"<p>These are based upon the HPCG Conjugate Gradient solver benchmark. At the time of writing, there are three benchmarks in the suite: the original implementation, one which solves the same problem with a hard-coded stencil, and one  which solves a different problem with an LFRic stencil and data.</p>"},{"location":"apps/hpcg/#usage","title":"Usage","text":"<p>From the top-level directory of the repository, you can run the benchmarks with</p> <pre><code>reframe -c benchmarks/apps/hpcg -r --performance-report\n</code></pre> <p>You can use the <code>-n/--name</code> argument to pick <code>HPCG_Original / HPCG_Stencil / HPCG_LFRic</code> to select a particular benchmark. Alternatively, if you want to compare the two implementations of the 27 point stencil problem (Original and Stencil), you can filter by tag <code>-t 27pt_stencil</code>.</p> <p>This app is currently intended to be parallelized with MPI, and it is recommended to use the <code>--system</code> argument to pick up the appropriate hardware details, as well as Spack libraries.</p>"},{"location":"apps/hpgmg/","title":"HPGMG","text":"<p>See hpgmg</p>"},{"location":"apps/hpl/","title":"High Performance Linpack","text":"<p>Run Intel optimised HPL tests on one and all nodes.</p>"},{"location":"apps/hpl/#requirements","title":"Requirements","text":""},{"location":"apps/hpl/#intel-hpl","title":"Intel HPL","text":"<p>This uses the pre-built binaries supplied with Intel's MKL package. Note: Intel MPI is also required.</p> <p>By default the <code>intel-mkl</code> and <code>intel-mpi</code> Spack recipes will be used. If these packages are already available on the system you are using and the Spack environment knows about them, the system packages will be automatically used, otherwise Spack will download and install them for you.</p> <p>If you want to use the oneAPI distribution of MKL and MPI, pass <code>--setvar spack_spec=\"intel-oneapi-mkl ^intel-oneapi-mpi\"</code> as additional argument to the ReFrame invocation (see below). As usual, if these packages are available in the system and the Spack environment knows about them, those packages will be used.</p>"},{"location":"apps/hpl/#hpldat-configuration-files","title":"<code>HPL.dat</code> configuration files","text":"<p>Appropriate <code>HPL.dat</code> configuration files must be generated and placed in <code>&lt;repo_root&gt;/benchmarks/apps/hpl/&lt;sysname&gt;/&lt;number of tasks&gt;</code>, if not already available. ReFrame will copy these files into the staging directories before running a test, so changes made to these files will persist and apply to the next run.</p> <p>Hints:</p> <ul> <li>Generally, set PxQ to equal number of nodes, with P equal or smaller than Q (as using 1x MPI rank per node)</li> <li>Select problem size N to use e.g. 80% of total memory</li> <li>Check Intel documentation to select appropriate block size NB</li> <li>When running, check on a single node that <code>pstree</code> and <code>top</code> appear as expected.</li> </ul> <p>Note: not all systems have appropriate input data, or not for the number of tasks you want to run, so you may have to create the <code>HPL.DAT</code> file yourself.</p> <p>If you want to use an <code>HPL.dat</code> file in a different directory, you can pass <code>--setvar config_dir=&lt;DIRECTORY&gt;</code> as additional argument to the ReFrame invocation (see below), where <code>&lt;DIRECTORY&gt;</code> is the absolute path of the directory where <code>HPL.dat</code> is.</p>"},{"location":"apps/hpl/#running-tests","title":"Running tests","text":"<p>Run using e.g.:</p> <pre><code>reframe -c benchmarks/apps/hpl --run --performance-report\n</code></pre> <p>You can set the number of nodes and tasks per node to use by setting the following variables:</p> <ul> <li><code>num_tasks_per_node</code> (default = 1)</li> <li><code>num_tasks</code> (default = 1)</li> </ul> <p>For example</p> <pre><code>reframe -c benchmarks/apps/hpl --run --performance-report --setvar num_tasks=4 # 4 MPI ranks\nreframe -c benchmarks/apps/hpl --run --performance-report --setvar num_tasks=8 --setvar num_tasks_per_node=2 # 8 MPI ranks, 2 for each node (for a total of 4 nodes)\n</code></pre>"},{"location":"apps/hpl/#outputs","title":"Outputs","text":"<p>The ReFrame performance variable is:</p> <ul> <li><code>Gflops</code>: The performance.</li> </ul>"},{"location":"apps/imb/","title":"Intel MPI Benchmarks","text":"<p>https://software.intel.com/en-us/imb-user-guide</p> <p>Builds automatically using spack.</p> <p>Runs the following MPI1 tests using Intel MPI and OpenMPI:</p> <ul> <li>PingPong (latency/bandwidth) on 2 nodes using 1 process per node</li> <li>Uniband and Biband (bandwidth) using a range of processes from 2 up to 256 using default task pinning (Fill up nodes one by one)</li> </ul> <p>The following tags are defined:</p> <ul> <li>Test mode, one of \"pingpong\", \"biband\", \"uniband\".</li> <li>MPI implementation, one of \"openmpi\", \"intel-mpi\"</li> </ul>"},{"location":"apps/omb/","title":"OSU Micro-Benchmarks","text":"<p>http://mvapich.cse.ohio-state.edu/static/media/mvapich/README-OMB.txt</p> <p>The following tests are run (extracted performance variables described in brackets):</p> <p>On 2x nodes using 1x process per node:</p> <ul> <li><code>osu_bw</code> - bandwidth (max value over all message sizes)</li> <li><code>osu_latency</code> - latency (min value over all message sizes)</li> <li><code>osu_bibw</code> - bi-directional bandwidth (max value over all message sizes)</li> </ul> <p>On 2x nodes using as many processes per node as there are physical cores:</p> <ul> <li><code>osu_allgather</code> - latency (mean value calculated at each message size over pairs, then min taken over all message sizes)</li> <li><code>osu_allreduce</code> - as above</li> <li><code>osu_alltoall</code> - as above</li> </ul> <p>The following tags are defined:</p> <ul> <li>Test name, as given above without the leading \"osu_\"</li> </ul>"},{"location":"apps/omb/#running","title":"Running","text":"<p>Run all tests using e.g.:</p> <pre><code>reframe -C reframe_config.py -c benchmarks/apps/omb/ --run --performance-report\n</code></pre> <p>Run only specified benchmark, by choosing the corresponding tag:</p> <pre><code>reframe -C reframe_config.py -c benchmarks/apps/omb/ --run --performance-report --tag alltoall\nreframe -C reframe_config.py -c benchmarks/apps/omb/ --run --performance-report --tag bw\n</code></pre>"},{"location":"apps/openmm/","title":"OpenMM benchmark","text":"<p>OpenMM is high-performance toolkit for molecular simulation. This directory includes a test based on the 1400k atom benchmark from the HECBioSim suite. Note: this benchmark can run only on systems with a CUDA GPU.</p>"},{"location":"apps/openmm/#usage","title":"Usage","text":"<p>From the top-level directory of the repository, you can run the benchmarks with</p> <pre><code>reframe -c benchmarks/apps/openmm -r --performance-report\n</code></pre>"},{"location":"apps/openmm/#figure-of-merit","title":"Figure of merit","text":"<p>The output of the program looks like</p> <pre><code>#\"Progress (%)\" \"Step\"  \"Potential Energy (kJ/mole)\"    \"Kinetic Energy (kJ/mole)\"      \"Total Energy (kJ/mole)\"        \"Temperature (K)\"       \"Speed (ns/day)\"        \"Time Remaining\"\n10.0%   1000    -15688785.887127012     3656752.4413931114      -12032033.445733901     301.1644297760901       0       --\n20.0%   2000    -15722326.52227436      3651648.2543405197      -12070678.26793384      300.7440568884525       8.58    2:41\n30.0%   3000    -15748457.618506134     3653282.2518931925      -12095175.366612941     300.8786303793008       8.6     2:20\n40.0%   4000    -15766187.389856085     3650127.3583686342      -12116060.03148745      300.6187982674595       8.6     2:00\n50.0%   5000    -15771978.47168088      3640930.7606806774      -12131047.711000202     299.86138082043146      8.61    1:40\n60.0%   6000    -15779433.041706115     3640669.6428865143      -12138763.398819601     299.8398755660168       8.65    1:19\n70.0%   7000    -15774388.543227583     3646512.6161559885      -12127875.927071594     300.3210937346243       8.67    0:59\n80.0%   8000    -15777731.520400822     3641287.017230322       -12136444.5031705       299.89072155441534      8.68    0:39\n90.0%   9000    -15784781.923775911     3647212.6162459007      -12137569.30753001      300.3787446506489       8.7     0:19\n100.0%  10000   -15794411.8787518       3646944.5551444986      -12147467.323607301     300.3566675562755       8.71    0:00\n</code></pre> <p>The figure of merit is the speed of the last step, in units of <code>ns/day</code>. In this example, the capture figure of merit is <code>8.71</code>.</p>"},{"location":"apps/ramses/","title":"Ramses","text":""},{"location":"apps/ramses/#prerequisites","title":"Prerequisites","text":"<p>This code is currently hosted on a private GitHub repo for the benchmarking purposes. If you want to run this benchmark you will first need to request access. Please speak to the RSE team at Leicester for access.</p> <p>This code requires the following input data.</p> <ul> <li><code>cosmo3d-IC-256.tar.gz</code></li> <li><code>cosmo3d-IC-322.tar.gz</code></li> <li><code>cosmo3d-IC-406.tar.gz</code></li> <li><code>cosmo3d-IC-512.tar.gz</code></li> </ul> <p>They are publicly available on zenodo.</p> <p>NB They will be automatically downloaded by reframe, but it takes roughly 15 mins at 5MB/s. They will only be downloaded once per run, but if you manually re-run tests you may prefer to use the following options <code>--restore-session</code> and <code>--keep-stage-files</code>.</p>"},{"location":"apps/ramses/#usage","title":"Usage","text":"<p>From the top-level directory of the repository, you can run the benchmarks with</p> <pre><code>reframe -c benchmarks/apps/ramses -r --performance-report\n</code></pre>"},{"location":"apps/ramses/#filtering-the-benchmarks","title":"Filtering the benchmarks","text":"<p>By default all benchmarks will be run. You can run individual benchmarks with the <code>--tag</code> option:</p> <ul> <li><code>weak</code> to run the weak scaling benchmarks</li> <li><code>strong</code> to run the strong scaling benchmarks</li> </ul> <p>Examples:</p> <pre><code>reframe -c benchmarks/apps/ramses -r --performance-report --tag weak\nreframe -c benchmarks/apps/ramses -r --performance-report --tag strong\n</code></pre>"},{"location":"apps/ramses/#compiler-support","title":"Compiler support","text":"<p>Currently, only the intel compiler is supported for this program.</p>"},{"location":"apps/sombrero/","title":"SOMBERO","text":"<p>SOMBRERO is a benchmarking utility  for high performance computing based on lattice field theory applications.</p> <p>SOMBRERO is composed of 6 similar benchmarks  that are based on different lattice field theories,  each one with a different arithmetic intensity  and a different compute/communication balance. Each benchmark consists of a fixed number (50) of iterations of the Conjugate Gradient algorithm, using the underlying Lattice Dirac operator  built in the relative theory.</p> <p>See the documentation  for more information.</p> <p>SOMBRERO uses a pure-mpi parallelisation. </p> <p>There are four benchmark cases that can be chosen  using the <code>--tag=&lt;TAG&gt;</code> command line option of <code>reframe</code>:</p> <ul> <li><code>mini</code>: A debug run, on a very small lattice, on 2 processes.</li> <li><code>ITT-sn</code>: A run on a single node, using all the cores in each node     (as described here).</li> <li><code>ITT-64n</code>: A run on 64 nodes, using all the cores in each node    (as described here).    The number of nodes used can be changed by setting the variable <code>num_nodes</code>,    for example <code>reframe ... -S num_nodes=48</code>.</li> <li><code>scaling</code>: A large benchmarking campaign, where of the benchmarks is launched               on a range of number of processes              (depending on the setup of the machine)              and 4 different lattice sizes               (details depend on how the cases are filtered). In all these cases, the benchmark for each theory is launched.</li> </ul> <p>The following performance variables are captured:</p> <ul> <li>'flops' : the computing performance (Gflop/second)</li> <li>'time' : time spent in the CG algorithm (seconds)</li> <li>'communicated': number of bytes communicated via MPI (bytes)</li> <li>'avg_arithmetic_intensity': average arithmetic intensity (from DRAM or L3) (Flops/byte)</li> <li>'computation/communication': the ratio of floating point operations                                 over the bytes communicated.</li> </ul>"},{"location":"apps/sphng/","title":"sphNG","text":""},{"location":"apps/sphng/#prerequisites","title":"Prerequisites","text":"<p>This code is currently hosted on a private GitHub repo for the benchmarking purposes. If you want to run this benchmark you will first need to request access. Please speak to the RSE team at Leicester for access.</p> <p>The main code is available at https://bitbucket.org/mrbate/sphng/src/master/g but the spack recipe is not currently set up to work with it. We are working on this and soon we will switch over to the actual version. This is because we need to be able to verify that the version we run is the same as the one already used for benchmarking.</p>"},{"location":"apps/sphng/#usage","title":"Usage","text":"<p>From the top-level directory of the repository, you can run the benchmarks with</p> <pre><code>reframe -c benchmarks/apps/sphng -r --performance-report\n</code></pre>"},{"location":"apps/sphng/#filtering-the-benchmarks","title":"Filtering the benchmarks","text":"<p>By default all benchmarks will be run. You can run individual benchmarks with the <code>--tag</code> option:</p> <ul> <li><code>single-node</code> to run benchmarks on a single node</li> </ul> <p>Examples:</p> <pre><code>reframe -c benchmarks/apps/sphng -r --performance-report --tag single-node\n</code></pre>"},{"location":"apps/sphng/#compiler-support","title":"Compiler support","text":"<p>Currently, only the intel compiler is supported for this program.</p>"},{"location":"apps/swift/","title":"Swift","text":"<p>See swift</p>"},{"location":"apps/trove/","title":"Trove","text":""},{"location":"apps/trove/#prerequisites","title":"Prerequisites","text":"<p>This code is currently hosted on a private GitHub repo for the benchmarking purposes. If you want to run this benchmark you will first need to request access. Please speak to the RSE team at Leicester for access.</p> <p>The main code is publicly available at https://github.com/Trovemaster/TROVE but the spack recipe is not currently set up to work with it. We are working on this and soon we will switch over to the public version. This is because we need to be able to verify that the version we run is the same as the one already used for benchmarking.</p>"},{"location":"apps/trove/#usage","title":"Usage","text":"<p>From the top-level directory of the repository, you can run the benchmarks with</p> <pre><code>reframe -c benchmarks/apps/trove -r --performance-report\n</code></pre>"},{"location":"apps/trove/#filtering-the-benchmarks","title":"Filtering the benchmarks","text":"<p>By default all benchmarks will be run. You can run individual benchmarks with the <code>--tag</code> option:</p> <ul> <li><code>12N</code> to run benchmarks w.r.t. N12.inp</li> </ul> <p>Examples:</p> <pre><code>reframe -c benchmarks/apps/trove -r --performance-report --tag 12N\n</code></pre>"},{"location":"apps/trove/#compiler-support","title":"Compiler support","text":"<p>Currently, only the intel compiler is supported for this program.</p>"},{"location":"apps/trove-pdsyev/","title":"Trove Pdsyev","text":""},{"location":"apps/trove-pdsyev/#prerequisites","title":"Prerequisites","text":"<p>This code is currently hosted on a private GitHub repo for the benchmarking purposes. If you want to run this benchmark you will first need to request access. Please speak to the RSE team at Leicester for access.</p> <p>The main code is publicly available at https://github.com/Trovemaster/PDSYEV but the spack recipe is not currently set up to work with it. We are working on this and soon we will switch over to the public version. This is because we need to be able to verify that the version we run is the same as the one already used for benchmarking.</p>"},{"location":"apps/trove-pdsyev/#usage","title":"Usage","text":"<p>From the top-level directory of the repository, you can run the benchmarks with</p> <pre><code>reframe -c benchmarks/apps/trove-pdsyev -r --performance-report\n</code></pre>"},{"location":"apps/trove-pdsyev/#filtering-the-benchmarks","title":"Filtering the benchmarks","text":"<p>By default all benchmarks will be run. You can run individual benchmarks with the <code>--tag</code> option:</p> <ul> <li><code>single-node</code> to run benchmarks on a single node</li> </ul> <p>Examples:</p> <pre><code>reframe -c benchmarks/apps/trove-pdsyev -r --performance-report --tag single-node\n</code></pre>"},{"location":"apps/trove-pdsyev/#compiler-support","title":"Compiler support","text":"<p>Currently, only the intel compiler is supported for this program.</p>"},{"location":"apps/wrf/","title":"Weather Research and Forecasting (WRF) Model","text":"<p>Results from WRF, the Weather Research &amp; Forecasting Model using the WRFV3 benchmarks:</p> <ul> <li>12km CONUS (medium-size case), tag <code>12km</code>: <p>48-hour, 12km resolution case over the Continental U.S. (CONUS) domain October 24, 2001 with a time step of 72 seconds. The benchmark period is hours 25-27 (3 hours), starting from a restart file from the end of hour 24.</p> </li> <li>2.5km CONUS (large case), tag <code>2.5km</code>: <p>Latter 3 hours of a 9-hour, 2.5km resolution case covering the Continental U.S. (CONUS) domain June 4, 2005 with a 15 second time step.  The benchmark period is hours 6-9 (3 hours), starting from a restart file from the end of the initial 6 hour period Descriptions from the above benchmark page.</p> </li> </ul> <p>The following performance variables are captured:</p> <ul> <li>'gflops': Gigaflops per second, calculated as described in the benchmark page, using the average time required per model timestep and the number of floating point operations required for the benchmark. The time required for each model timestep is reported by WRF itself.</li> </ul>"},{"location":"apps/wrf/#usage","title":"Usage","text":"<p>Run using e.g.:</p> <pre><code>reframe/bin/reframe -C reframe_config.py -c benchmarks/apps/wrf/ --run --performance-report\n</code></pre> <p>A precursor task automatically downloads the required benchmark files. This may take some time due to the files size.</p>"},{"location":"apps/wrf/#filtering-the-benchmark","title":"Filtering the benchmark","text":"<p>You can filter the benchmark to run by filtering by tag:</p> <pre><code># For the 12km data\nreframe/bin/reframe -c benchmarks/apps/wrf/ --run --performance-report --tag '12km'\n# For the 2.5km data\nreframe/bin/reframe -c benchmarks/apps/wrf/ --run --performance-report --tag '2.5km'\n</code></pre>"},{"location":"apps/wrf/#setting-the-number-of-threads-and-mpi-processes","title":"Setting the number of threads and MPI processes","text":"<p>By default, these benchmarks will use</p> <ul> <li><code>num_cpus_per_task</code>:   2</li> <li><code>num_tasks</code>:   <code>current_partition.processor.num_cpus // min(1, current_partition.processor.num_cpus_per_core) // num_cpus_per_task</code></li> <li><code>num_tasks_per_node</code>: <code>current_partition.processor.num_cpus // num_cpus_per_task</code></li> </ul> <p>You can override the values of these variables from the command line with the <code>--setvar</code> option, for example</p> <pre><code>reframe -c benchmarks/apps/wrf -r --performance-report --setvar=num_cpus_per_task=4 --setvar=num_tasks=16\n</code></pre> <p>Note: you're responsible for overriding these variables in a consistent way, so that, for example, <code>num_tasks_per_node</code> doesn't exceed the number of total tasks runnable on each node.</p>"},{"location":"post-processing/","title":"Post-processing","text":""},{"location":"post-processing/#benchmark-results-post-processing","title":"Benchmark Results Post-Processing","text":""},{"location":"post-processing/#overview","title":"Overview","text":"<p>The post-processing scripts provided with the ExCALIBUR tests package are intended to grant users a quick starting point for visualising benchmark results with basic graphs and tables. Their components can also be used inside custom users' scripts.</p> <p>There are four main post-processing components:</p>"},{"location":"post-processing/#perflog-parsing","title":"<code>Perflog parsing</code>","text":"<ul> <li>Data from benchmark performance logs are stored in a pandas DataFrame.</li> </ul>"},{"location":"post-processing/#data-filtering","title":"<code>Data filtering</code>","text":"<ul> <li>If more than one perflog is used for plotting, DataFrames from individual perflogs are concatenated together into one DataFrame.</li> <li>The DataFrame is then filtered, keeping only relevant rows and columns.</li> </ul>"},{"location":"post-processing/#data-transformation","title":"<code>Data transformation</code>","text":"<ul> <li>Axis value columns in the DataFrame are scaled according to user specifications.</li> </ul>"},{"location":"post-processing/#plotting","title":"<code>Plotting</code>","text":"<ul> <li>A filtered and transformed DataFrame is passed to a plotting script, which produces a graph and embeds it in a simple HTML file.</li> <li>Users may run the plotting script to generate a generic bar chart. Graph settings should be specified in a configuration YAML file.</li> </ul>"},{"location":"post-processing/#installation","title":"Installation","text":"<p>Post-processing is an optional dependency of the ExCALIBUR tests package, as it requires Python version 3.9 or later (while the base package requires Python version 3.7 or later).</p> <p>You can include post-processing in your <code>pip</code> installation of the package with the following command:</p> <pre><code>pip install -e .[post-processing]\n</code></pre>"},{"location":"post-processing/#usage","title":"Usage","text":""},{"location":"post-processing/#command-line","title":"Command line","text":"<pre><code>python post_processing.py log_path config_path [-p plot_type]\n</code></pre> <ul> <li><code>log_path</code> - Path to a perflog file, or a directory containing perflog files.</li> <li><code>config_path</code> - Path to a configuration file containing plot details.</li> <li><code>plot_type</code> - (Optional.) Type of plot to be generated. (<code>Note: only a generic bar chart is currently implemented.</code>)</li> </ul> <p>Run <code>post_processing.py -h</code> for more information (including debugging and file output flags).</p>"},{"location":"post-processing/#streamlit","title":"Streamlit","text":"<p>You may also run post-processing with Streamlit to interact with your plots:</p> <p><code>streamlit run streamlit_post_processing.py log_path -- [-c config_path]</code></p> <p>The config path is optional when running with Streamlit, as the UI allows you to create a new config on the fly. If you would still like to supply a config path, make sure to include <code>--</code> before any post-processing flags to indicate that the arguments belong to the post-processing script rather than Streamlit itself.</p>"},{"location":"post-processing/#configuration-structure","title":"Configuration Structure","text":"<p>Before running post-processing, create a config file including all necessary information for graph generation (you must specify at least plot title, x-axis, y-axis, and column types). See below for a template, an example, and some clarifying notes.</p> <ul> <li><code>title</code> - Plot title.</li> <li><code>x_axis</code>, <code>y_axis</code> - Axis information.<ul> <li><code>value</code> - Axis data points. Specified with a column name.</li> <li><code>units</code> - Axis units. Specified either with a column name or a custom label (may be null).</li> <li><code>scaling</code> - (Optional.) Scale axis values by either a column or a custom value.</li> <li><code>sort</code> - (Optional.) Sort categorical x-axis in descending order (otherwise values are sorted in ascending order by default).</li> </ul> </li> <li><code>filters</code> - (Optional.) Filter data rows based on specified conditions. (Specify an empty list if no filters are required.)<ul> <li><code>and</code> - Filter mask is determined from a logical AND of conditions in list.</li> <li><code>or</code> - Filter mask is determined from a logical OR of conditions in list.</li> <li><code>Format: [column_name, operator, value]</code></li> <li><code>Accepted operators: \"==\", \"!=\", \"&lt;\", \"&gt;\", \"&lt;=\", \"&gt;=\"</code></li> </ul> </li> <li><code>series</code> - (Optional.) Display several plots in the same graph and group x-axis data by specified column values. (Specify an empty list if there is only one series.)<ul> <li><code>Format: [column_name, value]</code></li> </ul> </li> <li><code>column_types</code> - Pandas dtype for each relevant column (axes, units, filters, series). Specified with a dictionary.<ul> <li><code>Accepted types: \"str\"/\"string\"/\"object\", \"int\"/\"int64\", \"float\"/\"float64\", \"datetime\"/\"datetime64\"</code></li> </ul> </li> <li><code>extra_columns_to_csv</code> - (Optional.) List of additional columns to include when exporting benchmark data to a CSV, in addition to the ones above. These columns are not used in plotting. (Specify an empty list if no additional columns are required.)</li> </ul>"},{"location":"post-processing/#a-note-on-replaced-reframe-columns","title":"A Note on Replaced ReFrame Columns","text":"<p>A perflog contains certain columns with complex information that has to be unpacked in order to be useful. Currently, such columns are <code>display_name</code>, <code>extra_resources</code>, <code>env_vars</code>, and <code>spack_spec_dict</code>. Those columns are parsed by the postprocessing, removed from the DataFrame, and substituted by new columns with the unpacked information. Therefore they will not be present in the DataFrame available to the graphing script and should not be referenced in a plot config file.</p> <p>When the row contents of <code>display_name</code> are parsed, they are separated into their constituent benchmark names and parameters. This column is replaced with a new <code>test_name</code> column and new parameter columns (if present). Similarly, the <code>extra_resources</code>, <code>env_vars</code>, and <code>spack_spec_dict</code> columns are replaced with their respective dictionary row contents (keys become columns, values become row contents).</p>"},{"location":"post-processing/#complete-config-template","title":"Complete Config Template","text":"<p>This template includes all possible config fields, some of which are optional or mutually exclusive (e.g. <code>column</code> and <code>custom</code>).</p> <pre><code>title: &lt;custom_label&gt;\n\nx_axis:\n  value: &lt;column_name&gt;\n  # use one of 'column' or 'custom'\n  units:\n    column: &lt;column_name&gt;\n    custom: &lt;custom_label&gt;\n  # optional (default: ascending)\n  sort: \"descending\"\n\ny_axis:\n  value: &lt;column_name&gt;\n  # use one of 'column' or 'custom'\n  units:\n    column: &lt;column_name&gt;\n    custom: &lt;custom_label&gt;\n  # optional (default: no data transformation)\n  # use one of 'column' or 'custom'\n  scaling:\n    column:\n      name: &lt;column_name&gt;\n      series: &lt;index&gt;\n      x_value: &lt;column_value&gt;\n    custom: &lt;custom_value&gt;\n\n# optional (default: include all data)\n# entry format: [&lt;column_name&gt;, &lt;operator&gt;, &lt;column_value&gt;]\n# accepted operators: ==, !=, &lt;, &gt;, &lt;=, &gt;=\nfilters:\n  and: &lt;condition_list&gt;\n  or: &lt;condition_list&gt;\n\n# optional (default: no x-axis grouping, one plot per graph)\n# entry format: [&lt;column_name&gt;, &lt;column_value&gt;]\nseries: &lt;series_list&gt;\n\n# include types for each column that is used in the config\n# accepted types: string/object, int, float, datetime\ncolumn_types:\n  &lt;column_name&gt;: &lt;column_type&gt;\n\n# optional (default: no extra columns exported to CSV file in addition to the ones above)\nextra_columns_to_csv: &lt;columns_list&gt;\n</code></pre>"},{"location":"post-processing/#example-config","title":"Example Config","text":"<p>This example more accurately illustrates what an actual config file may look like.</p> <pre><code>title: \"Plot Title\"\n\nx_axis:\n  value: \"x_axis_col\"\n  units:\n    custom: \"unit_label\"\n  sort: \"descending\"\n\ny_axis:\n  value: \"y_axis_col\"\n  units:\n    column: \"unit_col\"\n  scaling:\n    column:\n      name: \"scaling_col\"\n      series: 0\n      x_value: \"x_val_s\"\n\nfilters:\n  and: [[\"filter_col_1\", \"&lt;=\", filter_val_1],\n        [\"filter_col_2\", \"!=\", filter_val_2]]\n  or: []\n\nseries: [[\"series_col\", \"series_val_1\"],\n         [\"series_col\", \"series_val_2\"]]\n\ncolumn_types:\n  x_axis_col: \"str\"\n  y_axis_col: \"float\"\n  unit_col: \"str\"\n  scaling_col: \"float\"\n  filter_col_1: \"datetime\"\n  filter_col_2: \"int\"\n  series_col: \"str\"\n\nadditional_columns_to_csv:\n  [\"additional_col_1\", \"additional_col_2\"]\n</code></pre>"},{"location":"post-processing/#x-axis-grouping","title":"X-axis Grouping","text":"<p>The settings above will produce a graph that will have its x-axis data grouped based on the values in <code>x_axis_col</code> and <code>series_col</code>. (<code>Note: only groupings with one series column are currently supported.</code>) If we imagine that <code>x_axis_col</code> has two unique values, <code>\"x_val_1\"</code> and <code>\"x_val_2\"</code>, there will be four groups (and four bars) along the x-axis:</p> <ul> <li>(<code>x_val_1</code>, <code>series_val_1</code>)</li> <li>(<code>x_val_1</code>, <code>series_val_2</code>)</li> <li>(<code>x_val_2</code>, <code>series_val_1</code>)</li> <li>(<code>x_val_2</code>, <code>series_val_2</code>)</li> </ul>"},{"location":"post-processing/#scaling","title":"Scaling","text":"<p>When axis values are scaled, they are all divided by a number or a list of numbers. If using more than one number for scaling, the length of the list must match the length of the axis column being scaled. (<code>Note: scaling is currently only supported for y-axis data, as graphs with a non-categorical x-axis are still a work in progress.</code>)</p> <p>Custom Scaling</p> <p>Manually specify one value to scale axis values by.</p> <pre><code>y_axis:\n  value: \"y_axis_col\"\n  units:\n    column: \"unit_col\"\n  scaling:\n    custom: 2\n</code></pre> <p>In the snippet above, all y-axis values are to be divided by 2.</p> y_axis_col scaled_y_axis_col 3.2 3.2 / 2.0 = 1.6 5.4 5.4 / 2.0 = 2.7 2.4 2.4 / 2.0 = 1.2 5.0 5.0 / 2.0 = 2.5 <p>Column Scaling</p> <p>Specify one column to scale axis values by.</p> <pre><code>y_axis:\n  value: \"y_axis_col\"\n  units:\n    column: \"unit_col\"\n  scaling:\n    column:\n      name: \"scaling_col\"\n</code></pre> <p>In the snippet above, all y-axis values are to be divided by the corresponding values in the scaling column.</p> y_axis_col scaling_col scaled_y_axis_col 3.2 <code>1.6</code> 3.2 / 1.6 = 2.0 5.4 <code>2.0</code> 5.4 / 2.0 = 2.7 2.4 <code>0.6</code> 2.4 / 0.6 = 4.0 5.0 <code>2.5</code> 5.0 / 2.5 = 2.0 <p>Series Scaling</p> <p>Specify one series to scale axis values by. This is done with an index, which is used to find the correct series from a list.</p> <p>In the case of the list of series from the example config above, index 0 would select a scaling series of <code>[\"series_col\", \"series_val_1\"]</code>, while index 1 would scale by <code>[\"series_col\", \"series_val_2\"]</code>.</p> <pre><code>y_axis:\n  value: \"y_axis_col\"\n  units:\n    column: \"unit_col\"\n  scaling:\n    column:\n      name: \"scaling_col\"\n      series: 0\n</code></pre> <p>In the snippet above, all y-axis values are to be split by series and divided by the corresponding values in the scaling series.</p> y_axis_col scaling_col series_col scaled_y_axis_col 3.2 <code>1.6</code> <code>series_val_1</code> 3.2 / 1.6 = 2.0 5.4 <code>2.0</code> <code>series_val_1</code> 5.4 / 2.0 = 2.7 2.4 0.6 series_val_2 2.4 / 1.6 = 1.5 5.0 2.5 series_val_2 5.0 / 2.0 = 2.5 <p>Selected Value Scaling</p> <p>Specify one value from a column to scale axis values by.</p> <pre><code>y_axis:\n  value: \"y_axis_col\"\n  units:\n    column: \"unit_col\"\n  scaling:\n    column:\n      name: \"scaling_col\"\n      series: 0\n      x_value: \"x_val_s\"\n</code></pre> <p>In the snippet above, all y-axis values are to be divided by the scaling value found by filtering the scaling column by both series and x-axis value.</p> x_axis_col y_axis_col scaling_col series_col scaled_y_axis_col x_val_1 3.2 1.6 series_val_1 3.2 / 2.0 = 1.6 <code>x_val_s</code> 5.4 <code>2.0</code> <code>series_val_1</code> 5.4 / 2.0 = 2.7 x_val_2 2.4 0.7 series_val_2 2.4 / 2.0 = 1.2 x_val_s 5.0 2.5 series_val_2 5.0 / 2.0 = 2.5 <p>(<code>Note: if series are not present and x-axis values are all unique, it is enough to specify just the column name and x-value.</code>)</p>"},{"location":"post-processing/#filters","title":"Filters","text":"<p>A condition list for filtering has entries in the format <code>[&lt;column_name&gt;, &lt;operator&gt;, &lt;column_value&gt;]</code>. AND filters and OR filters are combined with a logical AND to produce the final filter mask applied to the DataFrame prior to graphing. For example:</p> <ul> <li><code>and_filters</code> = <code>cond1</code>, <code>cond2</code></li> <li><code>or_filters</code>= <code>cond3</code>, <code>cond4</code></li> </ul> <p>The filters above would produce the final filter <code>mask</code> = (<code>cond1</code> AND <code>cond2</code>) AND (<code>cond3</code> OR <code>cond4</code>).</p>"},{"location":"post-processing/#column-types","title":"Column Types","text":"<p>Types must be specified for all columns included in the config in the format <code>&lt;column_name&gt;:&lt;column_type&gt;</code>. Accepted types include <code>string/object</code>, <code>int</code>, <code>float</code>, and <code>datetime</code>.</p> <p>All user-specified types are internally converted to their nullable incarnations. As such:</p> <ul> <li>Strings are treated as <code>object</code> (str or mixed type).</li> <li>Floats are treated as <code>float64</code>.</li> <li>Integers are treated as <code>Int64</code>.</li> <li>Datetimes are treated as <code>datetime64[ns]</code>.</li> </ul>"},{"location":"post-processing/#future-development","title":"Future Development","text":"<p>The post-processing capabilities are still a work in progress. Some upcoming developments:</p> <ul> <li>Embed graphs in GitHub Pages, instead of a bare HTML file.</li> <li>Add scaling and regression plots.</li> </ul>"},{"location":"tutorial/excalibur-tests_tutorial/","title":"Automation in excalibur-tests","text":""},{"location":"tutorial/excalibur-tests_tutorial/#using-reframe-for-reproducible-and-portable-performance-benchmarking","title":"Using ReFrame for reproducible and portable performance benchmarking","text":"<p>In this tutorial you will set up the excalibur-tests benchmarking framework on a HPC system, build and run example benchmarks, create a new benchmark and explore benchmark data.</p>"},{"location":"tutorial/excalibur-tests_tutorial/#installing-the-framework","title":"Installing the Framework","text":""},{"location":"tutorial/excalibur-tests_tutorial/#set-up-python-environment","title":"Set up python environment","text":"CosmaARCHER2 <p>This tutorial is run on the Cosma supercomputer. It should be straightforward to run on a different platform, the requirements are  <code>gcc 4.5</code>, <code>git 2.39</code> and <code>python 3.7</code> or later.  (for the later parts you also need <code>make</code>, <code>autotools</code>, <code>cmake</code> and <code>spack</code> but these can be installed locally). Before proceeding to install ReFrame, we recommend creating a python virtual environment to avoid clashes with other installed python packages. First load a newer python module. <pre><code>module swap python/3.10.12\n</code></pre></p> <p>This tutorial is run on ARCHER2, you should have signed up for a training account before starting. It can be ran on other HPC systems with a batch scheduler but will require making some changes to the config. Before proceeding to install ReFrame, we recommend creating a python virtual environment to avoid clashes with other installed python packages. First load the system python module. <pre><code>module load cray-python\n</code></pre></p> <p>Then create an environment and activate it with</p> <pre><code>python3 -m venv reframe_tutorial\nsource reframe_tutorial/bin/activate\n</code></pre> <p>You will have to activate the environment each time you login. To deactivate the environment run <code>deactivate</code>.</p>"},{"location":"tutorial/excalibur-tests_tutorial/#change-to-work-directory","title":"Change to work directory","text":"CosmaARCHER2 <p>Move on to the next step.</p> <p>On ARCHER2, the compute nodes do not have access to your home directory, therefore it is important to install everything in a work file system.  Change to the work directory with</p> <pre><code>cd /work/ta131/ta131/${USER}\n</code></pre> <p>If you are tempted to use a symlink here, ensure you use <code>cd -P</code> when changing directory.  ARCHER2 compute nodes cannot read from <code>/home</code>, only <code>/work</code>, so not completely following symlinks can result in a broken installation.</p>"},{"location":"tutorial/excalibur-tests_tutorial/#clone-the-git-repository","title":"Clone the git repository","text":"<p>In the work directory, clone the excalibur-tests repository with</p> <pre><code>git clone https://github.com/ukri-excalibur/excalibur-tests.git\n</code></pre>"},{"location":"tutorial/excalibur-tests_tutorial/#create-a-virtual-environment","title":"Create a virtual environment","text":"<p>Before proceeding to install the software, we recommend creating a python virtual environment to avoid clashes with other installed python packages. You can do this with <pre><code>python3 -m venv excalibur-env\nsource excalibur-env/bin/activate\n</code></pre></p> <p>You should now see the name of the environment in parenthesis your terminal prompt, for example: <pre><code>(excalibur-env) tk-d193@ln03:/work/d193/d193/tk-d193&gt;\n</code></pre></p> <p>You will have to activate the environment each time you login. To deactivate the environment run <code>deactivate</code>.</p>"},{"location":"tutorial/excalibur-tests_tutorial/#install-the-excalibur-tests-framework","title":"Install the excalibur-tests framework","text":"<p>Now we can use <code>pip</code> to install the package in the virtual environment. Update pip to the latest version with  <pre><code>pip install --upgrade pip\n</code></pre> then install the framework with <pre><code>pip install -e ./excalibur-tests[post-processing]\n</code></pre> We used the <code>editable</code> flag <code>-e</code> because later in the tutorial you will edit the repository to develop a new benchmark.</p> <p>We included optional dependencies with <code>[post-processing]</code>. We will need those in the postprocessing section.</p>"},{"location":"tutorial/excalibur-tests_tutorial/#set-configuration-variables","title":"Set configuration variables","text":"<p>Configure the framework by setting these environment variables</p> <pre><code>export RFM_CONFIG_FILES=\"$(pwd)/excalibur-tests/benchmarks/reframe_config.py\"\nexport RFM_USE_LOGIN_SHELL=\"true\"\n</code></pre>"},{"location":"tutorial/excalibur-tests_tutorial/#install-and-configure-spack","title":"Install and configure spack","text":"<p>Finally, we need to install the <code>spack</code> package manager. The framework will use it to build the benchmarks. Clone spack with</p> <pre><code>git clone -c feature.manyFiles=true https://github.com/spack/spack.git\n</code></pre> <p>Then configure <code>spack</code> with</p> <pre><code>source ./spack/share/spack/setup-env.sh\n</code></pre> <p>Spack should now be in the default search path.</p>"},{"location":"tutorial/excalibur-tests_tutorial/#check-installation-was-successful","title":"Check installation was successful","text":"<p>You can check everything has been installed successfully by checking that <code>spack</code> and <code>reframe</code> are in path and the path to the ReFrame config file is set correctly</p> <pre><code>$ spack --version\n0.22.0.dev0 (88e738c34346031ce875fdd510dd2251aa63dad7)\n$ reframe --version\n4.4.1\n$ ls $RFM_CONFIG_FILES\n/work/d193/d193/tk-d193/excalibur-tests/benchmarks/reframe_config.py\n</code></pre>"},{"location":"tutorial/excalibur-tests_tutorial/#environment-summary","title":"Environment summary","text":"<p>If you log out and back in, you will have to run some of the above commands again to recreate your environment. These are (from your <code>work</code> directory):</p> ARCHER2 <pre><code>module load cray-python\nsource excalibur-env/bin/activate\nexport RFM_CONFIG_FILES=\"$(pwd)/excalibur-tests/benchmarks/reframe_config.py\"\nexport RFM_USE_LOGIN_SHELL=\"true\"\nsource ./spack/share/spack/setup-env.sh\n</code></pre>"},{"location":"tutorial/excalibur-tests_tutorial/#run-sombrero-example","title":"Run Sombrero Example","text":"<p>You can now use ReFrame to run benchmarks from the <code>benchmarks/examples</code> and <code>benchmarks/apps</code> directories. The basic syntax is  <pre><code>reframe -c &lt;path/to/benchmark&gt; -r\n</code></pre></p>"},{"location":"tutorial/excalibur-tests_tutorial/#system-specific-flags","title":"system specific flags","text":"ARCHER2 <p>In addition, on ARCHER2, you have to provide the quality of service (QoS) type for your job to ReFrame on the command line with <code>-J</code>. Use the \"short\" QoS to run the sombrero example with <pre><code>reframe -c excalibur-tests/benchmarks/examples/sombrero -r -J'--qos=short'\n</code></pre> You may notice you actually ran four benchmarks with that single command! That is because the benchmark is parametrized. We will talk about this in the next section.</p>"},{"location":"tutorial/excalibur-tests_tutorial/#output-sample","title":"Output sample","text":"ARCHER2 <pre><code>$ reframe -c benchmarks/examples/sombrero/ -r -J'--qos=short' --performance-report\n[ReFrame Setup]\n  version:           4.3.0\n  command:           '/mnt/lustre/a2fs-work3/work/d193/d193/tk-d193/excalibur-env/bin/reframe -c benchmarks/examples/sombrero/ -r -J--qos=short'\n  launched by:       tk-d193@ln03\n  working directory: '/mnt/lustre/a2fs-work3/work/d193/d193/tk-d193/excalibur-tests'\n  settings files:    '&lt;builtin&gt;', '/work/d193/d193/tk-d193/excalibur-tests/benchmarks/reframe_config.py'\n  check search path: '/mnt/lustre/a2fs-work3/work/d193/d193/tk-d193/excalibur-tests/benchmarks/examples/sombrero'\n  stage directory:   '/mnt/lustre/a2fs-work3/work/d193/d193/tk-d193/excalibur-tests/stage'\n  output directory:  '/mnt/lustre/a2fs-work3/work/d193/d193/tk-d193/excalibur-tests/output'\n  log files:         '/tmp/rfm-u1l6yt7f.log'\n\n[==========] Running 4 check(s)\n[==========] Started on Fri Jul  7 15:47:45 2023 \n\n[----------] start processing checks\n[ RUN      ] SombreroBenchmark %tasks=2 %cpus_per_task=2 /de04c10b @archer2:compute-node+default\n[ RUN      ] SombreroBenchmark %tasks=2 %cpus_per_task=1 /c52a123d @archer2:compute-node+default\n[ RUN      ] SombreroBenchmark %tasks=1 %cpus_per_task=2 /c1c3a3f1 @archer2:compute-node+default\n[ RUN      ] SombreroBenchmark %tasks=1 %cpus_per_task=1 /52e1ce98 @archer2:compute-node+default\n[       OK ] (1/4) SombreroBenchmark %tasks=1 %cpus_per_task=2 /c1c3a3f1 @archer2:compute-node+default\nP: flops: 0.67 Gflops/seconds (r:1.2, l:None, u:None)\n[       OK ] (2/4) SombreroBenchmark %tasks=1 %cpus_per_task=1 /52e1ce98 @archer2:compute-node+default\nP: flops: 0.67 Gflops/seconds (r:1.2, l:None, u:None)\n[       OK ] (3/4) SombreroBenchmark %tasks=2 %cpus_per_task=2 /de04c10b @archer2:compute-node+default\nP: flops: 1.27 Gflops/seconds (r:1.2, l:None, u:None)\n[       OK ] (4/4) SombreroBenchmark %tasks=2 %cpus_per_task=1 /c52a123d @archer2:compute-node+default\nP: flops: 1.24 Gflops/seconds (r:1.2, l:None, u:None)\n[----------] all spawned checks have finished\n\n[  PASSED  ] Ran 4/4 test case(s) from 4 check(s) (0 failure(s), 0 skipped, 0 aborted)\n[==========] Finished on Fri Jul  7 15:48:23 2023 \nLog file(s) saved in '/tmp/rfm-u1l6yt7f.log'\n</code></pre>"},{"location":"tutorial/excalibur-tests_tutorial/#benchmark-output","title":"Benchmark output","text":"<p>You can find build and run logs in the <code>output/</code> directory of a successful benchmark. They record how the benchmark was built by spack and ran by ReFrame.</p> <p>While the benchmark is running, the log files are kept in the <code>stage/</code> directory. They remain there if the benchmark fails to build or run.</p> <p>You can find the performance log file from the benchmark in <code>perflogs/</code>. The perflog records the captured figures of merit, environment variables and metadata about the job.</p>"},{"location":"tutorial/excalibur-tests_tutorial/#create-a-benchmark","title":"Create a Benchmark","text":"<p>In this section you will create a ReFrame benchmark by writing a python class that tells ReFrame how to build and run an application and collect data from its output. </p> <p>For simplicity, we use the <code>STREAM</code> benchmark. It is a simple memory bandwidth benchmark with minimal build dependencies.</p> <p>If you've already gone through the ReFrame tutorial some of the steps in creating the STREAM benchmark are repeated. However, pay attention to the <code>Create a Test Class</code> and <code>Add Build Recipe</code> steps.</p>"},{"location":"tutorial/excalibur-tests_tutorial/#how-reframe-works","title":"How ReFrame works","text":"<p>When ReFrame executes a test it runs a pipeline of the following stages</p> <p></p> <p>You can customise the behaviour of each stage or add a hook before or after each of them.  For more details, read the ReFrame pipeline documentation.</p>"},{"location":"tutorial/excalibur-tests_tutorial/#getting-started","title":"Getting started","text":"<p>To get started, open an empty <code>.py</code> file where you will write the ReFrame class, e.g. <code>stream.py</code>. Save the file in a new directory e.g. <code>excalibur-tests/benchmarks/apps/stream</code>.</p>"},{"location":"tutorial/excalibur-tests_tutorial/#include-reframe-modules","title":"Include ReFrame modules","text":"<p>The first thing you need is include a few modules from ReFrame. These should be available if the installation step was successful.</p> <pre><code>import reframe as rfm\nimport reframe.utility.sanity as sn\n</code></pre>"},{"location":"tutorial/excalibur-tests_tutorial/#create-a-test-class","title":"Create a Test Class","text":"<p>ReFrame has built-in support for the Spack package manager. In the following we will use the custom class <code>SpackTest</code> we created for our <code>benchmarks</code> module, which provides a tighter integration with Spack and reduces the boilerplate code you'd otherwise have to include.</p> <pre><code>from benchmarks.modules.utils import SpackTest\n\n@rfm.simple_test\nclass StreamBenchmark(SpackTest):\n</code></pre> <p>The data members and methods detailed in the following sections should be placed inside this class.</p>"},{"location":"tutorial/excalibur-tests_tutorial/#add-build-recipe","title":"Add Build Recipe","text":"<p>We prefer installing packages via spack whenever possible. In this exercise, the spack package for <code>stream</code> already exists in the global spack repository.</p> <p>The <code>SpackTest</code> base class takes care of setting up spack as the build system ReFrame uses. We only need to instruct ReFrame to install version <code>5.10</code> of the <code>stream</code> spack package using the <code>openmp</code> variant.</p> <pre><code>spack_spec = 'stream@5.10 +openmp'\n</code></pre> <p>Note that we did not specify a compiler. Spack will use a compiler from the spack environment. The complete spec is recorded in the build log.</p>"},{"location":"tutorial/excalibur-tests_tutorial/#add-run-configuration","title":"Add Run Configuration","text":"<p>The ReFrame class tells ReFrame where and how to run the benchmark. We want to run on one task on a full archer2 node using 128 OpenMP threads to use the full node.</p> <pre><code>valid_systems = ['*']\nvalid_prog_environs = ['default']\nexecutable = 'stream_c.exe'\nnum_tasks = 1\ntime_limit = '5m'\nuse_multithreading = False\n</code></pre>"},{"location":"tutorial/excalibur-tests_tutorial/#add-environment-variables","title":"Add environment variables","text":"<p>Environment variables can be added to the <code>env_vars</code> attribute.</p> <pre><code>env_vars['OMP_NUM_THREADS'] = f'{num_cpus_per_task}'\nenv_vars['OMP_PLACES'] = 'cores'\n</code></pre>"},{"location":"tutorial/excalibur-tests_tutorial/#add-sanity-check","title":"Add Sanity Check","text":"<p>The rest of the benchmark follows the Writing a Performance Test ReFrame Tutorial. First we need a sanity check that ensures the benchmark ran successfully. A function decorated with the <code>@sanity_function</code> decorator is used by ReFrame to check that the test ran successfully. The sanity function can perform a number of checks, in this case we want to match a line of the expected standard output.</p> <pre><code>@sanity_function\ndef validate_solution(self):\n    return sn.assert_found(r'Solution Validates', self.stdout)\n</code></pre>"},{"location":"tutorial/excalibur-tests_tutorial/#add-performance-pattern-check","title":"Add Performance Pattern Check","text":"<p>To record the performance of the benchmark, ReFrame should extract a figure of merit from the output of the test. A function decorated with the <code>@performance_function</code> decorator extracts or computes a performance metric from the test\u2019s output.</p> <p>In this example, we extract four performance variables, namely the memory bandwidth values for each of the \u201cCopy\u201d, \u201cScale\u201d, \u201cAdd\u201d and \u201cTriad\u201d sub-benchmarks of STREAM, where each of the performance functions use the <code>extractsingle()</code> utility function. For each of the sub-benchmarks we extract the \u201cBest Rate MB/s\u201d column of the output (see below) and we convert that to a float.</p> <pre><code>@performance_function('MB/s', perf_key='Copy')\ndef extract_copy_perf(self):\n    return sn.extractsingle(r'Copy:\\s+(\\S+)\\s+.*', self.stdout, 1, float)\n\n@performance_function('MB/s', perf_key='Scale')\ndef extract_scale_perf(self):\n    return sn.extractsingle(r'Scale:\\s+(\\S+)\\s+.*', self.stdout, 1, float)\n\n@performance_function('MB/s', perf_key='Add')\ndef extract_add_perf(self):\n    return sn.extractsingle(r'Add:\\s+(\\S+)\\s+.*', self.stdout, 1, float)\n\n@performance_function('MB/s', perf_key='Triad')\ndef extract_triad_perf(self):\n    return sn.extractsingle(r'Triad:\\s+(\\S+)\\s+.*', self.stdout, 1, float)\n</code></pre>"},{"location":"tutorial/excalibur-tests_tutorial/#run-stream-benchmark","title":"Run Stream Benchmark","text":"<p>You can now run the benchmark in the same way as the previous sombrero example</p> ARCHER2 <pre><code>reframe -c excalibur-tests/benchmarks/apps/stream/ -r --system archer2 -J'--qos=short'\n</code></pre>"},{"location":"tutorial/excalibur-tests_tutorial/#sample-output","title":"Sample Output","text":"ARCHER2 <pre><code>$ reframe -c excalibur-tests/benchmarks/examples/stream/ -r -J'--qos=short'\n[ReFrame Setup]\n  version:           4.4.1\n  command:           '/mnt/lustre/a2fs-work3/work/d193/d193/tk-d193/ciuk-demo/demo-env/bin/reframe -c excalibur-tests/benchmarks/examples/stream/ -r -J--qos=short'\n  launched by:       tk-d193@ln03\n  working directory: '/mnt/lustre/a2fs-work3/work/d193/d193/tk-d193/ciuk-demo'\n  settings files:    '&lt;builtin&gt;', '/work/d193/d193/tk-d193/ciuk-demo/excalibur-tests/benchmarks/reframe_config.py'\n  check search path: '/mnt/lustre/a2fs-work3/work/d193/d193/tk-d193/ciuk-demo/excalibur-tests/benchmarks/examples/stream'\n  stage directory:   '/mnt/lustre/a2fs-work3/work/d193/d193/tk-d193/ciuk-demo/stage'\n  output directory:  '/mnt/lustre/a2fs-work3/work/d193/d193/tk-d193/ciuk-demo/output'\n  log files:         '/tmp/rfm-z87x4min.log'\n\n  [==========] Running 1 check(s)\n  [==========] Started on Thu Nov 30 14:50:21 2023 \n\n  [----------] start processing checks\n  [ RUN      ] StreamBenchmark /8aeff853 @archer2:compute-node+default\n  [       OK ] (1/1) StreamBenchmark /8aeff853 @archer2:compute-node+default\n  P: Copy: 1380840.8 MB/s (r:0, l:None, u:None)\n  P: Scale: 1369568.7 MB/s (r:0, l:None, u:None)\n  P: Add: 1548666.1 MB/s (r:0, l:None, u:None)\n  P: Triad: 1548666.1 MB/s (r:0, l:None, u:None)\n  [----------] all spawned checks have finished\n\n  [  PASSED  ] Ran 1/1 test case(s) from 1 check(s) (0 failure(s), 0 skipped, 0 aborted)\n  [==========] Finished on Thu Nov 30 14:51:13 2023 \n  Log file(s) saved in '/tmp/rfm-z87x4min.log'\n</code></pre>"},{"location":"tutorial/excalibur-tests_tutorial/#interpreting-stream-results","title":"Interpreting STREAM results","text":"<p>With default compile options, STREAM uses arrays of 10 million elements. On a full node, the default array size fits into cache, and the benchmark does not report the correct memory bandwidth.  Therefore the numbers from this tutorial are not comparable with other, published, results.</p> <p>To avoid caching, increase the array size during build by adding e.g. <code>stream_array_size=64000000</code> to the spack spec. </p>"},{"location":"tutorial/excalibur-tests_tutorial/#parametrized-tests","title":"Parametrized tests","text":"<p>You can pass a list to the <code>parameter()</code> built-in function in the class body to create a parametrized test. You cannot access the individual parameter value within the class body, so any reference to them should be placed in the appropriate function, for example <code>__init__()</code></p> <p>Example: Parametrize the array size</p> <pre><code>array_size = parameter(int(i) for i in [4e6,8e6,16e6,32e6,64e6])\ndef __init__(self):\n    self.spack_spec = f\"stream@5.10 +openmp stream_array_size={self.array_size}\"\n</code></pre> ARCHER2 <pre><code>[----------] start processing checks\n[ RUN      ] StreamBenchmark %array_size=64000000 /bbfd0e71 @archer2:compute-node+default\n[ RUN      ] StreamBenchmark %array_size=32000000 /e16f9017 @archer2:compute-node+default\n[ RUN      ] StreamBenchmark %array_size=16000000 /abc01230 @archer2:compute-node+default\n[ RUN      ] StreamBenchmark %array_size=8000000 /51d83d77 @archer2:compute-node+default\n[ RUN      ] StreamBenchmark %array_size=4000000 /8399bc0b @archer2:compute-node+default\n[       OK ] (1/5) StreamBenchmark %array_size=32000000 /e16f9017 @archer2:compute-node+default\nP: Copy: 343432.5 MB/s (r:0, l:None, u:None)\nP: Scale: 291065.8 MB/s (r:0, l:None, u:None)\nP: Add: 275577.5 MB/s (r:0, l:None, u:None)\nP: Triad: 247425.0 MB/s (r:0, l:None, u:None)\n[       OK ] (2/5) StreamBenchmark %array_size=16000000 /abc01230 @archer2:compute-node+default\nP: Copy: 2538396.7 MB/s (r:0, l:None, u:None)\nP: Scale: 2349544.5 MB/s (r:0, l:None, u:None)\nP: Add: 2912500.4 MB/s (r:0, l:None, u:None)\nP: Triad: 2886402.8 MB/s (r:0, l:None, u:None)\n[       OK ] (3/5) StreamBenchmark %array_size=8000000 /51d83d77 @archer2:compute-node+default\nP: Copy: 1641807.1 MB/s (r:0, l:None, u:None)\nP: Scale: 1362616.5 MB/s (r:0, l:None, u:None)\nP: Add: 1959382.9 MB/s (r:0, l:None, u:None)\nP: Triad: 1940497.3 MB/s (r:0, l:None, u:None)\n[       OK ] (4/5) StreamBenchmark %array_size=64000000 /bbfd0e71 @archer2:compute-node+default\nP: Copy: 255622.4 MB/s (r:0, l:None, u:None)\nP: Scale: 235186.0 MB/s (r:0, l:None, u:None)\nP: Add: 204853.9 MB/s (r:0, l:None, u:None)\nP: Triad: 213072.2 MB/s (r:0, l:None, u:None)\n[       OK ] (5/5) StreamBenchmark %array_size=4000000 /8399bc0b @archer2:compute-node+default\nP: Copy: 1231355.3 MB/s (r:0, l:None, u:None)\nP: Scale: 1086783.2 MB/s (r:0, l:None, u:None)\nP: Add: 1519446.0 MB/s (r:0, l:None, u:None)\nP: Triad: 1548666.1 MB/s (r:0, l:None, u:None)\n[----------] all spawned checks have finished\n\n[  PASSED  ] Ran 5/5 test case(s) from 5 check(s) (0 failure(s), 0 skipped, 0 aborted)\n[==========] Finished on Thu Nov 30 14:34:48 2023 \n</code></pre>"},{"location":"tutorial/excalibur-tests_tutorial/#reference-values","title":"Reference values","text":"<p>ReFrame can automate checking that the results fall within an expected range. We can use it in our previous example of increasing the array size to avoid caching. You can set a different reference value for each <code>perf_key</code> in the performance function. For example, set the test to fail if it falls outside of +-25% of the values obtained with the largest array size.</p> ARCHER2 <pre><code>reference = {\n    'archer2': {\n        'Copy':  (260000, -0.25, 0.25, 'MB/s'),\n        'Scale': (230000, -0.25, 0.25, 'MB/s'),\n        'Add':   (210000, -0.25, 0.25, 'MB/s'),\n        'Triad': (210000, -0.25, 0.25, 'MB/s')\n    }\n}\n</code></pre> <p>The performance reference tuple consists of the reference value, the lower and upper thresholds expressed as fractional numbers relative to the reference value, and the unit of measurement. If any of the thresholds is not relevant, None may be used instead. Also, the units in this reference variable are entirely optional, since they were already provided through the @performance_function decorator.</p>"},{"location":"tutorial/excalibur-tests_tutorial/#useful-reading","title":"Useful Reading","text":""},{"location":"tutorial/excalibur-tests_tutorial/#reframe","title":"ReFrame","text":"<ul> <li>ReFrame Documentation</li> <li>ReFrame tutorials<ul> <li>Tutorial 1: Getting started with ReFrame</li> <li>Tutorial 2: Customizing Further a Regression Test</li> <li>Tutorial 3: Using Dependencies in ReFrame Tests</li> <li>Tutorial 4: Using Test Fixtures</li> <li>Tutorial 5: Using Build Automation Tools As a Build System</li> <li>Tutorial 6: Tips and Tricks</li> </ul> </li> <li>Libraries of ReFrame tests<ul> <li>Official ReFrame test library</li> <li>ReFrame GitHub organisation with various contributed test libraries</li> </ul> </li> </ul>"},{"location":"tutorial/excalibur-tests_tutorial/#spack","title":"Spack","text":"<ul> <li>Spack documentation</li> <li>Spack tutorial (including YouTube recordings)</li> <li>Spack package searchable list</li> </ul>"},{"location":"tutorial/getting-started/","title":"Getting Started on HPC systems","text":"<p>These tutorials have been run in in-person workshops on various HPC systems in the UK.  It should be possible to run on any of the supported systems, or to set up the tools on a local machine.  If you have access to one of the systems we've previously used, this tutorial helps you get set up. Otherwise, please consult the documentation</p>"},{"location":"tutorial/getting-started/#connecting","title":"Connecting","text":"CosmaARCHER2 <p>To run these tutorials on Cosma, you will need to connect to Cosma via ssh. You will need</p> <ol> <li>A Cosma account. You can request a new account if you haven't got one you can use. You can use an existing Cosma account to complete the tutorials.</li> <li>A command line terminal with an ssh client. Most Linux and Mac systems come with these preinstalled. Please see Connecting to ARCHER2 for more information and Windows instructions.</li> </ol> <p>To run these tutorials on ARCHER2, you will need to connect to ARCHER2 via ssh. You will need</p> <ol> <li>An ARCHER2 account. You can request a new account if you haven't got one you can use. You can use an existing ARCHER2 account to complete the tutorials.</li> <li>A command line terminal with an ssh client. Most Linux and Mac systems come with these preinstalled. Please see Connecting to ARCHER2 for more information and Windows instructions.</li> </ol>"},{"location":"tutorial/getting-started/#ssh","title":"ssh","text":"CosmaARCHER2 <p>Please see SSH access to Cosma for more information</p> <p>Once you have the above prerequisites, you have to generate an ssh key pair and upload the public key to SAFE. </p> <p>When you are done, check that you are able to connect to ARCHER2 with</p> <pre><code>ssh username@login.archer2.ac.uk\n</code></pre>"},{"location":"tutorial/getting-started/#mfa","title":"MFA","text":"CosmaARCHER2 <p>Cosma does not require MFA at present</p> <p>ARCHER2 has deployed mandatory multi-factor authentication (MFA)</p> <p>SSH keys will work as before, but instead of your ARCHER2 password, a Time-based One-Time Password (TOTP) code will be requested. </p> <p>TOTP is a six digit number, refreshed every 30 seconds, which is generated typically by an app running on your mobile phone or laptop.</p> <p>Thus authentication will require two factors:</p> <p>1) SSH key and passphrase 2) TOTP</p> <p>The SAFE documentation which details how to set up MFA on machine accounts (ARCHER2) is available at: https://epcced.github.io/safe-docs/safe-for-users/#how-to-turn-on-mfa-on-your-machine-account</p> <p>The documentation includes how to set this up without the need of a personal smartphone device.</p> <p>We have also updated the ARCHER2 documentation with details of the new connection process: https://docs.archer2.ac.uk/user-guide/connecting-totp/ https://docs.archer2.ac.uk/quick-start/quickstart-users-totp/ </p> <p>If there are any issues or concerns please contact support@archer2.ac.uk</p>"},{"location":"tutorial/postprocessing_tutorial/","title":"Postprocessing ReFrame output","text":""},{"location":"tutorial/postprocessing_tutorial/#postprocess-benchmark-results","title":"Postprocess Benchmark Results","text":"<p>Now let's browse the benchmark performance results, and create plots to visualise them.</p> <p>NOTE: The post-processing package is still under development. Please refer to the latest documentation to use it.</p>"},{"location":"tutorial/postprocessing_tutorial/#postprocessing-features","title":"Postprocessing features","text":"<p>The postprocessing can be performed either on a GUI or a CLI. It takes as input either a single perflog or a path that contains perflogs, and it is driven by a configuration YAML file (more on this later).  Its outputs can be csv files of the whole or filtered perflog contents, as well as plots.</p> <p></p> <p>We will explore its functionality features (series, filters, scaling) via the GUI first.</p>"},{"location":"tutorial/postprocessing_tutorial/#gui-demo","title":"GUI Demo","text":"<p>We can launch the GUI with</p> <p><code>streamlit run excalibur-tests/post-processing/streamlit_post_processing.py perflogs/&lt;system&gt;/&lt;partition&gt;/StreamTest.log</code></p> <p>Demo: - Start without config, explore unfiltered DataFrame - Create a plot with axes, series, filters, scaling, and extra columns     - See how the DataFrame gets filtered/modified as values are set for those fields     - See how those fields get modified in the configuration in real time - Export config</p> <p>Optional: - Modify config outside the GUI - Load it back in and generate new plot</p>"},{"location":"tutorial/postprocessing_tutorial/#the-plotting-configuration-file","title":"The plotting configuration file","text":"<p>We explored all those features in the GUI, just repeating them here for reference.</p> <p>The framework contains tools to plot the FOMs of benchmarks against any of the other parameters in the perflog. This generic plotting is driven by a configuration YAML file like the one we exported from the GUI - but can also be written from scratch.</p> <p>The file needs to include - Plot title  - Axis information - Optional: series, filters - Optional: scaling - Optional: extra columns - Data types</p>"},{"location":"tutorial/postprocessing_tutorial/#title-and-axes","title":"Title and Axes","text":"<p>Axes must have a value specified with a DataFrame column name, and units specified with either a DataFrame column name or a custom label (including <code>null</code>). <pre><code>title: Performance vs number of tasks and CPUs_per_task\n\nx_axis:\n  value: \"arraysize\"\n  units:\n    custom: null\n\ny_axis:\n  value: \"Copy_value\"\n  units:\n    column: \"Copy_unit\"\n</code></pre></p>"},{"location":"tutorial/postprocessing_tutorial/#filters","title":"Filters","text":"<p>Those can be of two types: series and filters.</p>"},{"location":"tutorial/postprocessing_tutorial/#data-series","title":"Data series","text":"<p>Display several data series in the same plot and group x-axis data by specified column values. Specify an empty list if you only want one series plotted.  In our STREAM example, we have two parameters. Therefore we need to either filter down to one, or make them separate series. Let's use separate series:</p> <p>Format: <code>[column_name, value]</code> <pre><code>series: [[\"param_cpus_per_task\", \"4\"], [\"param_cpus_per_task\", \"8\"]]\n</code></pre> NOTE: Currently, only one distinct <code>column_name</code> is supported. In the future, a second one will be allowed to be added. But in any case, unlimited number of series can be plotted for the same <code>column_name</code> but different <code>value</code>.</p>"},{"location":"tutorial/postprocessing_tutorial/#filtering","title":"Filtering","text":"<p>You can filter data rows based on specified conditions. Those can be combined in complex ways, using the \"and\" and \"or\" filter categories. Specify an empty list for no filters.</p> <p>Format: <code>[column_name, operator, value]</code>,  Accepted operators: \"==\", \"!=\", \"&lt;\", \"&gt;\", \"&lt;=\", \"&gt;=\" <pre><code>filters:\n  and:\n  - [job_completion_time, '&gt;=', '2024-04-26 11:21:30']\n  or: []\n</code></pre></p> <p>NOTE: After re-running the benchmarks a few times your perflog will get populated with multiple lines and you'll have to filter down to what you want to plot. Feel free to experiment with a dirtier perflog file or a folder with several perflog files.</p>"},{"location":"tutorial/postprocessing_tutorial/#scaling","title":"Scaling","text":"<p>You can scale the y axis values in various ways.</p> <p>By a fixed number: <pre><code>y_axis:\n  value: \"Copy_value\"\n  units:\n    column: \"Copy_unit\"\n  scaling:\n    custom: 2\n</code></pre></p> <p>By another column: <pre><code>y_axis:\n  value: \"Copy_value\"\n  units:\n    column: \"Copy_unit\"\n  scaling:\n    column:\n      name: \"Add_value\"\n</code></pre></p> <p>By one of the series: <pre><code>y_axis:\n  value: \"Copy_value\"\n  units:\n    column: \"Copy_unit\"\n  scaling:\n    column:\n      name: \"Copy_value\"\n      series: 0\n</code></pre> where the \"series\" value is the index of the series (i.e. \"0\" means the first series, \"1\" the second, and so on)</p> <p>By a specific value in the column: <pre><code>y_axis:\n  value: \"Copy_value\"\n  units:\n    column: \"Copy_unit\"\n  scaling:\n    column:\n      name: \"Copy_value\"\n      series: 0\n      x_value: 5\n</code></pre></p>"},{"location":"tutorial/postprocessing_tutorial/#data-types","title":"Data types","text":"<p>All columns used in axes, filters, and series must have a user-specified type for the data they contain. This would be the pandas dtype, e.g. <code>str/string/object</code>, <code>int/int64</code>, <code>float/float64</code>, <code>datetime/datetime64</code>. <pre><code>column_types:\n  arraysize: \"int\"\n  Copy_value: \"float\"\n  Copy_unit: \"str\"\n  param_cpus_per_task: \"int\"\n  job_completion_time: \"datetime\"\n</code></pre></p>"},{"location":"tutorial/postprocessing_tutorial/#extra-columns","title":"Extra columns","text":"<p>If you choose to save to a csv file the filtered DataFrame for further analysis, you can include extra columns, in addition to the ones you used for plotting. Those will not affect your plot. <pre><code>extra_columns_to_csv: [\"Scale_value\", \"Add_value\", \"Triad_value\"]\n</code></pre></p>"},{"location":"tutorial/postprocessing_tutorial/#run-the-cli-postprocessing","title":"Run the CLI postprocessing","text":"<p>Now that we have a config file, we can change it as required and run it in an automated way with new data, using the CLI: <pre><code>python post_processing.py &lt;log_path&gt; &lt;config_path&gt;\n</code></pre> where - <code>&lt;log_path&gt;</code> is the path to a perflog file or a directory containing perflog files. - <code>&lt;config_path&gt;</code> is the path to the configuration YAML file. - other useful flags: <code>-s</code> to save the filtered DataFrame, <code>-np</code> to skip the plotting.</p> <p>In our case, <pre><code>python excalibur-tests/post-processing/post_processing.py -s perflogs/archer2/compute-node/StreamTest.log ~/Downloads/Plotyplot.yaml\n</code></pre></p>"},{"location":"tutorial/postprocessing_tutorial/#view-the-output","title":"View the Output","text":"<p>And behold! Inside <code>excalibur-tests/post-processing</code>, we've generated the same plot and the csv file with the data it contains, in a reproducible way!</p> <p></p>"},{"location":"tutorial/profiling_tutorial/","title":"Profiling tutorial -- !Work in progress!","text":""},{"location":"tutorial/profiling_tutorial/#outline","title":"Outline","text":"<ol> <li>Running a benchmark with a profiler</li> <li>Sampling profilers (Nsight &amp; Vtune)</li> <li>Collecting roofline data (advisor-roofline)</li> </ol>"},{"location":"tutorial/profiling_tutorial/#profilers-in-excalibur-tests","title":"Profilers in excalibur-tests","text":"<p>The <code>excalibur-tests</code> framework allows you to run a profiler together with a benchmark application. To do this, you can set the profiler attribute on the command line using the <code>-S profiler=...</code> syntax</p> <p>We support profilers that can be spack installed without a lincense and don't require modifying the source code or the build system. Currently supported values for the profiler attribute are:</p> <ul> <li><code>advisor-roofline</code>: it produces a roofline model of your program using Intel Advisor;</li> <li><code>nsight</code>: it runs the code with the NVIDIA Nsight Systems profiler;</li> <li><code>vtune</code>: it runs the code with the Intel VTune profiler.</li> </ul> <p>For more details, see the User documentation</p>"},{"location":"tutorial/profiling_tutorial/#profiling-with-nsight","title":"Profiling with Nsight","text":"<ul> <li>NVIDIA Nsight Systems is a low-overhead sampling profiler that supports both CPU and GPU applications.</li> <li>Supports both x86 and ARM architectures</li> <li>We collect <code>nsys profile --trace=cuda,mpi,nvtx,openmp,osrt,opengl,syscall</code></li> </ul> <p>Run Nsight profiling with <pre><code>reframe -c path/to/application -r -S profiler=nsight\n</code></pre></p> <ul> <li>Spack installs the <code>nvidia-nsight-systems</code> package in the background, including the GUI</li> <li>The paths to the collected profile data, and to the GUI launcher are written into <code>rfm_job.out</code></li> <li>To run the GUI remotely, you need to login with <code>ssh -X</code>. It may be slow on a remote system.</li> <li>You can (spack) install the GUI locally to view the data.</li> </ul>"},{"location":"tutorial/profiling_tutorial/#profiling-with-vtune","title":"Profiling with VTune","text":"<ul> <li>Intel VTune is a low-overhead sampling profiler that supports CPU applications</li> <li>Only runs on x86 architectures</li> </ul> <p>Run VTune profiling with <pre><code>reframe -c path/to/application -r -S profiler=vtune\n</code></pre> - Spack installs <code>intel-oneapi-vtune</code> package in the background, including the GUI</p>"},{"location":"tutorial/profiling_tutorial/#roofline-analysis-with-advisor","title":"Roofline analysis with Advisor","text":"<ul> <li>Intel Advisor is a tool for on-node performance optimisation. It does analysis for efficient Vectorization, Threading, Memory Usage, and Accelerator Offloading</li> <li>Since ~2018 it has had support for automated roofline analysis</li> <li>Is only supports x86 CPU architecture</li> <li>Won't run on the MPI launcher (because it does on-node analysis). In our benchmarks we have to override it. It can run inside an MPI job on a single rank but we don't currently support it, hopefully will be available in the future.</li> </ul> <p>To run on a single MPI rank without <code>mpirun</code>, add the following decorated function to the test class <pre><code>    @run_before('run')\n    def replace_launcher(self):\n        self.job.launcher = getlauncher('local')()\n</code></pre></p> <ul> <li>We collect <code>advisor -collect roofline</code></li> </ul> <p>Run Advisor roofline collection with <pre><code>reframe -c path/to/stream -r -S profiler=advisor-roofline\n</code></pre></p> <ul> <li>Similar to Nsight, the GUI is installed by Spack but is slow to run remotely.</li> </ul>"},{"location":"tutorial/reframe_tutorial/","title":"Automating benchmarks with ReFrame","text":""},{"location":"tutorial/reframe_tutorial/#outline","title":"Outline","text":"<ol> <li>How ReFrame executes tests</li> <li>Structure of a ReFrame test -- Hello world example</li> <li>Configuring ReFrame to run tests on HPC systems</li> <li>Writing performance tests -- Stream example</li> <li>Working with build systems -- Make, CMake, Autotools, Spack examples</li> <li>Avoiding build systems -- Run-only tests</li> </ol> <p>This tutorial is adapted from ReFrame 4.5 tutorials, that also cover more ReFrame features. Direct quotes from the tutorial are marked with</p> <p>ReFrame Tutorials</p> <p>There's a new tutorial with a slightly different approach in ReFrame 4.6.</p>"},{"location":"tutorial/reframe_tutorial/#how-reframe-executes-tests","title":"How ReFrame executes tests","text":"<p>When ReFrame executes a test it runs a pipeline of the following stages</p> <p></p> <p>You can customise the behaviour of each stage or add a hook before or after each of them.  For more details, read the ReFrame pipeline documentation.</p>"},{"location":"tutorial/reframe_tutorial/#set-up-python-environment","title":"Set up python environment","text":"CosmaARCHER2 <p>This tutorial is run on the Cosma supercomputer. It should be straightforward to run on a different platform, the requirements are  <code>gcc 4.5</code>, <code>git 2.39</code> and <code>python 3.7</code> or later.  (for the later parts you also need <code>make</code>, <code>autotools</code>, <code>cmake</code> and <code>spack</code> but these can be installed locally). Before proceeding to install ReFrame, we recommend creating a python virtual environment to avoid clashes with other installed python packages. First load a newer python module. <pre><code>module swap python/3.10.12\n</code></pre></p> <p>This tutorial is run on ARCHER2, you should have signed up for a training account before starting. It can be ran on other HPC systems with a batch scheduler but will require making some changes to the config. Before proceeding to install ReFrame, we recommend creating a python virtual environment to avoid clashes with other installed python packages. First load the system python module. <pre><code>module load cray-python\n</code></pre></p> <p>Then create an environment and activate it with</p> <pre><code>python3 -m venv reframe_tutorial\nsource reframe_tutorial/bin/activate\n</code></pre> <p>You will have to activate the environment each time you login. To deactivate the environment run <code>deactivate</code>.</p>"},{"location":"tutorial/reframe_tutorial/#install-reframe","title":"Install ReFrame","text":"<p>Then install ReFrame with <code>pip</code>.</p> <pre><code>pip install reframe-hpc\n</code></pre> <p>Alternatively, you can</p> <pre><code>git clone -q --depth 1 --branch v4.5.2 https://github.com/reframe-hpc/reframe.git\nsource reframe/bootstrap.sh\n</code></pre> <p>You can also clone the ReFrame git repository to get the source code of the ReFrame tutorials. We will refer to some of the tutorial solutions later. ReFrame rewrote their tutorials in v4.6 and some of the examples we are using are not there anymore, therefore it's best to clone ReFrame v4.5.</p>"},{"location":"tutorial/reframe_tutorial/#hello-world-example","title":"Hello world example","text":"<p>There's a Hello world example in the ReFrame 4.5 tutorial that explains how to create a simple ReFrame test.</p> <p>ReFrame tests are python classes that describe how a test is run. To get started, open an empty <code>.py</code> file where you will write the ReFrame class, e.g. <code>hello.py</code>.</p>"},{"location":"tutorial/reframe_tutorial/#include-reframe-module","title":"Include ReFrame module","text":"<p>The first thing you need is to include ReFrame. We separately import sanity to simplify the syntax. These should be available if the installation step was successful.</p> <pre><code>import reframe as rfm\nimport reframe.utility.sanity as sn\n</code></pre>"},{"location":"tutorial/reframe_tutorial/#create-a-test-class","title":"Create a Test Class","text":"<p>ReFrame uses decorators to mark classes as tests. This marks <code>class HelloTest</code> as a <code>rfm.simple_test</code>. ReFrame tests ultimately derive from <code>RegressionTest</code>. There are other derived classes such as <code>RunOnlyRegressionTest</code>, we get to those later.</p> <pre><code>@rfm.simple_test\nclass HelloTest(rfm.RegressionTest):\n</code></pre> <p>The data members and methods detailed in the following sections should be placed inside this class.</p>"},{"location":"tutorial/reframe_tutorial/#add-mandatory-attributes","title":"Add mandatory attributes","text":"<ul> <li><code>valid_systems</code> for where this test can run. For now we haven't defined any systems so we leave it as <code>'*'</code> (any system)</li> <li><code>valid_prog_environs</code> for what compilers this test can build with. More on it later.</li> <li>In a test with a single source file, it is enough to define <code>sourcepath</code>. More on build systems later.</li> <li>We could add <code>sourcesdir</code> to point to the source directory, but it defaults to <code>src/</code></li> </ul> <pre><code>    valid_systems = ['*']\n    valid_prog_environs = ['*']\n    sourcepath = 'hello.c'\n</code></pre>"},{"location":"tutorial/reframe_tutorial/#add-sanity-check","title":"Add sanity check","text":"<ul> <li>ReFrame, by default, makes no assumption about whether a test is successful or not.</li> <li>A test must provide a validation function that asserts whether the test was successful</li> <li>ReFrame provides utility functions that help matching patterns and extract values from the test\u2019s output</li> <li>Here we match a string from stdout</li> </ul> <pre><code>    @sanity_function\n    def assert_hello(self):\n        return sn.assert_found(r'Hello, World\\!', self.stdout)\n</code></pre>"},{"location":"tutorial/reframe_tutorial/#run-the-benchmark","title":"Run the benchmark","text":"<p>The basic syntax to run ReFrame benchmarks is</p> <pre><code>reframe -c path/to/benchmark -r\n</code></pre>"},{"location":"tutorial/reframe_tutorial/#builtin-programming-environment","title":"Builtin programming environment","text":"<p>We didn't tell reframe anything about how to compile the hello world example. How did it compile? ReFrame uses a buitin programming environment by default. You can see this with <code>reframe --show-config</code> The builtin programming environment only contains the <code>cc</code> compiler, compiling a C++ or Fortran code will fail</p>"},{"location":"tutorial/reframe_tutorial/#configuring-reframe-for-hpc-systems","title":"Configuring ReFrame for HPC systems","text":"<p>In ReFrame, all the details of the various interactions of a test with the system environment are handled transparently and are set up in its configuration file.</p> <p>For the minimum configuration to run jobs on the system we need to</p> CosmaARCHER2 <ul> <li>Create a system with a name and a description</li> <li>Set the module system for accessing centrally installed modules</li> <li>Create a compute node partition</li> <li>Set a scheduler and a MPI launcher to run on compute nodes<ul> <li>On Cosma, the scheduler rejects jobs that don't set a time limit. Add <code>time_limit = 1m</code> to ReFrame tests to run on Cosma or set from command line with <code>-S time_limit='1m'</code></li> </ul> </li> <li>Set access options <pre><code>'access': ['--partition=bluefield1', '--account=do009'],\n</code></pre></li> <li>Create at least one programming environment to set compilers</li> </ul> <pre><code>site_configuration = {\n    'systems' : [\n        {\n            'name': 'cosma',\n            'descr': 'Cosma for performance workshop',\n            'hostnames': ['login[0-9][a-z].pri.cosma[0-9].alces.network'],\n            'modules_system': 'tmod4',\n            'partitions': [\n                {\n                    'name': 'compute-node',\n                    'scheduler': 'slurm',\n                    'launcher': 'mpiexec',\n                    'environs': ['gnu'],\n                    'access': ['--partition=bluefield1', '--account=do009'],\n                }\n            ]\n        }\n    ],\n    'environments': [\n        {\n            'name': 'gnu',\n            'modules': ['gnu_comp', 'openmpi'],\n            'cc': 'mpicc',\n            'cxx': 'mpic++',\n            'ftn': 'mpif90'\n        },\n    ]\n}\n</code></pre> <ul> <li>Create a system with a name and a description</li> <li>Set the module system for accessing centrally installed modules</li> <li>Create a compute node partition</li> <li>Set a scheduler and a MPI launcher to run on compute nodes</li> <li>Set access options with  <pre><code>'access': ['--partition=standard', '--qos=short'],\n</code></pre></li> <li>Create at least one programming environment to set compilers</li> </ul> <pre><code>site_configuration = {\n    'systems' : [\n        {\n            'name': 'archer2',\n            'descr': 'ARCHER2 config for CIUK workshop',\n            'hostnames': ['ln[0-9]+'],\n            'partitions': [\n                {\n                    'name': 'compute-node',\n                    'scheduler': 'slurm',\n                    'launcher': 'srun',\n                    'access': ['--partition=standard', '--qos=short'],\n                    'environs': ['cray'],\n                }\n            ]\n        }\n    ],\n    'environments': [\n        {\n            'name': 'cray',\n            'cc': 'mpicc',\n            'cxx': 'mpic++',\n            'ftn': 'mpif90'\n        },\n    ]\n}\n</code></pre>"},{"location":"tutorial/reframe_tutorial/#performance-tests","title":"Performance tests","text":"<p>Performance tests capture data in performance variables. For simplicity, we use the STREAM benchmark as an example. It is the de facto memory bandwidth benchmark. It has four kernels that stream arrays from memory and perform different floating point operations on them. - Copy: <code>A = B</code> - Scale: <code>C = a * B</code> - Add: <code>C = A + B</code> - Triad: <code>C = a * A + B</code></p>"},{"location":"tutorial/reframe_tutorial/#create-the-test-class","title":"Create the Test Class","text":"<p>The imports and the class declaration look the same as in the hello world example.  We can now specify valid systems and programming environments to run on the system we just configured.  You can adapt these to your system, or keep using <code>'*'</code> to run on any platform.</p> CosmaARCHER2 <pre><code>import reframe as rfm\nimport reframe.utility.sanity as sn\n\n@rfm.simple_test\nclass StreamTest(rfm.RegressionTest):\n    valid_systems = ['cosma']\n    valid_prog_environs = ['gnu']\n</code></pre> <pre><code>import reframe as rfm\nimport reframe.utility.sanity as sn\n\n@rfm.simple_test\nclass StreamTest(rfm.RegressionTest):\n    valid_systems = ['archer2']\n    valid_prog_environs = ['cray']\n</code></pre>"},{"location":"tutorial/reframe_tutorial/#git-cloning-the-source","title":"Git Cloning the source","text":"<p>We can retrieve specifically a Git repository by assigning its URL directly to the sourcesdir attribute:</p> <pre><code>    sourcesdir='https://github.com/jeffhammond/STREAM'\n</code></pre>"},{"location":"tutorial/reframe_tutorial/#environment-variables","title":"Environment variables","text":"<p>We can set environment variables in the <code>env_vars</code> dictionary.</p> <pre><code>    self.env_vars['OMP_NUM_THREADS'] = 4\n    self.env_vars['OMP_PLACES'] = 'cores'\n</code></pre>"},{"location":"tutorial/reframe_tutorial/#building-the-stream-benchmark","title":"Building the STREAM benchmark","text":"<p>Recall the pipeline ReFrame executes when running a test.  We can insert arbitrary functions between any steps in in the pipeline by decorating them with <code>@run_before</code> or <code>@run_after</code> Here we can set compiler flags before compiling. The STREAM benchmark takes the array size as a compile time argument.  It should be large enough to overflow all levels of cache so that there is no data reuse and we measure the main memory bandwidth.</p> <pre><code>    build_system='SingleSource'\n    sourcepath='stream.c'\n    arraysize = 2**20\n\n    @run_before('compile')\n    def set_compiler_flags(self):\n        self.build_system.cppflags = [f'-DSTREAM_ARRAY_SIZE={self.arraysize}']\n        self.build_system.cflags = ['-fopenmp', '-O3']\n</code></pre>"},{"location":"tutorial/reframe_tutorial/#add-sanity-check_1","title":"Add Sanity Check","text":"<p>The rest of the benchmark follows the Writing a Performance Test ReFrame Tutorial. First we need a sanity check that ensures the benchmark ran successfully. A function decorated with the <code>@sanity_function</code> decorator is used by ReFrame to check that the test ran successfully. The sanity function can perform a number of checks, in this case we want to match a line of the expected standard output.</p> <pre><code>@sanity_function\ndef validate_solution(self):\n    return sn.assert_found(r'Solution Validates', self.stdout)\n</code></pre>"},{"location":"tutorial/reframe_tutorial/#add-performance-pattern-check","title":"Add Performance Pattern Check","text":"<p>To record the performance of the benchmark, ReFrame should extract a figure of merit from the output of the test. A function decorated with the <code>@performance_function</code> decorator extracts or computes a performance metric from the test\u2019s output.</p> <p>In this example, we extract four performance variables, namely the memory bandwidth values for each of the \u201cCopy\u201d, \u201cScale\u201d, \u201cAdd\u201d and \u201cTriad\u201d sub-benchmarks of STREAM, where each of the performance functions use the <code>extractsingle()</code> utility function. For each of the sub-benchmarks we extract the \u201cBest Rate MB/s\u201d column of the output (see below) and we convert that to a float.</p> <pre><code>@performance_function('MB/s', perf_key='Copy')\ndef extract_copy_perf(self):\n    return sn.extractsingle(r'Copy:\\s+(\\S+)\\s+.*', self.stdout, 1, float)\n\n@performance_function('MB/s', perf_key='Scale')\ndef extract_scale_perf(self):\n    return sn.extractsingle(r'Scale:\\s+(\\S+)\\s+.*', self.stdout, 1, float)\n\n@performance_function('MB/s', perf_key='Add')\ndef extract_add_perf(self):\n    return sn.extractsingle(r'Add:\\s+(\\S+)\\s+.*', self.stdout, 1, float)\n\n@performance_function('MB/s', perf_key='Triad')\ndef extract_triad_perf(self):\n    return sn.extractsingle(r'Triad:\\s+(\\S+)\\s+.*', self.stdout, 1, float)\n</code></pre>"},{"location":"tutorial/reframe_tutorial/#perflogs","title":"Perflogs","text":"<p>The output from performance tests is written in perflogs. They are csv files that are appended each time a test is ran. By default the perflogs are output in <code>perflogs/&lt;system&gt;/&lt;partition&gt;</code>. By default a lot of information about the test is stored. This can be customized in the configuration file. By default there is not much information about build step, but ReFrame will provide a link back to build environment. A more verbose report is written in <code>.reframe/reports/</code>, you can use the <code>--report-file</code> option to direct the report to a different file.</p> <p><code>excalibur-tests</code> provides tools to read and process the perflogs. See the Next Tutorial for details.</p>"},{"location":"tutorial/reframe_tutorial/#reference-values","title":"Reference values","text":"<p>ReFrame can automate checking that the results fall within an expected range. You can set a different reference value for each <code>perf_key</code> in the performance function. For example, set the test to fail if it falls outside of +-25% of the values obtained with the previous array size.</p> CosmaArcher2 <pre><code>reference = {\n    'cosma': {\n        'Copy':  (40000, -0.25, 0.25, 'MB/s'),\n        'Scale': (20000, -0.25, 0.25, 'MB/s'),\n        'Add':   (20000, -0.25, 0.25, 'MB/s'),\n        'Triad': (20000, -0.25, 0.25, 'MB/s')\n    }\n}\n</code></pre> <pre><code>reference = {\n    'archer2': {\n        'Copy':  (260000, -0.25, 0.25, 'MB/s'),\n        'Scale': (200000, -0.25, 0.25, 'MB/s'),\n        'Add':   (200000, -0.25, 0.25, 'MB/s'),\n        'Triad': (200000, -0.25, 0.25, 'MB/s')\n    }\n}\n</code></pre> <p>The performance reference tuple consists of the reference value, the lower and upper thresholds expressed as fractional numbers relative to the reference value, and the unit of measurement. If any of the thresholds is not relevant, None may be used instead. Also, the units in this reference variable are entirely optional, since they were already provided through the @performance_function decorator.</p>"},{"location":"tutorial/reframe_tutorial/#parametrized-tests","title":"Parametrized tests","text":"<p>You can pass a list to the <code>parameter()</code> built-in function in the class body to create a parametrized test. You cannot access the individual parameter value within the class body, so any reference to them should be placed in the appropriate function, for example <code>__init__()</code></p> <p>For parametrisation you can add for example <pre><code>    arraysize = parameter([5,15,25])\n    self.build_system.cppflags = [f'-DSTREAM_ARRAY_SIZE=$((2 ** {self.arraysize}))']\n</code></pre></p> <p>You can have multiple parameters. ReFrame will run all parameter combinations by default.</p>"},{"location":"tutorial/reframe_tutorial/#build-systems","title":"Build systems","text":"<p>ReFrame supports many commonly used build systems, include Cmake, Autotools, Spack and Easybuild. See the Build systems Reference for details. Here we show a few examples.</p>"},{"location":"tutorial/reframe_tutorial/#make","title":"Make","text":"<ul> <li>Tutorial in <code>tutorials/advanced/makefiles/maketest.py</code>.</li> </ul> <p>First, if you\u2019re using any build system other than SingleSource, you must set the executable attribute of the test, because ReFrame cannot know what is the actual executable to be run. We then set the build system to Make and set the preprocessor flags as we would do with the SingleSource build system.</p>"},{"location":"tutorial/reframe_tutorial/#autotools","title":"Autotools","text":"<p>It is often the case that a configuration step is needed before compiling a code with make. To address this kind of projects, ReFrame aims to offer specific abstractions for \u201cconfigure-make\u201d style of build systems. It supports CMake-based projects through the CMake build system, as well as Autotools-based projects through the Autotools build system.</p> <ul> <li>Automake Hello example</li> </ul> <pre><code>import reframe as rfm\nimport reframe.utility.sanity as sn\n\n@rfm.simple_test\nclass AutoHelloTest(rfm.RegressionTest):\n    valid_systems = ['*']\n    valid_prog_environs = ['*']\n    sourcesdir = 'https://github.com/ntegan/amhello.git'\n    build_system = 'Autotools'\n    executable = './src/hello'\n    prebuild_cmds = ['autoreconf --install .']\n    time_limit = '5m'\n\n    @sanity_function\n    def assert_hello(self):\n        return sn.assert_found(r'Hello world\\!', self.stdout)\n</code></pre>"},{"location":"tutorial/reframe_tutorial/#cmake","title":"CMake","text":"<ul> <li>CMake Hello example</li> </ul> <pre><code>import reframe as rfm\nimport reframe.utility.sanity as sn\n\n\n@rfm.simple_test\nclass CMakeHelloTest(rfm.RegressionTest):\n    valid_systems = ['*']\n    valid_prog_environs = ['*']\n    sourcesdir = 'https://github.com/jameskbride/cmake-hello-world.git'\n    build_system = 'CMake'\n    executable = './CMakeHelloWorld'\n    time_limit = '5m'\n\n    @sanity_function\n    def assert_hello(self):\n        return sn.assert_found(r'Hello, world\\!', self.stdout)\n</code></pre>"},{"location":"tutorial/reframe_tutorial/#spack","title":"Spack","text":"<ul> <li>ReFrame will use a user-provided Spack environment in order to build and test a set of specs.</li> <li>Tutorial in <code>tutorials/build_systems/spack/spack_test.py</code></li> <li>In <code>rfm_job.out</code> you can see that it<ul> <li>Creates a blank environment</li> <li>Builds all dependencies -- takes quite long</li> </ul> </li> <li><code>excalibur-tests</code> provides utilities and settings for Spack builds in ReFrame. See the Next Tutorial for details.</li> </ul>"},{"location":"tutorial/reframe_tutorial/#run-only-tests","title":"Run-only tests","text":"<p>If you don't wish to build your application in ReFrame (we recommend that you do!), you can define a run-only test. Run-only tests derive from the <code>rfm.RunOnlyRegressionTest</code> class instead of <code>rfm.RegressionTest</code>. Instead of a build system, you define an executable which reframe expects to find in <code>$PATH</code>. See tutorial in <code>tutorials/advanced/runonly/echorand.py</code></p>"},{"location":"tutorial/setup-python/","title":"Setup python","text":"CosmaARCHER2 <p>This tutorial is run on the Cosma supercomputer. It should be straightforward to run on a different platform, the requirements are  <code>gcc 4.5</code>, <code>git 2.39</code> and <code>python 3.7</code> or later.  (for the later parts you also need <code>make</code>, <code>autotools</code>, <code>cmake</code> and <code>spack</code> but these can be installed locally). Before proceeding to install ReFrame, we recommend creating a python virtual environment to avoid clashes with other installed python packages. First load a newer python module. <pre><code>module swap python/3.10.12\n</code></pre></p> <p>This tutorial is run on ARCHER2, you should have signed up for a training account before starting. It can be ran on other HPC systems with a batch scheduler but will require making some changes to the config. Before proceeding to install ReFrame, we recommend creating a python virtual environment to avoid clashes with other installed python packages. First load the system python module. <pre><code>module load cray-python\n</code></pre></p> <p>Then create an environment and activate it with</p> <pre><code>python3 -m venv reframe_tutorial\nsource reframe_tutorial/bin/activate\n</code></pre> <p>You will have to activate the environment each time you login. To deactivate the environment run <code>deactivate</code>.</p>"},{"location":"tutorial/stream-sanity-and-performance/","title":"Stream sanity and performance","text":""},{"location":"tutorial/stream-sanity-and-performance/#add-sanity-check","title":"Add Sanity Check","text":"<p>The rest of the benchmark follows the Writing a Performance Test ReFrame Tutorial. First we need a sanity check that ensures the benchmark ran successfully. A function decorated with the <code>@sanity_function</code> decorator is used by ReFrame to check that the test ran successfully. The sanity function can perform a number of checks, in this case we want to match a line of the expected standard output.</p> <pre><code>@sanity_function\ndef validate_solution(self):\n    return sn.assert_found(r'Solution Validates', self.stdout)\n</code></pre>"},{"location":"tutorial/stream-sanity-and-performance/#add-performance-pattern-check","title":"Add Performance Pattern Check","text":"<p>To record the performance of the benchmark, ReFrame should extract a figure of merit from the output of the test. A function decorated with the <code>@performance_function</code> decorator extracts or computes a performance metric from the test\u2019s output.</p> <p>In this example, we extract four performance variables, namely the memory bandwidth values for each of the \u201cCopy\u201d, \u201cScale\u201d, \u201cAdd\u201d and \u201cTriad\u201d sub-benchmarks of STREAM, where each of the performance functions use the <code>extractsingle()</code> utility function. For each of the sub-benchmarks we extract the \u201cBest Rate MB/s\u201d column of the output (see below) and we convert that to a float.</p> <pre><code>@performance_function('MB/s', perf_key='Copy')\ndef extract_copy_perf(self):\n    return sn.extractsingle(r'Copy:\\s+(\\S+)\\s+.*', self.stdout, 1, float)\n\n@performance_function('MB/s', perf_key='Scale')\ndef extract_scale_perf(self):\n    return sn.extractsingle(r'Scale:\\s+(\\S+)\\s+.*', self.stdout, 1, float)\n\n@performance_function('MB/s', perf_key='Add')\ndef extract_add_perf(self):\n    return sn.extractsingle(r'Add:\\s+(\\S+)\\s+.*', self.stdout, 1, float)\n\n@performance_function('MB/s', perf_key='Triad')\ndef extract_triad_perf(self):\n    return sn.extractsingle(r'Triad:\\s+(\\S+)\\s+.*', self.stdout, 1, float)\n</code></pre>"}]}