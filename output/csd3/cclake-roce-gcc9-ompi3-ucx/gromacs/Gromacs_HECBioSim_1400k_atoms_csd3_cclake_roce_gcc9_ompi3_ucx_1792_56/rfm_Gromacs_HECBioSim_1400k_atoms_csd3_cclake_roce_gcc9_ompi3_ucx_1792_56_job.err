                      :-) GROMACS - gmx mdrun, 2016.6 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.6
Executable:   /home/hpcbras1/spack/opt/spack/linux-scientific7-skylake_avx512/gcc-9.1.0/gromacs-2016.6-kgomb677so5w42qytvzkbdmclmzk5mvl/bin/gmx_mpi
Data prefix:  /home/hpcbras1/spack/opt/spack/linux-scientific7-skylake_avx512/gcc-9.1.0/gromacs-2016.6-kgomb677so5w42qytvzkbdmclmzk5mvl
Working dir:  /rds/user/hpcbras1/hpc-work/csd3/cclake-roce-gcc9-ompi3-ucx/gromacs/Gromacs_HECBioSim_1400k_atoms_csd3_cclake_roce_gcc9_ompi3_ucx_1792_56
Command line:
  gmx_mpi mdrun -s benchmark.tpr -g 1400k-atoms.log -noconfout


Running on 32 nodes with total 32 cores, 32 logical cores
  Cores per node:            1 - 56
  Logical cores per node:    1 - 56
Hardware detected on host cpu-p-296 (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) Platinum 8276 CPU @ 2.20GHz
    SIMD instructions most likely to fit this hardware: AVX_512
    SIMD instructions selected at GROMACS compile time: AVX_512

  Hardware topology: Full, with devices

Reading file benchmark.tpr, VERSION 5.1.4 (single precision)
Note: file tpx version 103, software tpx version 110
Changing nstlist from 10 to 25, rlist from 1.2 to 1.23

The number of OpenMP threads was set by environment variable OMP_NUM_THREADS to 1

Will use 1344 particle-particle and 448 PME only ranks
This is a guess, check the performance at the end of the log file
Using 1792 MPI processes
Using 1 OpenMP thread per MPI process


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity
starting mdrun 'Her1-Her1'
10000 steps,     20.0 ps.

step 7500 Turning on dynamic load balancing, because the performance loss due to load imbalance is 2.4 %.


 Average load imbalance: 8.6 %
 Part of the total run time spent waiting due to load imbalance: 2.8 %
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 % Z 0 %
 Average PME mesh/force load: 1.355
 Part of the total run time spent waiting due to PP/PME imbalance: 11.2 %

NOTE: 11.2 % performance was lost because the PME ranks
      had more work to do than the PP ranks.
      You might want to increase the number of PME ranks
      or increase the cut-off and the grid spacing.


NOTE: 10 % of the run time was spent communicating energies,
      you might want to use the -gcom option of mdrun


               Core t (s)   Wall t (s)        (%)
       Time:    97749.736       54.548   179200.0
                 (ns/day)    (hour/ns)
Performance:       31.682        0.758

GROMACS reminds you: "I managed to get two hours of work done before work" (E. Lindahl)


real	1m16.946s
user	0m0.064s
sys	0m0.051s
